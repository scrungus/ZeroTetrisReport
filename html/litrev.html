<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"  
  "http://www.w3.org/TR/html4/loose.dtd">  
<html > 
<head><title></title> 
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"> 
<meta name="generator" content="TeX4ht (https://tug.org/tex4ht/)"> 
<meta name="originator" content="TeX4ht (https://tug.org/tex4ht/)"> 
<!-- html --> 
<meta name="src" content="litrev.tex"> 
<link rel="stylesheet" type="text/css" href="litrev.css"> 
</head><body 
>
<h3 class="sectionHead"><span class="titlemark">1   </span> <a 
 id="x1-10001"></a>The Bot</h3>
<!--l. 21--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">1.1   </span> <a 
 id="x1-20001.1"></a>Reinforcement Learning</h4>
<div class="epigraph">Reinforcement learning is the learning of a mapping from situations to actions so
as to maximize a scalar reward or reinforcement signal.<hr />
<div class="episource"><span 
class="cmti-12">Richard S. Sutton</span></div></div> Richard S. Sutton, who can be considered an authoritative
voice in the field of Reinforcement Learning (hereinafter out referred to as RL),
accurately summarizes the essence of this domain in the above quote. The
learning agent in a RL environment produces an output signal in the
form of an action, and receives an input signal in the form of a reward.
Trial-and-error search; namely, the balance between exploration of unknowns and
exploitation of current knowledge, and the concept of delayed reward can be
considered to be two of the distinguishing features when compared to
other machine learning paradigms (those of supervised and unsupervised
learning)^<sup class="textsuperscript"><span
class="cmr-10x-x-109">1</span></sup>.<br 
class="newline" /><br 
class="newline" />Exploration versus exploitation is an interesting trade-off; if an agent explores too
much, it risks spending too little time exploiting what it has learnt to its
advantage. If it explores too little, the agent risks missing out on the aquisition of
the optimal strategy. <br 
class="newline" /><br 
class="newline" />Every reinforcement learning method is essentially trying to find a solution to the
following equation (and variations of it): <hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
<a 
 id="x1-2001r1"></a>
                                                                          

                                                                          
<!--l. 31--><p class="noindent" ><span 
class="cmmi-12">V</span> <sub><span 
class="cmmi-8">&#x03C0;</span></sub>(<span 
class="cmmi-12">s</span>) = <span 
class="cmmi-12">E</span><sub><span 
class="cmmi-8">&#x03C0;</span></sub>[<span 
class="cmmi-12">R</span>(<span 
class="cmmi-12">s,a</span>) + <span 
class="cmmi-12">&#x03B3;V </span>(<span 
class="cmmi-12">s</span><span 
class="cmsy-10x-x-120">&#x2032;</span>)] = <img 
src="litrev0x.png" alt="&#x03A3;a"  class="" ><span 
class="cmmi-12">&#x03C0;</span>(<span 
class="cmmi-12">a</span><span 
class="cmsy-10x-x-120">|</span><span 
class="cmmi-12">s</span>)<img 
src="litrev1x.png" alt="&#x03A3;s&#x2032;&#x2208;S"  class="" ><img 
src="litrev2x.png" alt="r&#x03A3;&#x2208;R"  class="" ><span 
class="cmmi-12">p</span>(<span 
class="cmmi-12">s</span><span 
class="cmsy-10x-x-120">&#x2032;</span><span 
class="cmmi-12">,r</span><span 
class="cmsy-10x-x-120">|</span><span 
class="cmmi-12">s,a</span>)(<span 
class="cmmi-12">r </span>+ <span 
class="cmmi-12">&#x03B3;V</span> <sub><span 
class="cmmi-8">&#x03C0;</span></sub>(<span 
class="cmmi-12">s</span><span 
class="cmsy-10x-x-120">&#x2032;</span>))
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;1: </span><span  
class="content">The Bellman Equation</span></div><!--tex4ht:label?: x1-2001r1 -->
                                                                          

                                                                          
<!--l. 34--><p class="noindent" ></div><hr class="endfigure">
<!--l. 35--><p class="noindent" >The State-Value function assigns an expected value <span 
class="cmmi-12">E </span>following a policy <span 
class="cmmi-12">&#x03C0; </span>by
adding the reward of taking an action from policy <span 
class="cmmi-12">&#x03C0; </span>in the current state
to the discounted (where <span 
class="cmmi-12">&#x03B3; </span>is our discount factor) reward of the next
state. Throughout the literature review we will unpack and discuss this
equation.<br 
class="newline" /><br 
class="newline" />It is worth defining at this point a few key terms of the domain:
<!--l. 37--><p class="noindent" ><span class="paragraphHead"><a 
 id="x1-30001.1"></a><span 
class="cmbx-12">Agent</span></span>
The aspect of the problem under direct control; the learning and acting entity
that attempts to maximise the reward it receives.
<!--l. 38--><p class="noindent" ><span class="paragraphHead"><a 
 id="x1-40001.1"></a><span 
class="cmbx-12">Action</span></span>
A method of the agent through which it interacts with and changes its
environment.
<!--l. 39--><p class="noindent" ><span class="paragraphHead"><a 
 id="x1-50001.1"></a><span 
class="cmbx-12">Discount Factor </span><span 
class="cmmi-12">&#x03B3;</span></span>
A factor multiplying the future reward; a smaller discount factor signifies future
rewards are less important, and vice versa.
<!--l. 40--><p class="noindent" ><span class="paragraphHead"><a 
 id="x1-60001.1"></a><span 
class="cmbx-12">Environment</span></span>
Everything that is not under direct sovereignty of the agent; everything the agent
can interact with.
<!--l. 41--><p class="noindent" ><span class="paragraphHead"><a 
 id="x1-70001.1"></a><span 
class="cmbx-12">Episode</span></span>
The sequence of states between an initial state and a terminal state; a
play-through.<br 
class="newline" /><br 
class="newline" />It is hard to find a concrete definition of these terms, however all the literature
cited in this review uses these terms in the same way. Other terminology will be
introduced as required. <br 
class="newline" />
<!--l. 44--><p class="noindent" >Reinforcement Learning has a short but active history with two main threads of
origin; animal learning psychology and optimal control problems. Whilst
these two fields largely were largely independent of each other originally,
they eventually converged to form the field of reinforcement learning.
                                                                          

                                                                          
<br 
class="newline" /><br 
class="newline" />As noted by Watkins^<sup class="textsuperscript"><span 
class="cmr-10x-x-109">2</span></sup>, there were mainly two types of experimental procedure
used in animal learning studies; instrumental learning and classical (or Pavlovian
) learning. The former entails reinforcement stimuli depending on the action of the
animal, whereas the latter consists of reinforcers contingent on an event,
regardless of the animals response. Omitting the detail Watkins goes into in his
thesis, the essence is that the function of instrumental learning is to find the
optimal behaviour given certain criteria; a performance critereon which
allows us to judge the efficiency of our behaviour - in an animal this is
decided internally, and in reinforcement learning this is our reward signal <span 
class="cmmi-12">r</span>.
<br 
class="newline" /><br 
class="newline" />Sutton^<sup class="textsuperscript"><span 
class="cmr-10x-x-109">1</span></sup> describes the other origin of reinforcement learning; that of the domain of
optimal control problems. The term &#8217;optimal control&#8217; describes the set of
problems concerning the minimization of some measure of a dynamic systems
behaviour over time. This eventually lead to the derivation of the functional
equation now known as the Bellman equation (seen in Figure <a 
href="#x1-2001r1">1<!--tex4ht:ref: sve --></a>), and the
class of algorithms for solving this equation, now known as &#8217;dynamic
programming&#8217; (although the field of dynamic programming extends far beyond
solving Bellman equations). Bellman at this time also introduced Markov
Decision Processes (MDPs)^<sup class="textsuperscript"><span 
class="cmr-10x-x-109">3</span></sup>, discussed in Section <a 
href="#x1-80001.1.1">1.1.1<!--tex4ht:ref: markov --></a>, all of which became
the major corner stones underlying the modern field of reinforcement
learning.
<h5 class="subsubsectionHead"><span class="titlemark">1.1.1   </span> <a 
 id="x1-80001.1.1"></a>Markov Decision Processes (MDPs)</h5>
<!--l. 49--><p class="noindent" >Markov Decision Processes are a critical tool for modeling the interaction of an
agent and its environment, and as such are an indispensable feature of the field of
reinforcement learning and machine learning in general.<br 
class="newline" /><br 
class="newline" />A sequence of random variables <span 
class="cmmi-12">X</span><sub><span 
class="cmr-8">1</span></sub><span 
class="cmmi-12">,X</span><sub><span 
class="cmr-8">2</span></sub><span 
class="cmmi-12">,...X</span><sub><span 
class="cmmi-8">n</span></sub> is a Markov chain if the conditional
distribution of <span 
class="cmmi-12">X</span><sub><span 
class="cmmi-8">n</span><span 
class="cmr-8">=1</span></sub> given <span 
class="cmmi-12">X</span><sub><span 
class="cmr-8">1</span></sub><span 
class="cmmi-12">,X</span><sub><span 
class="cmr-8">2</span></sub><span 
class="cmmi-12">,...X</span><sub><span 
class="cmmi-8">n</span></sub>, depends on <span 
class="cmmi-12">X</span><sub><span 
class="cmmi-8">n</span></sub> only^<sup class="textsuperscript"><span 
class="cmr-10x-x-109">4</span></sup>. That is, if every
element in the sequence depends on the previous element only to determine its
probability distribution. A system can be considered to have the Markov property
if it satisfies the definition of a Markov chain^<sup class="textsuperscript"><span 
class="cmr-10x-x-109">5</span></sup>. As a probability it can be
denoted:
                                                                          

                                                                          
<div class="center" 
>
<!--l. 52--><p class="noindent" >
<!--l. 53--><p class="noindent" ><span 
class="cmmi-12">P</span>[<span 
class="cmmi-12">S</span><sub><span 
class="cmmi-8">t</span><span 
class="cmr-8">+1</span></sub><span 
class="cmsy-10x-x-120">|</span><span 
class="cmmi-12">S</span><sub><span 
class="cmmi-8">t</span></sub>] = <span 
class="cmmi-12">P</span>[<span 
class="cmmi-12">S</span><sub><span 
class="cmmi-8">t</span><span 
class="cmr-8">+1</span></sub><span 
class="cmsy-10x-x-120">|</span><span 
class="cmmi-12">S</span><sub><span 
class="cmr-8">1</span></sub><span 
class="cmmi-12">,S</span><sub><span 
class="cmr-8">2</span></sub><span 
class="cmmi-12">,...,S</span><sub><span 
class="cmmi-8">t</span></sub>]</div>
<!--l. 55--><p class="noindent" >Which simply states that the probability of a system entering state <span 
class="cmmi-12">S</span><sub><span 
class="cmmi-8">t</span><span 
class="cmr-8">+1</span></sub> given
<span 
class="cmmi-12">S</span><sub><span 
class="cmmi-8">t</span></sub> is the same as the probability given every state up to <span 
class="cmmi-12">S</span><sub><span 
class="cmmi-8">t</span></sub>. With this
set of states <span 
class="cmmi-12">S </span>satisfying the Markov property, we can now construct a
Markov Decision Process, which is defined as a tuple (<span 
class="cmmi-12">S,A,</span><span 
class="cmsy-10x-x-120"><img 
src="cmsy10-c-52.png" alt="R" class="10-120x-x-52" /></span><span 
class="cmmi-12">,P,R,d</span><sub><span 
class="cmr-8">0</span></sub><span 
class="cmmi-12">,&#x03B3;</span>)
where^<sup class="textsuperscript"><span 
class="cmr-10x-x-109">6</span></sup>:
      <ul class="itemize1">
      <li class="itemize"><span 
class="cmmi-12">t </span>denotes the time step, where <span 
class="cmmi-12">t &#x003E;</span> <span 
class="bbold-12">&#x2115;</span><sub><span 
class="cmr-8">0</span></sub>
      </li>
      <li class="itemize"><span 
class="cmmi-12">S </span>denotes the set of possible states an agent can be in, and <span 
class="cmmi-12">s </span>denotes
      an individual state in the set <span 
class="cmmi-12">S</span>.
      </li>
      <li class="itemize"><span 
class="cmmi-12">A </span>denotes the set of possible actions of an agent^<sup class="textsuperscript"><span 
class="cmr-10x-x-109">7</span></sup>
      </li>
      <li class="itemize"><span 
class="cmsy-10x-x-120"><img 
src="cmsy10-c-52.png" alt="R" class="10-120x-x-52" />&#x2286; </span><span 
class="bbold-12">&#x211D; </span>denotes the set of possible rewards an agent can receive
      </li>
      <li class="itemize"><span 
class="cmmi-12">P </span>: <span 
class="cmmi-12">S</span><span 
class="cmsy-10x-x-207">&#x00D7;</span> <span 
class="cmmi-12">A</span><span 
class="cmsy-10x-x-207">&#x00D7;</span> <span 
class="cmmi-12">S </span><span 
class="cmsy-10x-x-120">&#x2192; </span>[0<span 
class="cmmi-12">, </span>1] is the transition function
      </li>
      <li class="itemize"><span 
class="cmmi-12">R </span>denotes the reward function
      </li>
      <li class="itemize"><span 
class="cmmi-12">d</span><sub><span 
class="cmr-8">0</span></sub> denotes the initial state distribution
      </li>
      <li class="itemize"><span 
class="cmmi-12">y </span><span 
class="cmsy-10x-x-120">&#x2208; </span>[0<span 
class="cmmi-12">, </span>1] denotes the reward discount factor</li></ul>
<!--l. 67--><p class="noindent" >The elements of transition function <span 
class="cmmi-12">P </span>are called transition probabilties^<sup class="textsuperscript"><span 
class="cmr-10x-x-109">7</span></sup>. The
transition probability <span 
class="cmmi-12">P</span><sub><span 
class="cmmi-8">i,j</span></sub> is the conditional probability of being in state <span 
class="cmmi-12">s</span><sub><span 
class="cmmi-8">j</span></sub> at
time <span 
class="cmmi-12">t </span>+ 1 given that we are in state <span 
class="cmmi-12">s</span><sub><span 
class="cmmi-8">i</span></sub> at time <span 
class="cmmi-12">t </span>[7]. The conditional
probability of <span 
class="cmmi-12">X</span><sub><span 
class="cmmi-8">n</span><span 
class="cmr-8">+1</span></sub> given <span 
class="cmmi-12">X</span><sub><span 
class="cmmi-8">n</span></sub> (here we use the notation of a Markov chain, but
recall that the states <span 
class="cmmi-12">S </span>observe the Markov property and therefore form a
                                                                          

                                                                          
Markov chain) is the same for all <span 
class="cmmi-12">t </span>[7]. This property is known as time
homogeneity, and in general it is assumed that all Markov chains observe this
property^<sup class="textsuperscript"><span 
class="cmr-10x-x-109">7</span></sup>.<br 
class="newline" /><br 
class="newline" />To exemplify the use of an MDP, here is a well-known learning problem:
<h5 class="subsubsectionHead"><span class="titlemark">1.1.2   </span> <a 
 id="x1-90001.1.2"></a>The Recycling Robot</h5>
<!--l. 70--><p class="noindent" >A robot collects rubbish around an office. At any given time, the robot can either
(a) search for rubbish (b) wait or (c) return to it&#8217;s charging station. The robot is
either in a <span 
class="cmmi-12">high </span>state or <span 
class="cmmi-12">low </span>state, indicating its battery level. The reward it
receives is the number of items of rubbish it collects. The MDP can be
represented as the figure seen below: <hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
<a 
 id="x1-9001r2"></a>
                                                                          

                                                                          
<div class="center" 
>
<!--l. 72--><p class="noindent" >
<!--l. 100--><p class="noindent" ><object data="litrev-1.svg" width="496.82582 " height="166.23398 " type="image/svg+xml"><p>SVG-Viewer needed.</p></object>
</div>
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;2: </span><span  
class="content">an MDP of the recycling robot problem</span></div><!--tex4ht:label?: x1-9001r2 -->
                                                                          

                                                                          
<!--l. 104--><p class="noindent" ></div><hr class="endfigure">
<!--l. 105--><p class="noindent" >&#x00A0;<br 
class="newline" />On each transition, the label reads <tspan font-family="cmmi" font-size="12">action</tspan>(<tspan font-family="cmmi" font-size="12">transition</tspan>_<tspan font-family="cmmi" font-size="12">probability,reward</tspan>). A
<tspan font-family="cmmi" font-size="12">search </tspan>completed from the high state returns the robot to the <tspan font-family="cmmi" font-size="12">high </tspan>state with the
probability <tspan font-family="cmmi" font-size="12">&#x03B1; </tspan>and to the <tspan font-family="cmmi" font-size="12">low </tspan>state with probability 1 <tspan font-family="cmsy" font-size="10">- </tspan><tspan font-family="cmmi" font-size="12">&#x03B1;</tspan>. A <tspan font-family="cmmi" font-size="12">search </tspan>from
the <tspan font-family="cmmi" font-size="12">low </tspan>state returns the robot to the <tspan font-family="cmmi" font-size="12">low </tspan>state with probability <tspan font-family="cmmi" font-size="12">&#x03B2; </tspan>and
depletes the battery with probability 1 <tspan font-family="cmsy" font-size="10">- </tspan><tspan font-family="cmmi" font-size="12">&#x03B2;</tspan>. If the battery is depleted, the
robot must be manually returned to the charging point, thus the reward is
negative (here I have put -5 but the amount is not significant, just enough
so that the robot considers it worthwhile to avoid this outcome). For
the robot to operate effectively, we must have <tspan font-family="cmsy" font-size="10"><img 
src="cmsy10-c-52.png" alt="R" class="10-120x-x-52" /></tspan><sup><tspan font-family="cmmi" font-size="8">search</tspan></sup> <tspan font-family="cmmi" font-size="12">&#x003E; </tspan><tspan font-family="cmsy" font-size="10"><img 
src="cmsy10-c-52.png" alt="R" class="10-120x-x-52" /></tspan><sup><tspan font-family="cmmi" font-size="8">wait</tspan></sup>, else the
robot will simply do nothing. Formally, the MDP above can be considered
as:
      <ul class="itemize1">
      <li class="itemize"><tspan font-family="cmmi" font-size="12">S </tspan>= <tspan font-family="cmsy" font-size="10">{</tspan><tspan font-family="cmmi" font-size="12">high,low</tspan><tspan font-family="cmsy" font-size="10">}</tspan>
      </li>
      <li class="itemize"><tspan font-family="cmmi" font-size="12">A </tspan>= <tspan font-family="cmsy" font-size="10">{</tspan><tspan font-family="cmmi" font-size="12">search,wait,recharge</tspan><tspan font-family="cmsy" font-size="10">}</tspan>
      </li>
      <li class="itemize"><tspan font-family="cmsy" font-size="10"><img 
src="cmsy10-c-52.png" alt="R" class="10-120x-x-52" /> </tspan>= <tspan font-family="cmsy" font-size="10">{<img 
src="cmsy10-c-52.png" alt="R" class="10-120x-x-52" /></tspan><sup><tspan font-family="cmmi" font-size="8">search</tspan></sup><tspan font-family="cmmi" font-size="12">,</tspan><tspan font-family="cmsy" font-size="10"><img 
src="cmsy10-c-52.png" alt="R" class="10-120x-x-52" /></tspan><sup><tspan font-family="cmmi" font-size="8">wait</tspan></sup><tspan font-family="cmmi" font-size="12">, </tspan>0<tspan font-family="cmmi" font-size="12">,</tspan><tspan font-family="cmsy" font-size="10">-</tspan>5<tspan font-family="cmsy" font-size="10">}</tspan></li></ul>
<!--l. 111--><p class="noindent" >And the transition function: <hr class="figure"><div class="figure" 
><a 
 id="x1-9002r3"></a> <!--tex4ht:inline--><div class="tabular"> <table id="TBL-1" class="tabular" 
 
><colgroup id="TBL-1-1g"><col 
id="TBL-1-1"></colgroup><colgroup id="TBL-1-2g"><col 
id="TBL-1-2"></colgroup><colgroup id="TBL-1-3g"><col 
id="TBL-1-3"></colgroup><colgroup id="TBL-1-4g"><col 
id="TBL-1-4"></colgroup><colgroup id="TBL-1-5g"><col 
id="TBL-1-5"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-1-1-"><td  style="white-space:nowrap; text-align:center;" id="TBL-1-1-1"  
class="td11"><tspan font-family="cmmi" font-size="12">s </tspan>= <tspan font-family="cmmi" font-size="12">s</tspan><sup><tspan font-family="cmmi" font-size="8">t</tspan></sup></td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-1-2"  
class="td11">   <tspan font-family="cmmi" font-size="12">A    </tspan></td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-1-3"  
class="td11"><tspan font-family="cmmi" font-size="12">s </tspan>= <tspan font-family="cmmi" font-size="12">s</tspan><sup><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">+1</tspan></sup></td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-1-4"  
class="td11">  <tspan font-family="cmmi" font-size="12">P  </tspan></td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-1-5"  
class="td11">  <tspan font-family="cmsy" font-size="10"><img 
src="cmsy10-c-52.png" alt="R" class="10-120x-x-52" /> </tspan></td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-1-2-"><td  style="white-space:nowrap; text-align:center;" id="TBL-1-2-1"  
class="td11"> high  </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-2-2"  
class="td11"> search </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-2-3"  
class="td11">  high  </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-2-4"  
class="td11">  <tspan font-family="cmmi" font-size="12">&#x03B1;  </tspan></td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-2-5"  
class="td11"><tspan font-family="cmsy" font-size="10"><img 
src="cmsy10-c-52.png" alt="R" class="10-120x-x-52" /></tspan><sup><tspan font-family="cmmi" font-size="8">search</tspan></sup></td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-1-3-"><td  style="white-space:nowrap; text-align:center;" id="TBL-1-3-1"  
class="td11"> high  </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-3-2"  
class="td11"> search </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-3-3"  
class="td11">  low   </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-3-4"  
class="td11">1 <tspan font-family="cmsy" font-size="10">- </tspan><tspan font-family="cmmi" font-size="12">&#x03B1;</tspan></td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-3-5"  
class="td11"><tspan font-family="cmsy" font-size="10"><img 
src="cmsy10-c-52.png" alt="R" class="10-120x-x-52" /></tspan><sup><tspan font-family="cmmi" font-size="8">search</tspan></sup></td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-1-4-"><td  style="white-space:nowrap; text-align:center;" id="TBL-1-4-1"  
class="td11"> low  </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-4-2"  
class="td11"> search </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-4-3"  
class="td11">  low   </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-4-4"  
class="td11">  <tspan font-family="cmmi" font-size="12">&#x03B2;  </tspan></td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-4-5"  
class="td11"><tspan font-family="cmsy" font-size="10"><img 
src="cmsy10-c-52.png" alt="R" class="10-120x-x-52" /></tspan><sup><tspan font-family="cmmi" font-size="8">search</tspan></sup></td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-1-5-"><td  style="white-space:nowrap; text-align:center;" id="TBL-1-5-1"  
class="td11"> low  </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-5-2"  
class="td11"> search </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-5-3"  
class="td11">  high  </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-5-4"  
class="td11">1 <tspan font-family="cmsy" font-size="10">- </tspan><tspan font-family="cmmi" font-size="12">&#x03B2;</tspan></td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-5-5"  
class="td11">  <tspan font-family="cmsy" font-size="10">-</tspan>5   </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-1-6-"><td  style="white-space:nowrap; text-align:center;" id="TBL-1-6-1"  
class="td11"> high  </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-6-2"  
class="td11">  wait   </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-6-3"  
class="td11">  high  </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-6-4"  
class="td11">  1   </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-6-5"  
class="td11"> <tspan font-family="cmsy" font-size="10"><img 
src="cmsy10-c-52.png" alt="R" class="10-120x-x-52" /></tspan><sup><tspan font-family="cmmi" font-size="8">wait</tspan></sup> </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-1-7-"><td  style="white-space:nowrap; text-align:center;" id="TBL-1-7-1"  
class="td11"> high  </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-7-2"  
class="td11">  wait   </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-7-3"  
class="td11">  low   </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-7-4"  
class="td11">  0   </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-7-5"  
class="td11"> <tspan font-family="cmsy" font-size="10"><img 
src="cmsy10-c-52.png" alt="R" class="10-120x-x-52" /></tspan><sup><tspan font-family="cmmi" font-size="8">wait</tspan></sup> </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-1-8-"><td  style="white-space:nowrap; text-align:center;" id="TBL-1-8-1"  
class="td11"> low  </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-8-2"  
class="td11">  wait   </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-8-3"  
class="td11">  low   </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-8-4"  
class="td11">  1   </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-8-5"  
class="td11"> <tspan font-family="cmsy" font-size="10"><img 
src="cmsy10-c-52.png" alt="R" class="10-120x-x-52" /></tspan><sup><tspan font-family="cmmi" font-size="8">wait</tspan></sup> </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-1-9-"><td  style="white-space:nowrap; text-align:center;" id="TBL-1-9-1"  
class="td11"> low  </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-9-2"  
class="td11">  wait   </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-9-3"  
class="td11">  high  </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-9-4"  
class="td11">  0   </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-9-5"  
class="td11"> <tspan font-family="cmsy" font-size="10"><img 
src="cmsy10-c-52.png" alt="R" class="10-120x-x-52" /></tspan><sup><tspan font-family="cmmi" font-size="8">wait</tspan></sup> </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-1-10-"><td  style="white-space:nowrap; text-align:center;" id="TBL-1-10-1"  
class="td11"> high  </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-10-2"  
class="td11">recharge</td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-10-3"  
class="td11">  high  </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-10-4"  
class="td11">  1   </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-10-5"  
class="td11">  0    </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-1-11-"><td  style="white-space:nowrap; text-align:center;" id="TBL-1-11-1"  
class="td11"> high  </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-11-2"  
class="td11">recharge</td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-11-3"  
class="td11">  low   </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-11-4"  
class="td11">  0   </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-11-5"  
class="td11">  0    </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-1-12-"><td  style="white-space:nowrap; text-align:center;" id="TBL-1-12-1"  
class="td11"> low  </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-12-2"  
class="td11">recharge</td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-12-3"  
class="td11">  high  </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-12-4"  
class="td11">  1   </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-12-5"  
class="td11">  0    </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-1-13-"><td  style="white-space:nowrap; text-align:center;" id="TBL-1-13-1"  
class="td11"> low  </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-13-2"  
class="td11">recharge</td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-13-3"  
class="td11">  low   </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-13-4"  
class="td11">  0   </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-13-5"  
class="td11">  0    </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-1-14-"><td  style="white-space:nowrap; text-align:center;" id="TBL-1-14-1"  
class="td11">      </td></tr></table></div>
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;3: </span><span  
class="content">Transition Probabilities and Rewards</span></div><!--tex4ht:label?: x1-9002r3 -->
<!--l. 131--><p class="noindent" ></div><hr class="endfigure">
<!--l. 132--><p class="noindent" >Most learning problems (although there are exceptions) can be modelled as
MDPs, including the game of Go. For problems with continous values, once they
are quantized again they can be represented as an MDP, or as a continuous
MDP.
                                                                          

                                                                          
<h4 class="subsectionHead"><span class="titlemark">1.2   </span> <a 
 id="x1-100001.2"></a>Neural Networks</h4>
<!--l. 134--><p class="noindent" >Neural Networks have gone in and out of fashion since their conception in 1943 by
McCullough and Pitts^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">8</tspan></sup>, enjoying a resurgence in the 1980s , in part due to
Werbos&#8217; 1974 backpropagation algorithm^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">9</tspan></sup>, and once again falling back into
obscurity before an uptake in recent decades through the veil of &#8217;deep
learning&#8217;.<br 
class="newline" /><br 
class="newline" />Artificial Neural Networks (ANNs), usually shortened to Neural Networks (NNs)
are computational systems modelled after the animal brain. The field of NNs has
evolved independently many separate times in many different fields, so it remains
difficult to reach consensus on a canonical formalization. Haykin^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">10</tspan></sup> offers the
following definition: <br 
class="newline" /><br 
class="newline" />
    <tspan font-family="cmti" font-size="12">A neural network is a massively parallel distributed processor made up of</tspan>
    <tspan font-family="cmti" font-size="12">simple processing units that has a natural propensity for storing experiential</tspan>
    <tspan font-family="cmti" font-size="12">knowledge and making it available for use. It resembles the brain in two</tspan>
    <tspan font-family="cmti" font-size="12">respects:</tspan>
          <ol  class="enumerate1" >
          <li 
  class="enumerate" id="x1-10002x1"><tspan font-family="cmti" font-size="12">Knowledge is acquired by the network from its environment through</tspan>
          <tspan font-family="cmti" font-size="12">a learning process.</tspan>
          </li>
          <li 
  class="enumerate" id="x1-10004x2"><tspan font-family="cmti" font-size="12">Interneuron connection strengths, known as synaptic weights, are</tspan>
          <tspan font-family="cmti" font-size="12">used to store the acquired knowledge.</tspan></li></ol>
<!--l. 145--><p class="noindent" >Note that this is a very high level interpretation of a neural network and we will
expand this definition later. This fits in line with our previous idea of the
neural network modelling itself after the brain. The simple processing units
mentioned in this definition are our &#8217;neurons&#8217;, and their connections the
&#8217;synapses&#8217;. The purpose of modelling a brain, intuitively, is to be able
to emulate functions of the brain; namely problem-solving and pattern
recognition. Pattern recognition allows us to generalize - from data we have
seen to data we have not previously encountered. This is the primary
use of neural networks; to provide function approximations for complex,
intractable problems. We can start by formalizing the idea of the neuron:
                                                                          

                                                                          
<br 
class="newline" /><br 
class="newline" />The simplest and earliest model of a neuron is known as the McCulloch&#8211;Pitts
Model^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">8</tspan></sup>, definied in their 1943 paper. It consists of:
      <ol  class="enumerate1" >
      <li 
  class="enumerate" id="x1-10006x1">A set of input <tspan font-family="cmmi" font-size="12">synapses</tspan>, each of which have a weight which signals the
      importance of this synapse.
      </li>
      <li 
  class="enumerate" id="x1-10008x2">A combiner function for summing these input signals.
      </li>
      <li 
  class="enumerate" id="x1-10010x3">An activation function, which decides whether a neuron fires or not.</li></ol>
<!--l. 153--><p class="noindent" ><hr class="figure"><div class="figure" 
><a 
 id="x1-10011r4"></a> <img 
src="2.png" alt="PIC"  
width="57" height="57" >
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;4: </span><span  
class="content">A McCulloch-Pitts Neuron </span></div><!--tex4ht:label?: x1-10011r4 -->
<!--l. 158--><p class="noindent" ></div><hr class="endfigure">
<!--l. 159--><p class="noindent" >Figure <a 
href="#x1-10011r4">4<!--tex4ht:ref: neuron --></a> illustrates a basic neuron of a neural network. We can define our
combiner or aggregator function as : <hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
                                                                          

                                                                          
<!--l. 162--><p class="noindent" ><tspan font-family="cmmi" font-size="12">u</tspan><sub><tspan font-family="cmmi" font-size="8">k</tspan></sub> = <tspan font-family="cmex" font-size="10">&#x2211;</tspan>
  <sub><tspan font-family="cmmi" font-size="8">i</tspan><tspan font-family="cmr" font-size="8">=0</tspan></sub><sup><tspan font-family="cmmi" font-size="8">i</tspan><tspan font-family="cmr" font-size="8">=</tspan><tspan font-family="cmmi" font-size="8">n</tspan></sup><tspan font-family="cmmi" font-size="12">w</tspan><sub>
<tspan font-family="cmmi" font-size="8">i</tspan></sub><tspan font-family="cmmi" font-size="12">x</tspan><sub><tspan font-family="cmmi" font-size="8">i</tspan></sub>
                                                                          

                                                                          
<!--l. 163--><p class="noindent" ></div><hr class="endfigure">
<!--l. 164--><p class="noindent" >Which simply sums the input signals multiplied by their weight. Notice that the
sum of the aggregator function iterates from 0 through n, however the input nodes
in Figure <a 
href="#x1-10011r4">4<!--tex4ht:ref: neuron --></a> range from 1 to n. This is to account for <tspan font-family="cmmi" font-size="12">w</tspan><sub><tspan font-family="cmr" font-size="8">0</tspan></sub>, the bias of our network
(the input <tspan font-family="cmmi" font-size="12">x</tspan><sub><tspan font-family="cmr" font-size="8">0</tspan></sub> is fixed at 1). We can think of the bias as the <tspan font-family="cmmi" font-size="12">c </tspan>if we compare the
function of a neuron to the straight line equation <tspan font-family="cmmi" font-size="12">y </tspan>= <tspan font-family="cmmi" font-size="12">mx </tspan>+ <tspan font-family="cmmi" font-size="12">c</tspan>. Our output <tspan font-family="cmmi" font-size="12">y</tspan><sub><tspan font-family="cmmi" font-size="8">k</tspan></sub> is
defined as: <hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
                                                                          

                                                                          
<!--l. 167--><p class="noindent" ><tspan font-family="cmmi" font-size="12">y</tspan><sub><tspan font-family="cmmi" font-size="8">k</tspan></sub> = <tspan font-family="cmmi" font-size="12">&#x03D5;</tspan>(<tspan font-family="cmmi" font-size="12">u</tspan><sub><tspan font-family="cmmi" font-size="8">k</tspan></sub>)
                                                                          

                                                                          
<!--l. 168--><p class="noindent" ></div><hr class="endfigure">
<!--l. 169--><p class="noindent" >Simply our activation function applied to our input (from the aggregator
function). <br 
class="newline" /><br 
class="newline" />There are many different activation functions. Referring back to our definition of a
neural network, we know that the purpose of NNs is to approximate functions,
which fundamentally come in two different formats; linear and non-linear. If we
have no activation function; that is, if the signal passes through neurons
unchanged, then the network effectively becomes a linear regression model, only
capable of solving linear equations. To solve any more complex problem, we need
to introduce non-linearity; an activation function. The type of activation function
we choose depends on the problem we are trying to model. Here are a few of the
most basic activation functions:
<!--l. 171--><p class="noindent" ><span class="paragraphHead"><a 
 id="x1-110001.2"></a><tspan font-family="cmbx" font-size="12">Threshold Function</tspan></span>
A boolean function that distinguishes ranges of values above a certain
&#8217;threshold&#8217;^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">11</tspan></sup>. This satisfies the <tspan font-family="cmmi" font-size="12">all </tspan><tspan font-family="cmsy" font-size="10">-</tspan><tspan font-family="cmmi" font-size="12">or </tspan><tspan font-family="cmsy" font-size="10">-</tspan><tspan font-family="cmmi" font-size="12">none </tspan>property of neurons as describes in
McCulloch&#8217;s and Pitt&#8217;s paper^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">8</tspan></sup>; nerve cells will give the maximum response or
none at all. This could be used for yes-no type problems, although not multi-class
classification problems as by its very nature it can only distinguish between two
classes.
<!--l. 172--><p class="noindent" ><span class="paragraphHead"><a 
 id="x1-120001.2"></a><tspan font-family="cmbx" font-size="12">Sigmoid Function</tspan></span>
Another commonly used activation function, otherwise known as the standard
logistic function in the field of ANNs^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">12</tspan></sup>; It provides a return value in the range 0
to 1, with larger inputs resulting in a value closer to 1, thus being a useful
activation function for problems requiring a probability as output. It is defined as
follows^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">12</tspan></sup>: <hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
                                                                          

                                                                          
<!--l. 175--><p class="noindent" ><tspan font-family="cmmi" font-size="12">S</tspan>(<tspan font-family="cmmi" font-size="12">x</tspan>) = <img 
src="litrev3x.png" alt="--1---
1+e-ax"  class="frac" align="middle">
                                                                          

                                                                          
<!--l. 176--><p class="noindent" ></div><hr class="endfigure">
<!--l. 177--><p class="noindent" >Where <tspan font-family="cmmi" font-size="12">a </tspan>is an optional <tspan font-family="cmti" font-size="12">slope parameter </tspan>that dictates the gradient. Sometimes,
stochastic models of neurons can be useful: <hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
                                                                          

                                                                          
<!--l. 180--><p class="noindent" ><tspan font-family="cmmi" font-size="12">S</tspan>(<tspan font-family="cmmi" font-size="12">x</tspan>) = <img 
src="litrev4x.png" alt="---1---
1+e-x&#x2215;T"  class="frac" align="middle">
                                                                          

                                                                          
<!--l. 182--><p class="noindent" ></div><hr class="endfigure">
<!--l. 183--><p class="noindent" >Where <tspan font-family="cmmi" font-size="12">T </tspan>is a &#8217;temperature&#8217; that represents noise (randomness) in the
network^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">12</tspan></sup><sup><tspan font-family="cmmi" font-size="8">,</tspan></sup>^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">10</tspan></sup><sup><tspan font-family="cmmi" font-size="8">,</tspan></sup>^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">13</tspan></sup>. This noise level parameter is an emulation of the actual noise
that occurs in a biological neural network^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">13</tspan></sup>. This is where we depart from the
simple McCulloch&#8211;Pitts neuron model; Whilst the McCulloch&#8211;Pitts model was
deterministic in its output, for cases where we want to model partially random
behaviour, for example if we wanted to emulate human responses to some stimuli
like the Rubin&#8217;s vase^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">14</tspan></sup>. Admittedly the applications for stochastic neural nets are
few and far between. <br 
class="newline" /><br 
class="newline" />Having covered the basic elements of a neuron, we can now discuss NN
architectures. While there are dozens of types of NN architectures, there are two
basic fundamental types.
<h5 class="subsubsectionHead"><span class="titlemark">1.2.1   </span> <a 
 id="x1-130001.2.1"></a>Feedforward Networks</h5>
<!--l. 186--><p class="noindent" >Feedforward networks were the first type of ANN, the simplest and earliest of
which was Rosenblatt&#8217;s Perceptron^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">15</tspan></sup>; a single neuron (of the McCulloch-Pitts
model) neural network with the aforementioned threshold function (more
specifically the Heaviside step function^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">16</tspan></sup>) as its activation function. Feedforward
networks are named as such because information in the input layer is &#8217;fed forward&#8217;
through all the layers until reaching the output layer, without any loops or
feedback connections (i.e. outputs from neurons are not fed back into the neural
network). The simplest type of feedforward network has no hidden layers, and can
be seen below: <hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
<a 
 id="x1-13001r5"></a>
                                                                          

                                                                          
<!--l. 189--><p class="noindent" ><img 
src="3.png" alt="PIC"  
width="50" height="50" >
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;5: </span><span  
class="content">Single-Layered Feedforward Network</span></div><!--tex4ht:label?: x1-13001r5 -->
                                                                          

                                                                          
<!--l. 192--><p class="noindent" ></div><hr class="endfigure">
<!--l. 193--><p class="noindent" >Single-Layered Networks as seen in Figure <a 
href="#x1-13001r5">5<!--tex4ht:ref: singlelayer --></a> consist of a single input layer and
a single output layer; it is considered single-layered as there is only a
single computational layer (no computation is performed in the input
layer). Data flows directly from the left to the right, ergo &#8217;feedforward&#8217;.
<br 
class="newline" /><br 
class="newline" />Note here that since every neuron in the output layer is connected to every node
in the input layer, the network is said to be fully connected, or densely
connected^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">10</tspan></sup>. Similarly, if a layer is fully connected to the previous layer, this
is a fully connected layer. Fully connected layers are what we generally
see in neural networks, and they are for general purpose learning; no
neuron within a layer is differentiated from one another. As a consequence
however, the linear combination each neuron must perform of its inputs
is larger, and as such the computational requirements are heavier. The
inverse of this, namely partially connected layers, is generally only used in a
convolutional layer of a CNN (Convolutional Neural Network) as these partial
connections are particularly useful for feature recognition in image processing
tasks^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">17</tspan></sup>.<br 
class="newline" /><br 
class="newline" />Certain problems may require multiple transformations of the data to reach a
desired output. Crudely speaking, hidden layers allow us to solve &#8217;higher-order
statistical problems&#8217;^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">18</tspan></sup>.<br 
class="newline" /><br 
class="newline" />With a single computational layer, we are only able to answer questions such as
&#8217;Does A correlate with B?&#8217;; that is, questions that can be solved with a single
non-linear function application. For questions of any more complexity than this,
such as &#8217;How do A,B,C,D correlate?&#8217;, hidden layers give the network the &#8217;global
perspective&#8217; to be able to answer these^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">18</tspan></sup>.<br 
class="newline" /><br 
class="newline" />To elucidate this matter further with an example; suppose that we want to
use a NN to calulate a logical XOR of our set of inputs, and allow that
activation functions in our network can only be simple logical operators.
Using simple operators, we cannot compute the XOR of a set of inputs in
a single layer, however if we allow for an additional hidden layer, this
computation becomes possible. Assuming an interpretation of XOR as below:
<hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
                                                                          

                                                                          
<!--l. 198--><p class="noindent" >
<div class="math-display" >
<img 
src="litrev5x.png" alt="        {
         1   iff  one or more, but not all, of x1 ...xn &#x2208; &#x20D7;x is 1
f (&#x20D7;x ) =  0   otherwise
" class="math-display" ></div>
                                                                          

                                                                          
<!--l. 203--><p class="noindent" ></div><hr class="endfigure">
<!--l. 204--><p class="noindent" >As opposed to the more standard two input version, see the figure below for a
neural net to compute this function: <hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
<a 
 id="x1-13002r6"></a>
                                                                          

                                                                          
<!--l. 207--><p class="noindent" ><img 
src="4.png" alt="PIC"  
width="57" height="57" >
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;6: </span><span  
class="content">Multi-Layered Feedforward Neural Network</span></div><!--tex4ht:label?: x1-13002r6 -->
                                                                          

                                                                          
<!--l. 210--><p class="noindent" ></div><hr class="endfigure">
<!--l. 211--><p class="noindent" >Figure <a 
href="#x1-13002r6">6<!--tex4ht:ref: multilayer --></a> shows us that adding a hidden layer enables us to transform our input in
a helpful way for our output; with OR and NAND as the activation functions of
the respective hidden layer neurons, our output layer is able to make use
of &#8217;higher-order&#8217; statistics that would not be available in a single-layer
network.<br 
class="newline" /><br 
class="newline" />Whereas input and output layer neuron counts are easily determined; the number
of input nodes must match the shape of the data, and the number of output nodes
is determined by the answer desired, configuration of hidden layers is a highly
contentious subject with no academic consensus on any formal rules for
determining the parameters of hidden layers (such as number of nodes and
number of layers). However there are some general rules of thumb to go by: few
problems benefit from more than one hidden layer, and the number of
nodes is almost always between your input size and your output size^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">19</tspan></sup>.
This is further supported by the work of Cybenko^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">20</tspan></sup> and Hornik^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">21</tspan></sup>, who
jointly proved the universal approximation theorem, which states that a
feed-forward neural network with a single hidden layer and a finite number of
neurons can approximate any continuous function (note however, that
this does not imply that it would be <tspan font-family="cmti" font-size="12">easy </tspan>for the NN to actually learn
something). Techniques are available, namely Pruning algorithms, for optimizing
network structure, a in-depth survey of which can be found by Reed^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">22</tspan></sup>.
<br 
class="newline" /><br 
class="newline" />In light of these new concepts, we can now fully understand a more formal
definition of a neural network, one which is generally accepted in the
field and has been cited or paraphrased many times^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">23</tspan></sup><sup><tspan font-family="cmmi" font-size="8">,</tspan></sup>^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">24</tspan></sup><sup><tspan font-family="cmmi" font-size="8">,</tspan></sup>^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">25</tspan></sup>, and reads as
follows:<br 
class="newline" />
    <tspan font-family="cmti" font-size="12">A  neural  network  is  a  parallel,  distributed  information  processing</tspan>
    <tspan font-family="cmti" font-size="12">structure consisting of processing elements (which can possess a local</tspan>
    <tspan font-family="cmti" font-size="12">memory and can carry out localized information processing operations)</tspan>
    <tspan font-family="cmti" font-size="12">interconnected  together  with  unidirectional  signal  channels  called</tspan>
    <tspan font-family="cmti" font-size="12">connections. Each processing element has a single output connection</tspan>
    <tspan font-family="cmti" font-size="12">which  branches  (&#8221;fans  out&#8221;)  into  as  many  collateral  connections  as</tspan>
    <tspan font-family="cmti" font-size="12">desired (each carrying the same signal - the processing element output</tspan>
    <tspan font-family="cmti" font-size="12">signal). The processing element output signal can be of any mathematical</tspan>
                                                                          

                                                                          
    <tspan font-family="cmti" font-size="12">type desired. All of the processing that goes on within each processing</tspan>
    <tspan font-family="cmti" font-size="12">elemeni must be completely local: i.e., it must depend only upon the</tspan>
    <tspan font-family="cmti" font-size="12">current values of the input signals arriving at the processing element via</tspan>
    <tspan font-family="cmti" font-size="12">impinging connections and upon values stored in the processing element&#8217;s</tspan>
    <tspan font-family="cmti" font-size="12">local memory.</tspan><br 
class="newline" />
<!--l. 222--><p class="noindent" >In this definition we can identify previously defined concepts such as activation
functions, fully or partially connected layers, weights and linear combinator
functions.
<h5 class="subsubsectionHead"><span class="titlemark">1.2.2   </span> <a 
 id="x1-140001.2.2"></a>Backpropagation</h5>
<!--l. 224--><p class="noindent" >Backpropagation, commonly credited to Rumelhart in 1986^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">26</tspan></sup> although earlier
incarnations discovered independently by Bryson &amp; Ho^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">27</tspan></sup>, Werbos^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">9</tspan></sup> and
Parker^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">28</tspan></sup> existed and were acknowledged, was a major stepping stone in the
development of the field. Research stagnated in 1969 after Minsky and
Papert^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">16</tspan></sup> discovered two key problems with the NNs of the time; basic
perceptrons were not able to process XOR circuits (as illustrated above
in Figure <a 
href="#x1-13002r6">6<!--tex4ht:ref: multilayer --></a>), and computational requirements of larger neural nets far
exceeded the capabilities of contemporary machines. Backpropagation was
the fix enabling practical multilayered networks, which were the turning
point in NN research enabling the learning of complex multidimensional
mappings - hence Werbos&#8217;s description of backpropagation as going &#8216;Beyond
Regression&#8217;^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">9</tspan></sup>. Whilst initially developed as a supervised learning technique,
backpropagation has become the workhorse of reinforcement learning too, as
the same principle applies to learning reward functions and calculating
error.<br 
class="newline" /><br 
class="newline" />In essence, backpropagation minimizes the error function for a given network by
gradient descent.<br 
class="newline" /><br 
class="newline" />In the context of supervised learning; let <tspan font-family="cmmi" font-size="12">f</tspan><sub><tspan font-family="cmmi" font-size="8">&#x03B8;</tspan></sub>(<img 
src="litrev6x.png" alt="&#x20D7;x"  class="vec" >) be the output calculated by a
given ANN, where <tspan font-family="cmmi" font-size="12">&#x03B8; </tspan>is the set of parameters for the network (parameters being
the weights and biases of the individual nodes within the network). The error
function can be defined as some <tspan font-family="cmmi" font-size="12">E</tspan>(<tspan font-family="cmmi" font-size="12">X,&#x03B8;</tspan>) where <tspan font-family="cmmi" font-size="12">X </tspan>= <tspan font-family="cmsy" font-size="10">{</tspan>(<img 
src="litrev7x.png" alt="&#x20D7;x"  class="vec" ><sub><tspan font-family="cmr" font-size="8">1</tspan></sub><tspan font-family="cmmi" font-size="12">,</tspan><img 
src="litrev8x.png" alt="&#x20D7;y"  class="vec" ><sub><tspan font-family="cmr" font-size="8">1</tspan></sub>)<tspan font-family="cmmi" font-size="12">,</tspan><tspan font-family="cmmi" font-size="12">&#x2026;</tspan><tspan font-family="cmmi" font-size="12">, </tspan>(<img 
src="litrev9x.png" alt="&#x20D7;x"  class="vec" ><sub><tspan font-family="cmmi" font-size="8">n</tspan></sub><tspan font-family="cmmi" font-size="12">,</tspan><img 
src="litrev10x.png" alt="&#x20D7;y"  class="vec" ><sub><tspan font-family="cmmi" font-size="8">n</tspan></sub>)<tspan font-family="cmsy" font-size="10">} </tspan>is the
set of input-output pairs of the network (i.e. expected output from a given input),
such that <tspan font-family="cmmi" font-size="12">E</tspan>(<tspan font-family="cmmi" font-size="12">X,&#x03B8;</tspan>) is small when <tspan font-family="cmmi" font-size="12">f</tspan><sub><tspan font-family="cmmi" font-size="8">&#x03B8;</tspan></sub>(<img 
src="litrev11x.png" alt="&#x20D7;x"  class="vec" ><sub><tspan font-family="cmmi" font-size="8">i</tspan></sub>) <tspan font-family="cmsy" font-size="10">&#x2248; </tspan><tspan font-family="cmmi" font-size="12">y</tspan><sub><tspan font-family="cmmi" font-size="8">i</tspan></sub>^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">24</tspan></sup>. A typically used error
                                                                          

                                                                          
function is mean-squared error or MSE, which can be defined as follows^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">29</tspan></sup>:
<hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
<a 
 id="x1-14001r7"></a>
                                                                          

                                                                          
<!--l. 231--><p class="noindent" ><tspan font-family="cmmi" font-size="12">MSE </tspan><tspan font-family="cmsy" font-size="10">&#x2261;</tspan><img 
src="litrev12x.png" alt="1
n"  class="frac" align="middle"> <tspan font-family="cmex" font-size="10">&#x2211;</tspan>
   <sub><tspan font-family="cmmi" font-size="8">i</tspan><tspan font-family="cmr" font-size="8">=1</tspan></sub><sup><tspan font-family="cmmi" font-size="8">i</tspan><tspan font-family="cmr" font-size="8">=</tspan><tspan font-family="cmmi" font-size="8">n</tspan></sup>(<tspan font-family="cmmi" font-size="12">y</tspan><sub>
<tspan font-family="cmr" font-size="8">1</tspan></sub> <tspan font-family="cmsy" font-size="10">- </tspan><tspan font-family="cmmi" font-size="12">f</tspan><sub><tspan font-family="cmmi" font-size="8">&#x03B8;</tspan></sub>(<img 
src="litrev13x.png" alt="&#x20D7;x"  class="vec" >))<sup><tspan font-family="cmr" font-size="8">2</tspan></sup>
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;7: </span><span  
class="content">Mean-Squared Error</span></div><!--tex4ht:label?: x1-14001r7 -->
                                                                          

                                                                          
<!--l. 234--><p class="noindent" ></div><hr class="endfigure">
<!--l. 235--><p class="noindent" >Or in English; the mean of the sum of squares of all the errors, where error is
the difference between the expected value of the network and the actual
value.<br 
class="newline" /><br 
class="newline" />The intuition behind backpropagation is that we want to minimize the error
function (also known as the cost function) by nudging the values of the weights
and biases of the network in the direction that most efficiently minimizes the error
function. This can loosely be analogized to Hebbian theory^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">30</tspan></sup>, which is often
summed up as &#8217;Neurons that fire together, wire together&#8216;^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">31</tspan></sup>; we want to connect
more strongly those processing units (neurons) that fire when receiving an
input to those that <tspan font-family="cmti" font-size="12">should </tspan>fire on the expected output for that input.
<br 
class="newline" /><br 
class="newline" /><tspan font-family="cmti" font-size="12">Note : The derivation of the backpropagation algorithm on the following pages has</tspan>
<tspan font-family="cmti" font-size="12">been done with reference to papers by Hecht-Nielsen</tspan>^<sup class="textsuperscript"><tspan font-family="cmti" font-size="10">24</tspan></sup><tspan font-family="cmti" font-size="12">, Rumelhart, David</tspan>
<tspan font-family="cmti" font-size="12">E and Durbin, Richard and Golden, Richard and Chauvin, Yves</tspan>^<sup class="textsuperscript"><tspan font-family="cmti" font-size="10">32</tspan></sup> <tspan font-family="cmti" font-size="12">and</tspan>
<tspan font-family="cmti" font-size="12">Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J</tspan>^<sup class="textsuperscript"><tspan font-family="cmti" font-size="10">26</tspan></sup><tspan font-family="cmti" font-size="12">.</tspan>
<hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
<a 
 id="x1-14002r8"></a>
                                                                          

                                                                          
<!--l. 241--><p class="noindent" ><img 
src="5.png" alt="PIC"  
width="65" height="65" >
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;8: </span><span  
class="content">A <tspan font-family="cmmi" font-size="12">k </tspan>+ 2 layered perceptron</span></div><!--tex4ht:label?: x1-14002r8 -->
                                                                          

                                                                          
<!--l. 244--><p class="noindent" ></div><hr class="endfigure">
<!--l. 245--><p class="noindent" >More formally and with reference to Figure <a 
href="#x1-14002r8">8<!--tex4ht:ref: backprop --></a>, We denote the layers of the hidden
network <tspan font-family="cmmi" font-size="12">L </tspan>where <tspan font-family="cmmi" font-size="12">L</tspan><sub><tspan font-family="cmmi" font-size="8">k</tspan></sub> is the output layer of the network, and denote the
weight of node <tspan font-family="cmmi" font-size="12">j </tspan>in layer <tspan font-family="cmmi" font-size="12">L</tspan><sub><tspan font-family="cmmi" font-size="8">k</tspan></sub> for incoming node <tspan font-family="cmmi" font-size="12">i </tspan>in layer <tspan font-family="cmmi" font-size="12">L</tspan><sub><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmsy" font-size="8">-</tspan><tspan font-family="cmr" font-size="8">1</tspan></sub> as <tspan font-family="cmmi" font-size="12">w</tspan><sub><tspan font-family="cmmi" font-size="8">ij</tspan></sub><sup><tspan font-family="cmmi" font-size="8">k</tspan></sup>.
Assume some squashing sigmoid function <tspan font-family="cmmi" font-size="12">&#x03C3; </tspan>as our activation function, and
assume a single-output neural network for the purposes of this proof.
<br 
class="newline" /><br 
class="newline" />The output <tspan font-family="cmmi" font-size="12">a</tspan><sub><tspan font-family="cmmi" font-size="8">i</tspan></sub><sup><tspan font-family="cmmi" font-size="8">k</tspan></sup> for node <tspan font-family="cmmi" font-size="12">i </tspan>in layer <tspan font-family="cmmi" font-size="12">L</tspan><sub>
<tspan font-family="cmmi" font-size="8">k</tspan></sub> is defined as: <br 
class="newline" /><br 
class="newline" /><hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
                                                                          

                                                                          
<!--l. 249--><p class="noindent" ><tspan font-family="cmmi" font-size="12">a</tspan><sub><tspan font-family="cmmi" font-size="8">i</tspan></sub><sup><tspan font-family="cmmi" font-size="8">k</tspan></sup> = <tspan font-family="cmmi" font-size="12">&#x03C3;</tspan>(<tspan font-family="cmex" font-size="10">&#x2211;</tspan><sub>
<tspan font-family="cmmi" font-size="8">j</tspan><tspan font-family="cmr" font-size="8">=0</tspan></sub><sup><tspan font-family="cmmi" font-size="8">j</tspan><tspan font-family="cmr" font-size="8">=</tspan><tspan font-family="cmmi" font-size="8">n</tspan><sub><tspan font-family="cmmi" font-size="6">L</tspan><tspan font-family="cmsy" font-size="6">-</tspan><tspan font-family="cmr" font-size="6">1</tspan></sub></sup><tspan font-family="cmmi" font-size="12">w</tspan><sub>
<tspan font-family="cmmi" font-size="8">ji</tspan></sub><sup><tspan font-family="cmmi" font-size="8">k</tspan></sup><tspan font-family="cmmi" font-size="12">a</tspan><sub>
<tspan font-family="cmmi" font-size="8">j</tspan></sub><sup><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmsy" font-size="8">-</tspan><tspan font-family="cmr" font-size="8">1</tspan></sup>)
                                                                          

                                                                          
<!--l. 250--><p class="noindent" ></div><hr class="endfigure">
<!--l. 251--><p class="noindent" >Or in English; the sum of the weights multiplied by the activations of the
incoming nodes from the previous layer (<tspan font-family="cmmi" font-size="12">n</tspan><sub><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmsy" font-size="8">-</tspan><tspan font-family="cmr" font-size="8">1</tspan></sub> represents the number of nodes in
layer <tspan font-family="cmmi" font-size="12">L</tspan><sub><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmsy" font-size="8">-</tspan><tspan font-family="cmr" font-size="8">1</tspan></sub>), all squashed by <tspan font-family="cmmi" font-size="12">&#x03C3;</tspan>. It will be helpful to denote the weighted sum that
<tspan font-family="cmmi" font-size="12">&#x03C3; </tspan>is applied to as <tspan font-family="cmmi" font-size="12">z</tspan><sub><tspan font-family="cmmi" font-size="8">i</tspan></sub><sup><tspan font-family="cmmi" font-size="8">k</tspan></sup>. Note that the bias in this case for <tspan font-family="cmmi" font-size="12">a</tspan><sub>
<tspan font-family="cmmi" font-size="8">i</tspan></sub><sup><tspan font-family="cmmi" font-size="8">k</tspan></sup> is the special weight
<tspan font-family="cmmi" font-size="12">w</tspan><sub><tspan font-family="cmr" font-size="8">0</tspan><tspan font-family="cmmi" font-size="8">i</tspan></sub><sup><tspan font-family="cmmi" font-size="8">k</tspan></sup>, and <tspan font-family="cmmi" font-size="12">a</tspan><sub>
<tspan font-family="cmr" font-size="8">0</tspan></sub><sup><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmsy" font-size="8">-</tspan><tspan font-family="cmr" font-size="8">1</tspan></sup> = 1. Recall the previous definition of the error function <tspan font-family="cmmi" font-size="12">E </tspan>as the
<tspan font-family="cmmi" font-size="12">MSE </tspan>(see Figure <a 
href="#x1-14001r7">7<!--tex4ht:ref: mse --></a>), and recall that that the purpose of backpropagation is to
minimize <tspan font-family="cmmi" font-size="12">E</tspan>. This is done by solving the derivative <img 
src="litrev14x.png" alt="&#x03B4;&#x03B4;wEk--
  ij"  class="frac" align="middle"> for all <tspan font-family="cmmi" font-size="12">w</tspan><sub><tspan font-family="cmmi" font-size="8">ij</tspan></sub><sup><tspan font-family="cmmi" font-size="8">k</tspan></sup>, or more
formally: <hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
<a 
 id="x1-14003r9"></a>
                                                                          

                                                                          
<!--l. 254--><p class="noindent" ><tspan font-family="cmmi" font-size="12">&#x03B4;E</tspan>(<tspan font-family="cmmi" font-size="12">X,&#x03B8;</tspan>)_________
   <tspan font-family="cmmi" font-size="12">&#x03B4;w</tspan><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmmi" font-size="8">ij</tspan>   = 1 
<tspan font-family="cmmi" font-size="12">n</tspan> <tspan font-family="cmmi" font-size="8">m</tspan><tspan font-family="cmr" font-size="8">=</tspan><tspan font-family="cmmi" font-size="8">n</tspan>
     <tspan font-family="cmex" font-size="10">&#x2211;</tspan>
  <tspan font-family="cmmi" font-size="8">m</tspan><tspan font-family="cmr" font-size="8">=0</tspan>    <tspan font-family="cmmi" font-size="12">&#x03B4;</tspan> __
<tspan font-family="cmmi" font-size="12">&#x03B4;w</tspan><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmmi" font-size="8">ij</tspan> (<tspan font-family="cmmi" font-size="12">y</tspan><tspan font-family="cmmi" font-size="8">d</tspan> <tspan font-family="cmsy" font-size="10">- </tspan><tspan font-family="cmmi" font-size="12">f</tspan><tspan font-family="cmmi" font-size="8">&#x03B8;</tspan>(<tspan font-family="cmmi" font-size="12">&#x20D7;</tspan><tspan font-family="cmmi" font-size="12">x</tspan>)<tspan font-family="cmmi" font-size="8">d</tspan>)<tspan font-family="cmr" font-size="8">2</tspan> = <tspan font-family="cmmi" font-size="8">m</tspan><tspan font-family="cmr" font-size="8">=</tspan><tspan font-family="cmmi" font-size="8">n</tspan>
 <tspan font-family="cmex" font-size="10">&#x2211;</tspan>
 <tspan font-family="cmmi" font-size="8">m</tspan><tspan font-family="cmr" font-size="8">=0</tspan>  <tspan font-family="cmmi" font-size="12">&#x03B4;E</tspan><tspan font-family="cmmi" font-size="8">d</tspan> 
<tspan font-family="cmmi" font-size="12">&#x03B4;w</tspan><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmmi" font-size="8">ij</tspan>
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;9: </span><span  
class="content">Equation 1</span></div><!--tex4ht:label?: x1-14003r9 -->
                                                                          

                                                                          
<!--l. 258--><p class="noindent" ></div><hr class="endfigure">
<!--l. 259--><p class="noindent" >Since the derivative of a sum of functions is the sum of the derivative of each
function (Sum rule), and the error function <tspan font-family="cmmi" font-size="12">E</tspan>(<tspan font-family="cmmi" font-size="12">X,&#x03B8;</tspan>) is a sum itself, we can explain
backpropagation with respect to one error term (i.e. one input-output pair) and
combine them after the fact. Thus we will look at <tspan font-family="cmmi" font-size="12">E </tspan>= (<tspan font-family="cmmi" font-size="12">y </tspan><tspan font-family="cmsy" font-size="10">- </tspan><tspan font-family="cmmi" font-size="12">f</tspan><sub><tspan font-family="cmmi" font-size="8">&#x03B8;</tspan></sub>(<img 
src="litrev15x.png" alt="&#x20D7;x"  class="vec" >))<sup><tspan font-family="cmr" font-size="8">2</tspan></sup> as our error
function, omitting subscript <tspan font-family="cmmi" font-size="12">d </tspan>for simplicity. According to the chain rule :
<hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
                                                                          

                                                                          
<!--l. 263--><p class="noindent" > <tspan font-family="cmmi" font-size="12">&#x03B4;E</tspan>_
<tspan font-family="cmmi" font-size="12">&#x03B4;w</tspan><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmmi" font-size="8">ij</tspan> = <tspan font-family="cmmi" font-size="12">&#x03B4;E</tspan> 
<tspan font-family="cmmi" font-size="12">&#x03B4;a</tspan><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmmi" font-size="8">j</tspan> <tspan font-family="cmmi" font-size="12">&#x03B4;a</tspan><tspan font-family="cmmi" font-size="8">k</tspan>
<tspan font-family="cmmi" font-size="8">j</tspan> _
<tspan font-family="cmmi" font-size="12">&#x03B4;z</tspan><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmmi" font-size="8">j</tspan>  <tspan font-family="cmmi" font-size="12">&#x03B4;z</tspan><tspan font-family="cmmi" font-size="8">k</tspan>
<tspan font-family="cmmi" font-size="8">j</tspan> _
<tspan font-family="cmmi" font-size="12">&#x03B4;w</tspan><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmmi" font-size="8">ij</tspan>
                                                                          

                                                                          
<!--l. 265--><p class="noindent" ></div><hr class="endfigure">
<!--l. 266--><p class="noindent" >Note that <tspan font-family="cmmi" font-size="12">E </tspan>= (<tspan font-family="cmmi" font-size="12">y </tspan><tspan font-family="cmsy" font-size="10">- </tspan><tspan font-family="cmmi" font-size="12">f</tspan><sub><tspan font-family="cmmi" font-size="8">&#x03B8;</tspan></sub>(<img 
src="litrev16x.png" alt="&#x20D7;x"  class="vec" >))<sup><tspan font-family="cmr" font-size="8">2</tspan></sup> is equivalent to <tspan font-family="cmmi" font-size="12">E </tspan>= (<tspan font-family="cmmi" font-size="12">y </tspan><tspan font-family="cmsy" font-size="10">- </tspan><tspan font-family="cmmi" font-size="12">a</tspan><sub>
<tspan font-family="cmr" font-size="8">1</tspan></sub><sup><tspan font-family="cmmi" font-size="8">m</tspan></sup>)<sup><tspan font-family="cmr" font-size="8">2</tspan></sup> where <tspan font-family="cmmi" font-size="12">m </tspan>is our final
layer, as we have a single output node. Denote <img 
src="litrev17x.png" alt="&#x03B4;Ek-
&#x03B4;aj"  class="frac" align="middle"><img 
src="litrev18x.png" alt=" k
&#x03B4;ajk-
&#x03B4;zj"  class="frac" align="middle"> as the error term <tspan font-family="cmmi" font-size="12">&#x03B4;</tspan><sub><tspan font-family="cmmi" font-size="8">j</tspan></sub><sup><tspan font-family="cmmi" font-size="8">k</tspan></sup>. For
the rightmost term of the above function, only one term in the sum <tspan font-family="cmmi" font-size="12">z</tspan><sub><tspan font-family="cmmi" font-size="8">j</tspan></sub><sup><tspan font-family="cmmi" font-size="8">k</tspan></sup> depends
on <tspan font-family="cmmi" font-size="12">w</tspan><sub><tspan font-family="cmmi" font-size="8">ij</tspan></sub><sup><tspan font-family="cmmi" font-size="8">k</tspan></sup>, so we have: <hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
                                                                          

                                                                          
<!--l. 269--><p class="noindent" > <tspan font-family="cmmi" font-size="12">&#x03B4;z</tspan><tspan font-family="cmmi" font-size="8">k</tspan>
<tspan font-family="cmmi" font-size="8">j</tspan>_
<tspan font-family="cmmi" font-size="12">&#x03B4;w</tspan><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmmi" font-size="8">ij</tspan> =   <tspan font-family="cmmi" font-size="12">&#x03B4;</tspan> __
<tspan font-family="cmmi" font-size="12">&#x03B4;w</tspan><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmmi" font-size="8">ij</tspan> (<tspan font-family="cmmi" font-size="8">l</tspan><tspan font-family="cmr" font-size="8">=</tspan><tspan font-family="cmmi" font-size="8">n</tspan><tspan font-family="cmmi" font-size="6">L</tspan><tspan font-family="cmsy" font-size="6">-</tspan><tspan font-family="cmr" font-size="6">1</tspan>
  <tspan font-family="cmex" font-size="10">&#x2211;</tspan>
 <tspan font-family="cmmi" font-size="8">l</tspan><tspan font-family="cmr" font-size="8">=0</tspan>    <tspan font-family="cmmi" font-size="12">w</tspan><tspan font-family="cmmi" font-size="8">k</tspan>
<tspan font-family="cmmi" font-size="8">lj</tspan><tspan font-family="cmmi" font-size="12">a</tspan><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmsy" font-size="8">-</tspan><tspan font-family="cmr" font-size="8">1</tspan>
<tspan font-family="cmmi" font-size="8">l</tspan>    ) = <tspan font-family="cmmi" font-size="12">a</tspan><sub><tspan font-family="cmmi" font-size="8">i</tspan></sub><sup><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmsy" font-size="8">-</tspan><tspan font-family="cmr" font-size="8">1</tspan></sup>
                                                                          

                                                                          
<!--l. 270--><p class="noindent" ></div><hr class="endfigure">
<!--l. 271--><p class="noindent" >And thus: <hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
<a 
 id="x1-14004r10"></a>
                                                                          

                                                                          
<!--l. 274--><p class="noindent" > <tspan font-family="cmmi" font-size="12">&#x03B4;E</tspan><tspan font-family="cmmi" font-size="8">d</tspan>
<tspan font-family="cmmi" font-size="12">&#x03B4;w</tspan><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmmi" font-size="8">ij</tspan> = <tspan font-family="cmmi" font-size="12">&#x03B4;</tspan><tspan font-family="cmmi" font-size="8">k</tspan>
<tspan font-family="cmmi" font-size="8">j</tspan> <tspan font-family="cmmi" font-size="12">a</tspan><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmsy" font-size="8">-</tspan><tspan font-family="cmr" font-size="8">1</tspan>
<tspan font-family="cmmi" font-size="8">i</tspan>
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;10: </span><span  
class="content">Equation 2</span></div><!--tex4ht:label?: x1-14004r10 -->
                                                                          

                                                                          
<!--l. 278--><p class="noindent" ></div><hr class="endfigure">
<!--l. 279--><p class="noindent" >Thus the partial derivative of the error function for a single input-output pair
with respect to weight <tspan font-family="cmmi" font-size="12">w</tspan><sub><tspan font-family="cmmi" font-size="8">ij</tspan></sub><sup><tspan font-family="cmmi" font-size="8">k</tspan></sup> is a product of the error term at node <tspan font-family="cmmi" font-size="12">j </tspan>in layer <tspan font-family="cmmi" font-size="12">L</tspan><sub>
<tspan font-family="cmmi" font-size="8">k</tspan></sub>
and the output <tspan font-family="cmmi" font-size="12">a</tspan><sub><tspan font-family="cmmi" font-size="8">i</tspan></sub><sup><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmsy" font-size="8">-</tspan><tspan font-family="cmr" font-size="8">1</tspan></sup> of node <tspan font-family="cmmi" font-size="12">i </tspan>in layer <tspan font-family="cmmi" font-size="12">L</tspan><sub>
<tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmsy" font-size="8">-</tspan><tspan font-family="cmr" font-size="8">1</tspan></sub>. Inuitively, we can understand
this as the amount to which the change in weight <tspan font-family="cmmi" font-size="12">w</tspan><sub><tspan font-family="cmmi" font-size="8">ij</tspan></sub><sup><tspan font-family="cmmi" font-size="8">k</tspan></sup> influenced the
error function depends on the strength of the neuron <tspan font-family="cmmi" font-size="12">i </tspan>in layer <tspan font-family="cmmi" font-size="12">L</tspan><sub><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmsy" font-size="8">-</tspan><tspan font-family="cmr" font-size="8">1</tspan></sub>.
<br 
class="newline" /><br 
class="newline" />The error term <tspan font-family="cmmi" font-size="12">&#x03B4;</tspan><sub><tspan font-family="cmmi" font-size="8">j</tspan></sub><sup><tspan font-family="cmmi" font-size="8">k</tspan></sup> must be calculated with respect to a specific error function
(i.e. we cannot make this derivation general for all error functions), and since we
are using mean-squared error: <hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
                                                                          

                                                                          
<!--l. 283--><p class="noindent" ><tspan font-family="cmmi" font-size="12">E </tspan>= (<tspan font-family="cmmi" font-size="12">y </tspan><tspan font-family="cmsy" font-size="10">- </tspan><tspan font-family="cmmi" font-size="12">f</tspan><tspan font-family="cmmi" font-size="8">&#x03B8;</tspan>(<tspan font-family="cmmi" font-size="12">&#x20D7;</tspan><tspan font-family="cmmi" font-size="12">x</tspan>))<tspan font-family="cmr" font-size="8">2</tspan> = (<tspan font-family="cmmi" font-size="12">y </tspan><tspan font-family="cmsy" font-size="10">- </tspan><tspan font-family="cmmi" font-size="12">a</tspan><tspan font-family="cmmi" font-size="8">m</tspan>
<tspan font-family="cmr" font-size="8">1</tspan> )<tspan font-family="cmr" font-size="8">2</tspan> = (<tspan font-family="cmmi" font-size="12">y </tspan><tspan font-family="cmsy" font-size="10">- </tspan><tspan font-family="cmmi" font-size="12">&#x03C3;</tspan>(<tspan font-family="cmmi" font-size="12">z</tspan><tspan font-family="cmmi" font-size="8">m</tspan>
<tspan font-family="cmr" font-size="8">1</tspan>  ))<tspan font-family="cmr" font-size="8">2</tspan>
                                                                          

                                                                          
<!--l. 286--><p class="noindent" ></div><hr class="endfigure">
<!--l. 287--><p class="noindent" >Where <tspan font-family="cmmi" font-size="12">m </tspan>is the final layer of the network. Backpropagation starts by defining <tspan font-family="cmmi" font-size="12">&#x03B4;</tspan><sub><tspan font-family="cmr" font-size="8">1</tspan></sub><sup><tspan font-family="cmmi" font-size="8">m</tspan></sup>
and &#8217;propogating&#8217; the error backwards through the hidden layer, thus we must
start on the output layer <tspan font-family="cmmi" font-size="12">L</tspan><sub><tspan font-family="cmmi" font-size="8">m</tspan></sub> (which as previously mentioned has one node for
the purposes of derivation). Thus our error term for the output layer is:
<hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
                                                                          

                                                                          
<!--l. 290--><p class="noindent" ><tspan font-family="cmmi" font-size="12">&#x03B4;</tspan><tspan font-family="cmmi" font-size="8">m</tspan>
<tspan font-family="cmr" font-size="8">1</tspan>   =  <tspan font-family="cmmi" font-size="12">&#x03B4;E</tspan> _
<tspan font-family="cmmi" font-size="12">&#x03B4;a</tspan><tspan font-family="cmmi" font-size="8">m</tspan><tspan font-family="cmr" font-size="8">1</tspan> <tspan font-family="cmmi" font-size="12">&#x03B4;a</tspan><tspan font-family="cmmi" font-size="8">m</tspan>
<tspan font-family="cmr" font-size="8">1</tspan> _
<tspan font-family="cmmi" font-size="12">&#x03B4;z</tspan><tspan font-family="cmmi" font-size="8">m</tspan><tspan font-family="cmr" font-size="8">1</tspan>  = 2 <tspan font-family="cmsy" font-size="10">&#x22C5; </tspan><tspan font-family="cmmi" font-size="12">&#x03C3;</tspan><tspan font-family="cmsy" font-size="8">&#x2032;</tspan>(<tspan font-family="cmmi" font-size="12">z</tspan><tspan font-family="cmmi" font-size="8">m</tspan>
<tspan font-family="cmr" font-size="8">1</tspan>  )(<tspan font-family="cmmi" font-size="12">y </tspan><tspan font-family="cmsy" font-size="10">- </tspan><tspan font-family="cmmi" font-size="12">&#x03C3;</tspan>(<tspan font-family="cmmi" font-size="12">z</tspan><tspan font-family="cmmi" font-size="8">m</tspan>
<tspan font-family="cmr" font-size="8">1</tspan>  )) = 2 <tspan font-family="cmsy" font-size="10">&#x22C5; </tspan><tspan font-family="cmmi" font-size="12">&#x03C3;</tspan><tspan font-family="cmsy" font-size="8">&#x2032;</tspan>(<tspan font-family="cmmi" font-size="12">z</tspan><tspan font-family="cmmi" font-size="8">m</tspan>
<tspan font-family="cmr" font-size="8">1</tspan>  )(<tspan font-family="cmmi" font-size="12">y </tspan><tspan font-family="cmsy" font-size="10">- </tspan><tspan font-family="cmmi" font-size="12">f</tspan><tspan font-family="cmmi" font-size="8">&#x03B8;</tspan>(<tspan font-family="cmmi" font-size="12">&#x20D7;</tspan><tspan font-family="cmmi" font-size="12">x</tspan>))
                                                                          

                                                                          
<!--l. 293--><p class="noindent" ></div><hr class="endfigure">
<!--l. 294--><p class="noindent" >And putting it all together for the derivative of the error function with respect to
the final weight <tspan font-family="cmmi" font-size="12">w</tspan><sub><tspan font-family="cmmi" font-size="8">i</tspan><tspan font-family="cmr" font-size="8">1</tspan></sub><sup><tspan font-family="cmmi" font-size="8">m</tspan></sup>: <hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
<a 
 id="x1-14005r11"></a>
                                                                          

                                                                          
<!--l. 297--><p class="noindent" > <tspan font-family="cmmi" font-size="12">&#x03B4;E</tspan>_
<tspan font-family="cmmi" font-size="12">&#x03B4;w</tspan><tspan font-family="cmmi" font-size="8">m</tspan><tspan font-family="cmmi" font-size="8">i</tspan><tspan font-family="cmr" font-size="8">1</tspan> = <tspan font-family="cmmi" font-size="12">&#x03B4;</tspan><tspan font-family="cmmi" font-size="8">m</tspan>
<tspan font-family="cmr" font-size="8">1</tspan> <tspan font-family="cmmi" font-size="12">a</tspan><tspan font-family="cmmi" font-size="8">m</tspan><tspan font-family="cmsy" font-size="8">-</tspan><tspan font-family="cmr" font-size="8">1</tspan>
<tspan font-family="cmmi" font-size="8">i</tspan>      = 2 <tspan font-family="cmsy" font-size="10">&#x22C5; </tspan><tspan font-family="cmmi" font-size="12">&#x03C3;</tspan><tspan font-family="cmsy" font-size="8">&#x2032;</tspan>(<tspan font-family="cmmi" font-size="12">z</tspan><tspan font-family="cmmi" font-size="8">m</tspan>
<tspan font-family="cmr" font-size="8">1</tspan>  )(<tspan font-family="cmmi" font-size="12">y </tspan><tspan font-family="cmsy" font-size="10">- </tspan><tspan font-family="cmmi" font-size="12">f</tspan><tspan font-family="cmmi" font-size="8">&#x03B8;</tspan>(<tspan font-family="cmmi" font-size="12">&#x20D7;</tspan><tspan font-family="cmmi" font-size="12">x</tspan>))<tspan font-family="cmmi" font-size="12">a</tspan><tspan font-family="cmmi" font-size="8">m</tspan><tspan font-family="cmsy" font-size="8">-</tspan><tspan font-family="cmr" font-size="8">1</tspan>
<tspan font-family="cmmi" font-size="8">i</tspan>
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;11: </span><span  
class="content">Equation 3</span></div><!--tex4ht:label?: x1-14005r11 -->
                                                                          

                                                                          
<!--l. 302--><p class="noindent" ></div><hr class="endfigure">
<!--l. 303--><p class="noindent" >That is, the degree to which the error function <tspan font-family="cmmi" font-size="12">E </tspan>changes with respect to the final
weight depends on the change in the error in the final error multiplied by the
strength of the previous neuron. <br 
class="newline" /><br 
class="newline" />Now we have the case for the error function derivatives for the hidden
layers. The error term <tspan font-family="cmmi" font-size="12">&#x03B4;</tspan><sub><tspan font-family="cmmi" font-size="8">j</tspan></sub><sup><tspan font-family="cmmi" font-size="8">k</tspan></sup> for the hidden layers 1 <tspan font-family="cmmi" font-size="12">&#x003C; k &#x003C; m </tspan>is defined as:
<hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
                                                                          

                                                                          
<!--l. 307--><p class="noindent" ><tspan font-family="cmmi" font-size="12">&#x03B4;</tspan><tspan font-family="cmmi" font-size="8">k</tspan>
<tspan font-family="cmmi" font-size="8">j</tspan>  = <tspan font-family="cmmi" font-size="12">&#x03B4;E</tspan> 
<tspan font-family="cmmi" font-size="12">&#x03B4;a</tspan><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmmi" font-size="8">j</tspan> <tspan font-family="cmmi" font-size="12">&#x03B4;a</tspan><tspan font-family="cmmi" font-size="8">k</tspan>
<tspan font-family="cmmi" font-size="8">j</tspan> _
<tspan font-family="cmmi" font-size="12">&#x03B4;z</tspan><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmmi" font-size="8">j</tspan>  = <tspan font-family="cmmi" font-size="8">l</tspan><tspan font-family="cmr" font-size="8">=</tspan><tspan font-family="cmmi" font-size="8">L</tspan><tspan font-family="cmmi" font-size="6">k</tspan><tspan font-family="cmr" font-size="6">+1</tspan>
    <tspan font-family="cmex" font-size="10">&#x2211;</tspan>
 <tspan font-family="cmmi" font-size="8">l</tspan><tspan font-family="cmr" font-size="8">=1</tspan>      <tspan font-family="cmmi" font-size="12">&#x03B4;E</tspan> __
<tspan font-family="cmmi" font-size="12">&#x03B4;a</tspan><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmr" font-size="8">+1</tspan>
<tspan font-family="cmmi" font-size="8">l</tspan>     <tspan font-family="cmmi" font-size="12">&#x03B4;a</tspan><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmr" font-size="8">+1</tspan>
<tspan font-family="cmmi" font-size="8">l</tspan> ____
<tspan font-family="cmmi" font-size="12">&#x03B4;z</tspan><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmr" font-size="8">+1</tspan>
<tspan font-family="cmmi" font-size="8">l</tspan>     <tspan font-family="cmmi" font-size="12">&#x03B4;z</tspan><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmr" font-size="8">+1</tspan>
<tspan font-family="cmmi" font-size="8">l</tspan> ____
  <tspan font-family="cmmi" font-size="12">&#x03B4;z</tspan><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmmi" font-size="8">j</tspan>   = <tspan font-family="cmmi" font-size="8">l</tspan><tspan font-family="cmr" font-size="8">=</tspan><tspan font-family="cmmi" font-size="8">L</tspan><tspan font-family="cmmi" font-size="6">k</tspan><tspan font-family="cmr" font-size="6">+1</tspan>
    <tspan font-family="cmex" font-size="10">&#x2211;</tspan>
 <tspan font-family="cmmi" font-size="8">l</tspan><tspan font-family="cmr" font-size="8">=1</tspan>   <tspan font-family="cmmi" font-size="12">&#x03B4;</tspan><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmr" font-size="8">+1</tspan>
<tspan font-family="cmmi" font-size="8">l</tspan>     <tspan font-family="cmmi" font-size="12">&#x03B4;z</tspan><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmr" font-size="8">+1</tspan>
<tspan font-family="cmmi" font-size="8">l</tspan> ____
  <tspan font-family="cmmi" font-size="12">&#x03B4;z</tspan><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmmi" font-size="8">j</tspan>
                                                                          

                                                                          
<!--l. 310--><p class="noindent" ></div><hr class="endfigure">
<!--l. 311--><p class="noindent" >That is, the error term for a node <tspan font-family="cmmi" font-size="12">j </tspan>in layer <tspan font-family="cmmi" font-size="12">L</tspan><sub><tspan font-family="cmmi" font-size="8">k</tspan></sub> is the sum of the rate of
changes of all the error terms in the next layer <tspan font-family="cmmi" font-size="12">k </tspan>+ 1 with respect to
itself. Note that the bias <tspan font-family="cmmi" font-size="12">a</tspan><sub><tspan font-family="cmr" font-size="8">0</tspan></sub><sup><tspan font-family="cmmi" font-size="8">k</tspan></sup> and its corresponding weight <tspan font-family="cmmi" font-size="12">w</tspan><sub>
<tspan font-family="cmr" font-size="8">0</tspan></sub><sup><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmr" font-size="8">+1</tspan></sup><tspan font-family="cmmi" font-size="12">j </tspan>is not
dependent on the changes of the nodes in the previous layer, and so is
not included in this sum. Using our original definition of <tspan font-family="cmmi" font-size="12">z</tspan><sub><tspan font-family="cmmi" font-size="8">j</tspan></sub><sup><tspan font-family="cmmi" font-size="8">k</tspan></sup>, we have:
<hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
                                                                          

                                                                          
<!--l. 314--><p class="noindent" ><tspan font-family="cmmi" font-size="12">z</tspan><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmr" font-size="8">+1</tspan>
<tspan font-family="cmmi" font-size="8">l</tspan>     = <tspan font-family="cmmi" font-size="8">j</tspan><tspan font-family="cmr" font-size="8">=</tspan><tspan font-family="cmmi" font-size="8">n</tspan><tspan font-family="cmmi" font-size="6">L</tspan>
  <tspan font-family="cmex" font-size="10">&#x2211;</tspan>
 <tspan font-family="cmmi" font-size="8">j</tspan><tspan font-family="cmr" font-size="8">=1</tspan>  <tspan font-family="cmmi" font-size="12">w</tspan><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmr" font-size="8">+1</tspan>
<tspan font-family="cmmi" font-size="8">jl</tspan>   <tspan font-family="cmmi" font-size="12">a</tspan><tspan font-family="cmmi" font-size="8">k</tspan>
<tspan font-family="cmmi" font-size="8">j</tspan>  = <tspan font-family="cmmi" font-size="8">j</tspan><tspan font-family="cmr" font-size="8">=</tspan><tspan font-family="cmmi" font-size="8">n</tspan><tspan font-family="cmmi" font-size="6">L</tspan>
  <tspan font-family="cmex" font-size="10">&#x2211;</tspan>
 <tspan font-family="cmmi" font-size="8">j</tspan><tspan font-family="cmr" font-size="8">=1</tspan>  <tspan font-family="cmmi" font-size="12">w</tspan><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmr" font-size="8">+1</tspan>
<tspan font-family="cmmi" font-size="8">jl</tspan>   <tspan font-family="cmmi" font-size="12">&#x03C3;</tspan>(<tspan font-family="cmmi" font-size="12">z</tspan><tspan font-family="cmmi" font-size="8">k</tspan>
<tspan font-family="cmmi" font-size="8">j</tspan> )  =<tspan font-family="cmsy" font-size="10">&#x21D2;</tspan> <tspan font-family="cmmi" font-size="12">&#x03B4;z</tspan><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmr" font-size="8">+1</tspan>
<tspan font-family="cmmi" font-size="8">l</tspan> ____
  <tspan font-family="cmmi" font-size="12">&#x03B4;z</tspan><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmmi" font-size="8">j</tspan>   = <tspan font-family="cmmi" font-size="12">w</tspan><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmr" font-size="8">+1</tspan>
<tspan font-family="cmmi" font-size="8">jl</tspan>   <tspan font-family="cmmi" font-size="12">&#x03C3;</tspan><tspan font-family="cmsy" font-size="8">&#x2032;</tspan>(<tspan font-family="cmmi" font-size="12">z</tspan><tspan font-family="cmmi" font-size="8">k</tspan>
<tspan font-family="cmmi" font-size="8">j</tspan> )
                                                                          

                                                                          
<!--l. 317--><p class="noindent" ></div><hr class="endfigure">
<!--l. 318--><p class="noindent" >Plugging all of the above into our equation for <tspan font-family="cmmi" font-size="12">&#x03B4;</tspan><sub><tspan font-family="cmmi" font-size="8">j</tspan></sub><sup><tspan font-family="cmmi" font-size="8">k</tspan></sup> yields: <hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
<a 
 id="x1-14006r12"></a>
                                                                          

                                                                          
<!--l. 321--><p class="noindent" ><tspan font-family="cmmi" font-size="12">&#x03B4;</tspan><tspan font-family="cmmi" font-size="8">k</tspan>
<tspan font-family="cmmi" font-size="8">j</tspan>  = <tspan font-family="cmmi" font-size="8">l</tspan><tspan font-family="cmr" font-size="8">=</tspan><tspan font-family="cmmi" font-size="8">L</tspan><tspan font-family="cmmi" font-size="6">k</tspan><tspan font-family="cmr" font-size="6">+1</tspan>
    <tspan font-family="cmex" font-size="10">&#x2211;</tspan>
 <tspan font-family="cmmi" font-size="8">l</tspan><tspan font-family="cmr" font-size="8">=1</tspan>   <tspan font-family="cmmi" font-size="12">&#x03B4;</tspan><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmr" font-size="8">+1</tspan>
<tspan font-family="cmmi" font-size="8">l</tspan>     <tspan font-family="cmmi" font-size="12">&#x03B4;z</tspan><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmr" font-size="8">+1</tspan>
<tspan font-family="cmmi" font-size="8">l</tspan> ____
  <tspan font-family="cmmi" font-size="12">&#x03B4;z</tspan><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmmi" font-size="8">j</tspan>   <tspan font-family="cmsy" font-size="10">&#x2261;</tspan> <tspan font-family="cmmi" font-size="8">l</tspan><tspan font-family="cmr" font-size="8">=</tspan><tspan font-family="cmmi" font-size="8">L</tspan><tspan font-family="cmmi" font-size="6">k</tspan><tspan font-family="cmr" font-size="6">+1</tspan>
    <tspan font-family="cmex" font-size="10">&#x2211;</tspan>
 <tspan font-family="cmmi" font-size="8">l</tspan><tspan font-family="cmr" font-size="8">=1</tspan>   <tspan font-family="cmmi" font-size="12">&#x03B4;</tspan><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmr" font-size="8">+1</tspan>
<tspan font-family="cmmi" font-size="8">l</tspan>    <tspan font-family="cmmi" font-size="12">w</tspan><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmr" font-size="8">+1</tspan>
<tspan font-family="cmmi" font-size="8">jl</tspan>   <tspan font-family="cmmi" font-size="12">&#x03C3;</tspan><tspan font-family="cmsy" font-size="8">&#x2032;</tspan>(<tspan font-family="cmmi" font-size="12">z</tspan><tspan font-family="cmmi" font-size="8">k</tspan>
<tspan font-family="cmmi" font-size="8">j</tspan> )
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;12: </span><span  
class="content">Equation 4</span></div><!--tex4ht:label?: x1-14006r12 -->
                                                                          

                                                                          
<!--l. 326--><p class="noindent" ></div><hr class="endfigure">
<!--l. 327--><p class="noindent" >Finally, rearranging <tspan font-family="cmmi" font-size="12">&#x03C3;</tspan><tspan font-family="cmsy" font-size="10">&#x2032;</tspan>(<tspan font-family="cmmi" font-size="12">z</tspan><sub><tspan font-family="cmmi" font-size="8">j</tspan></sub><sup><tspan font-family="cmmi" font-size="8">k</tspan></sup>) out of the sum since it does not depend on the
iterator, we get a final derivative for the error function with respect to a weight in
the hidden layers as : <hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
                                                                          

                                                                          
<!--l. 330--><p class="noindent" > <tspan font-family="cmmi" font-size="12">&#x03B4;E</tspan>_
<tspan font-family="cmmi" font-size="12">&#x03B4;w</tspan><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmmi" font-size="8">ij</tspan> = <tspan font-family="cmmi" font-size="12">&#x03B4;</tspan><tspan font-family="cmmi" font-size="8">k</tspan>
<tspan font-family="cmmi" font-size="8">j</tspan> <tspan font-family="cmmi" font-size="12">a</tspan><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmsy" font-size="8">-</tspan><tspan font-family="cmr" font-size="8">1</tspan>
<tspan font-family="cmmi" font-size="8">i</tspan>     = <tspan font-family="cmmi" font-size="12">a</tspan><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmsy" font-size="8">-</tspan><tspan font-family="cmr" font-size="8">1</tspan>
<tspan font-family="cmmi" font-size="8">i</tspan>    <tspan font-family="cmmi" font-size="12">&#x03C3;</tspan><tspan font-family="cmsy" font-size="8">&#x2032;</tspan>(<tspan font-family="cmmi" font-size="12">z</tspan><tspan font-family="cmmi" font-size="8">k</tspan>
<tspan font-family="cmmi" font-size="8">j</tspan> ) <tspan font-family="cmmi" font-size="8">l</tspan><tspan font-family="cmr" font-size="8">=</tspan><tspan font-family="cmmi" font-size="8">L</tspan><tspan font-family="cmmi" font-size="6">k</tspan><tspan font-family="cmr" font-size="6">+1</tspan>
   <tspan font-family="cmex" font-size="10">&#x2211;</tspan>
 <tspan font-family="cmmi" font-size="8">l</tspan><tspan font-family="cmr" font-size="8">=1</tspan>   <tspan font-family="cmmi" font-size="12">&#x03B4;</tspan><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmr" font-size="8">+1</tspan>
<tspan font-family="cmmi" font-size="8">l</tspan>    <tspan font-family="cmmi" font-size="12">w</tspan><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmr" font-size="8">+1</tspan>
<tspan font-family="cmmi" font-size="8">jl</tspan>
                                                                          

                                                                          
<!--l. 333--><p class="noindent" ></div><hr class="endfigure">
<!--l. 334--><p class="noindent" >And so, the change to an individual weight in the network is calculated as:
<hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
<a 
 id="x1-14007r13"></a>
                                                                          

                                                                          
<!--l. 337--><p class="noindent" >&#x0394;<tspan font-family="cmmi" font-size="12">w</tspan><tspan font-family="cmmi" font-size="8">k</tspan>
<tspan font-family="cmmi" font-size="8">ij</tspan> = <tspan font-family="cmsy" font-size="10">-</tspan><tspan font-family="cmmi" font-size="12">&#x03B1;</tspan><tspan font-family="cmmi" font-size="12">&#x03B4;E</tspan>(<tspan font-family="cmmi" font-size="12">X,&#x03B8;</tspan>)
   <tspan font-family="cmmi" font-size="12">&#x03B4;w</tspan><tspan font-family="cmmi" font-size="8">k</tspan><tspan font-family="cmmi" font-size="8">ij</tspan>
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;13: </span><span  
class="content">Equation 5</span></div><!--tex4ht:label?: x1-14007r13 -->
                                                                          

                                                                          
<!--l. 342--><p class="noindent" ></div><hr class="endfigure">
<!--l. 343--><p class="noindent" >This is the &#8217;gradient descent&#8217; aspect of backpropagation; we want to change our
parameters <tspan font-family="cmmi" font-size="12">&#x03B8; </tspan>(of which <tspan font-family="cmmi" font-size="12">w</tspan><sub><tspan font-family="cmmi" font-size="8">ij</tspan></sub><sup><tspan font-family="cmmi" font-size="8">k</tspan></sup> is a member of) in our neural network <tspan font-family="cmmi" font-size="12">f</tspan><sub>
<tspan font-family="cmmi" font-size="8">&#x03B8;</tspan></sub>(<img 
src="litrev19x.png" alt="&#x20D7;x"  class="vec" >) such
that we &#8217;walk down&#8217; (descend in the negative direction, hence the &#8217;<tspan font-family="cmsy" font-size="10">-</tspan>&#8217;) the
gradient of our error function to find some local minima, so that our network
minimizes error. The rate at which we change a weight in our parameters <tspan font-family="cmmi" font-size="12">&#x03B8;</tspan>
depends on how much that weight affects our error function, and the learning rate
<tspan font-family="cmmi" font-size="12">&#x03B1;</tspan>. The learning rate is a hyperparameter of the network that affects the rate of
adjustment of weights. <br 
class="newline" /><br 
class="newline" />So, assuming some learning rate <tspan font-family="cmmi" font-size="12">&#x03B1;</tspan>, and the weights of the networks randomly
initialized, our general backpropagation algorithm is as follows:
      <ol  class="enumerate1" >
      <li 
  class="enumerate" id="x1-14009x1">Process all the input-output pairs in <tspan font-family="cmmi" font-size="12">X </tspan>with the neural network.
      </li>
      <li 
  class="enumerate" id="x1-14011x2">Evaluate the error term for the output layer using Equation 3.
      </li>
      <li 
  class="enumerate" id="x1-14013x3">Propagate  the  error  backwards  for  layers  <tspan font-family="cmmi" font-size="12">m </tspan><tspan font-family="cmsy" font-size="10">- </tspan>1  through  0  using
      Equation 4.
      </li>
      <li 
  class="enumerate" id="x1-14015x4">Evaluate the respective partial derivatives using Equation 2.
      </li>
      <li 
  class="enumerate" id="x1-14017x5">Combine the derivatives in Equation 1.
      </li>
      <li 
  class="enumerate" id="x1-14019x6">Iterate through all the weights in the network, adjusting them using
      Equation 5.</li></ol>
<h5 class="subsubsectionHead"><span class="titlemark">1.2.3   </span> <a 
 id="x1-150001.2.3"></a>Recurrent Networks</h5>
<!--l. 354--><p class="noindent" >Recurrent Neural Networks distinguish themselves from Feedforward networks in
that they contain at least one feedback loop^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">10</tspan></sup>. A feedback loop implies
that output from neurons is fed back into the network in some form or
another (either to another neuron, or to itself, which is referred to as
self-feedback^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">10</tspan></sup>), resulting in a form of memory within the network. To
                                                                          

                                                                          
clarify, whilst all neural networks have memory, RNNs have sequential
memory in that they can relate information from a previous input within the
same training example to a current input. This is different from a FNN
(Feedforward NN) that can have &#8217;inter-data&#8217; memory as opposed to the RNNs
&#8217;intra-data&#8217; memory. Memorization in a FNN is equivalent to overfitting. There
are many RNN variants, but the most general type would be the Fully
Recurrent network (FRNN). An FRNN is one where every unit is connected to
every other unit in the network^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">33</tspan></sup>. A typical example can be seen below:
<hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
<a 
 id="x1-15001r14"></a>
                                                                          

                                                                          
<!--l. 359--><p class="noindent" ><img 
src="6.png" alt="PIC"  
width="50" height="50" >
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;14: </span><span  
class="content">A simplified view of a RNN</span></div><!--tex4ht:label?: x1-15001r14 -->
                                                                          

                                                                          
<!--l. 362--><p class="noindent" ></div><hr class="endfigure">
<!--l. 363--><p class="noindent" >Note that the time steps <tspan font-family="cmmi" font-size="12">t </tspan>must be discretized, with the activations updated at
each time step, and a delay unit is needed to hold activations for one time step^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">34</tspan></sup>.
From Figure <a 
href="#x1-15001r14">14<!--tex4ht:ref: rnn --></a>, <tspan font-family="cmmi" font-size="12">x</tspan>(<tspan font-family="cmmi" font-size="12">t</tspan>) is our network input from some sequence at time <tspan font-family="cmmi" font-size="12">t </tspan>and <tspan font-family="cmmi" font-size="12">h</tspan>(<tspan font-family="cmmi" font-size="12">t</tspan>)
is our output. This section was mainly included to give context to the different
types of network architectures as it is not directly relevant to the learning
methods we will be using.
<h4 class="subsectionHead"><span class="titlemark">1.3   </span> <a 
 id="x1-160001.3"></a>Policy Iteration</h4>
<!--l. 365--><p class="noindent" >To talk about the subsequent learning methods, we must first introduce a concept
from Dynamic Programming (DP) known as policy iteration^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">35</tspan></sup>.<br 
class="newline" /><br 
class="newline" />Policy Iteration, in the more general form General Policy Iteration (GPI)^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">36</tspan></sup>,
consists of two interacting, alternating processes; those of policy evaluation and
policy improvement. Policy evaluation is the process of making the value function
consistent with the current policy; that is, finding the total expected
reward for a policy <tspan font-family="cmmi" font-size="12">&#x03C0;</tspan>^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">37</tspan></sup>. Policy Improvement is the act of making the policy
greedy with respect to the current value function; that is, choosing a new
action that yields the highest reward given the current value function. This
is summed up with Policy Improvement Theorem, which states^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">2</tspan></sup><sup><tspan font-family="cmmi" font-size="8">,</tspan></sup>^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">38</tspan></sup><sup><tspan font-family="cmmi" font-size="8">,</tspan></sup>^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">39</tspan></sup>:
<hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
                                                                          

                                                                          
<!--l. 370--><p class="noindent" ><tspan font-family="cmti" font-size="12">Let </tspan><tspan font-family="cmmi" font-size="12">&#x03C0; </tspan><tspan font-family="cmti" font-size="12">and </tspan><tspan font-family="cmmi" font-size="12">&#x03C0;</tspan><tspan font-family="cmsy" font-size="10">&#x2032; </tspan><tspan font-family="cmti" font-size="12">be policies, and </tspan><tspan font-family="cmmi" font-size="12">&#x03C0;</tspan><tspan font-family="cmsy" font-size="10">&#x2032; </tspan><tspan font-family="cmti" font-size="12">be chosen such that:</tspan><br />
&#x00A0;<br />
<tspan font-family="cmmi" font-size="12">Q</tspan><sub><tspan font-family="cmmi" font-size="8">&#x03C0;</tspan></sub>(<tspan font-family="cmmi" font-size="12">s,&#x03C0;</tspan><tspan font-family="cmsy" font-size="10">&#x2032;</tspan>(<tspan font-family="cmmi" font-size="12">s</tspan>)) <tspan font-family="cmsy" font-size="10">&#x2265; </tspan><tspan font-family="cmmi" font-size="12">V</tspan> <sub><tspan font-family="cmmi" font-size="8">&#x03C0;</tspan></sub>(<tspan font-family="cmmi" font-size="12">s</tspan>)<br />
&#x00A0;<br />
<tspan font-family="cmti" font-size="12">Then we have:</tspan><br />
&#x00A0;<br />
<tspan font-family="cmmi" font-size="12">V</tspan> <sub><tspan font-family="cmmi" font-size="8">&#x03C0;</tspan><tspan font-family="cmsy" font-size="8">&#x2032;</tspan></sub>(<tspan font-family="cmmi" font-size="12">s</tspan>) <tspan font-family="cmsy" font-size="10">&#x2265; </tspan><tspan font-family="cmmi" font-size="12">V</tspan> <sub><tspan font-family="cmmi" font-size="8">&#x03C0;</tspan></sub>(<tspan font-family="cmmi" font-size="12">s</tspan>)
                                                                          

                                                                          
<!--l. 375--><p class="noindent" ></div><hr class="endfigure">
<!--l. 376--><p class="noindent" >What this says in essence is that if we have a policy <tspan font-family="cmmi" font-size="12">&#x03C0;</tspan><tspan font-family="cmsy" font-size="10">&#x2032; </tspan>that chooses greedily
among the options, and thereafter follows <tspan font-family="cmmi" font-size="12">&#x03C0;</tspan>, then this policy is universally better
than <tspan font-family="cmmi" font-size="12">&#x03C0;</tspan>. A formal proof can be found in Sutton&#8217;s book^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">38</tspan></sup> and Precup, Sutton and
Singh&#8217;s paper^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">39</tspan></sup>.<br 
class="newline" /><br 
class="newline" />Alternation between improvement and evaluation continues, in some form or
another (in policy iteration it is alternating, in asynchronous DP-like methods^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">40</tspan></sup>
they are interleaved at a finer grain) until the algorithm converges to the optimal
policy, and correspondingly an optimal value function.
<h4 class="subsectionHead"><span class="titlemark">1.4   </span> <a 
 id="x1-170001.4"></a>Monte Carlo Method</h4>
<!--l. 379--><p class="noindent" >The Monte Carlo method is a numerical method of solving problems by
simulation of random variables^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">41</tspan></sup>. Whilst it is vaguely defined, a generally
accepted conception date for the Monte Carlo method is 1949 when it was first
published in the American Statistical Association^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">42</tspan></sup>. The general principle of
Monte Carlo Methods can be summed up in the famous Hit-or-Miss example^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">41</tspan></sup><sup><tspan font-family="cmmi" font-size="8">,</tspan></sup>^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">43</tspan></sup>:
<hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
<a 
 id="x1-17001r15"></a>
                                                                          

                                                                          
<!--l. 382--><p class="noindent" ><img 
src="7.png" alt="PIC"  
width="50" height="50" >
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;15: </span><span  
class="content">Hit-or-Miss example - Monte-Carlo Integration</span></div><!--tex4ht:label?: x1-17001r15 -->
                                                                          

                                                                          
<!--l. 385--><p class="noindent" ></div><hr class="endfigure">
<!--l. 386--><p class="noindent" >If we want to estimate the size of some arbitrary shape <tspan font-family="cmmi" font-size="12">S </tspan>within some unit square
(see Figure <a 
href="#x1-17001r15">15<!--tex4ht:ref: hom --></a>), we can use Monte Carlo to estimate this value. Let <tspan font-family="cmmi" font-size="12">N </tspan>be a
random number of points within the square (red and blue dots), and <tspan font-family="cmmi" font-size="12">N</tspan><tspan font-family="cmsy" font-size="10">&#x2032; </tspan>be the
number that fall within our shape <tspan font-family="cmmi" font-size="12">S </tspan>(blue dots only). It is clear then that the area
of <tspan font-family="cmmi" font-size="12">S </tspan>is approximately the same as the ratio <tspan font-family="cmmi" font-size="12">N</tspan><tspan font-family="cmsy" font-size="10">&#x2032;</tspan><tspan font-family="cmmi" font-size="12">&#x2215;N</tspan>. This is the essence of Monte
Carlo methods; using random variables to estimate solutions to deterministic
problems. <br 
class="newline" /><br 
class="newline" />In the context of reinforcement learning, recall that we are trying to solve the
Bellman equation, which in its simplest, unexpanded form looks like:
<hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
                                                                          

                                                                          
<!--l. 390--><p class="noindent" ><tspan font-family="cmmi" font-size="12">V</tspan><tspan font-family="cmmi" font-size="8">&#x03C0;</tspan>(<tspan font-family="cmmi" font-size="12">s</tspan>) = <tspan font-family="cmmi" font-size="12">E</tspan><tspan font-family="cmmi" font-size="8">&#x03C0;</tspan>[<tspan font-family="cmmi" font-size="12">R</tspan>(<tspan font-family="cmmi" font-size="12">s</tspan>) + <tspan font-family="cmmi" font-size="12">&#x03B3;V </tspan>(<tspan font-family="cmmi" font-size="12">s</tspan><tspan font-family="cmsy" font-size="8">&#x2032;</tspan>)]
                                                                          

                                                                          
<!--l. 393--><p class="noindent" ></div><hr class="endfigure">
<!--l. 394--><p class="noindent" >Which in essence says; the value of state <tspan font-family="cmmi" font-size="12">s </tspan>given policy <tspan font-family="cmmi" font-size="12">&#x03C0; </tspan>is the reward of the
current state plus the the discounted reward of the next state having followed
policy <tspan font-family="cmmi" font-size="12">&#x03C0;</tspan>. Now, since this is a recursive function, it is not immediately obvious how
to compute the policy <tspan font-family="cmmi" font-size="12">&#x03C0; </tspan>- how can we find the value of the current state when it
relies on the value of the future states? This leads us to our first issue of the credit
assignment problem.<br 
class="newline" /><br 
class="newline" />The credit assignment problem^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">2</tspan></sup><sup><tspan font-family="cmmi" font-size="8">,</tspan></sup>^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">44</tspan></sup> is the problem of properly assigning values to
individual states in a learning episode which lead to a particular outcome.
Imagine we have a sequence of actions that lead to a particular positive outcome
for an agent. Early actions may be essential for this positive outcome to occur, yet
may not have an immediate reward themselves. Conversely, we may at some point
enter a &#8217;doomed&#8217; state, from which there is no possible positive outcome for our
agent. It would be wrong to assign credit for this to the states immediately
preceding this outcome, we must propogate these values all the way up to the
point in which we enter this &#8217;doomed&#8217; scenario. This is the essence of the credit
assignment problem. Monte Carlo methods provide one solution to this
problem. To be clear, this is a form of model-free learning^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">45</tspan></sup>, where the
transition probabilities of our MDP are unknown, in contrast to model-based
learning (with DP techniques such as Policy/Value Iteration). Note that the
algorithms we will be looking at later such as AlphaGo and MuZero do in
fact begin with an unknown transition matrix, however the first step of
learning for these algorithms is to build a model of their environment.
<br 
class="newline" /><br 
class="newline" />In contrast to basic TD-learning (not TD(<tspan font-family="cmmi" font-size="12">&#x03BB;</tspan>) as this is a compromise between
Monte Carlo and regular TD-learning) covered in the next section, Monte-Carlo
methods do not bootstrap. Bootstrapping is when a learning algorithm
updates its current estimates of value within an episode based on previous
estimates (online)^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">46</tspan></sup>, in constrast to updating values at the end of an episode
(episode-by-episode). <br 
class="newline" /><br 
class="newline" />When a model of the environment is known; that is, the agent knows which states
will result in which actions, then the state-value function is sufficient to calculate
optimal policy. However, in a model-free learning approach, where next states are
unknown, it is necessary to assign values to actions themselves, thus we must used
the Q-value function^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">47</tspan></sup>: <hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
                                                                          

                                                                          
<!--l. 400--><p class="noindent" ><tspan font-family="cmmi" font-size="12">Q</tspan><tspan font-family="cmmi" font-size="8">&#x03C0;</tspan>(<tspan font-family="cmmi" font-size="12">s,a</tspan>) = <tspan font-family="cmmi" font-size="12">E</tspan><tspan font-family="cmmi" font-size="8">&#x03C0;</tspan>[<tspan font-family="cmmi" font-size="12">R</tspan>(<tspan font-family="cmmi" font-size="12">s,a</tspan>) + <tspan font-family="cmmi" font-size="12">&#x03B3;V </tspan>(<tspan font-family="cmmi" font-size="12">s</tspan><tspan font-family="cmsy" font-size="8">&#x2032;</tspan>)]
                                                                          

                                                                          
<!--l. 403--><p class="noindent" ></div><hr class="endfigure">
<!--l. 404--><p class="noindent" >Which reads - The value of the state-action pair (<tspan font-family="cmmi" font-size="12">s,a</tspan>) is the value of taking
action <tspan font-family="cmmi" font-size="12">a </tspan>in the current state <tspan font-family="cmmi" font-size="12">s </tspan>and thereafter following policy <tspan font-family="cmmi" font-size="12">&#x03C0;</tspan>. As is the
case for all learning methods, the aim is to find the optimal policy, thus
we will be following a policy iteration protocol as described in Section
<a 
href="#x1-160001.3">1.3<!--tex4ht:ref: polit --></a><span class="footnote-mark"><a 
href="litrev2.html#fn2x0">^<sup class="textsuperscript">&#8224;</sup></a></span><a 
 id="x1-17002f0"></a>  .
The essence of Monte-Carlo policy iteration is to generate <tspan font-family="cmmi" font-size="12">N </tspan>number of random
episodes (or runs through from an initial state to a terminal state), where
the higher <tspan font-family="cmmi" font-size="12">N </tspan>is the more accurate the estimate of the Q-value will be,
and average over these runs to generate estimates for state-action pairs.
There are two types of Monte-Carlo protocol for policy iteration; first-visit
and every-visit^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">48</tspan></sup>. First-visit protocols only average the values of the first
appearance of a state in an episode, whereas every-visit averages over all
appearances of a state. There are advantages and disadvantages to both, but for
simplicity we will go through first-visit protocols. Note that both protocols
eventually converge to the true value of the Q-function as <tspan font-family="cmmi" font-size="12">N </tspan>tends to
infinity.<br 
class="newline" /><br 
class="newline" />An obvious problem of the Monte-Carlo protocol is, by the very nature of random
sampling, it is possible that in a large state space, many state-action pairs will
never be visited in the sampling stage, thus no Q-values for these pairs. A solution
to this is the assumption of stochastic policies, where the action in each state is
chosen by a dice roll, and we refine our probabilities assigned to each action
according to the Q function. <br 
class="newline" /><br 
class="newline" />For our Monte Carlo policy iteration approach, we can either choose on-policy or
off-policy methods. An on-policy method updates Q-values based on the next
state <tspan font-family="cmmi" font-size="12">s</tspan><tspan font-family="cmsy" font-size="10">&#x2032; </tspan>and the on-policy action <tspan font-family="cmmi" font-size="12">a</tspan><tspan font-family="cmsy" font-size="10">&#x2032;&#x2032;</tspan>, whereas off-policy methods update the
Q-value based on the next state <tspan font-family="cmmi" font-size="12">s</tspan><tspan font-family="cmsy" font-size="10">&#x2032; </tspan>and the greedy action <tspan font-family="cmmi" font-size="12">a</tspan><tspan font-family="cmsy" font-size="10">&#x2032;</tspan>^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">36</tspan></sup>. Thus the
distinction between on-policy and off-policy only exists if the current policy is not
greedy; that is, if the agent explores. However, an agent would never learn
anything about the environment if it did not explore, so this is rarely the case. A
non-greedy policy where <tspan font-family="cmmi" font-size="12">&#x03C0;</tspan>(<tspan font-family="cmmi" font-size="12">a</tspan><tspan font-family="cmsy" font-size="10">|</tspan><tspan font-family="cmmi" font-size="12">s</tspan>) <tspan font-family="cmmi" font-size="12">&#x003E; </tspan>0<tspan font-family="cmmi" font-size="12">,</tspan><tspan font-family="cmsy" font-size="10">&#x2200;</tspan><tspan font-family="cmmi" font-size="12">s </tspan><tspan font-family="cmsy" font-size="10">&#x2208; </tspan><tspan font-family="cmmi" font-size="12">S,</tspan><tspan font-family="cmsy" font-size="10">&#x2200;</tspan><tspan font-family="cmmi" font-size="12">a </tspan><tspan font-family="cmsy" font-size="10">&#x2208; </tspan><tspan font-family="cmmi" font-size="12">A</tspan>(<tspan font-family="cmmi" font-size="12">s</tspan>) can also be called a soft
                                                                          

                                                                          
policy, and the aim of policy iteration is to gradually shift this towards a
deterministic policy as the optimal policy is approached. A typical soft policy
used, such as for Q-learning^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">47</tspan></sup> and Deep Q-learning^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">49</tspan></sup><sup><tspan font-family="cmmi" font-size="8">,</tspan></sup>^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">50</tspan></sup> is an <tspan font-family="cmmi" font-size="12">&#x03F5;</tspan><tspan font-family="cmti" font-size="12">-greedy </tspan>policy,
where in each state, the agent chooses the greedy action, but with a chance <img 
src="litrev20x.png" alt="|A(&#x03F5;s)|-"  class="frac" align="middle">
of choosing a random action. <br 
class="newline" /><br 
class="newline" />Without going into detail, an on-policy Monte-Carlo method would repeatedly
evaluate the Q-value following a policy <tspan font-family="cmmi" font-size="12">&#x03C0; </tspan>using random sampling of episodes, and
then improving policy by moving towards a policy <tspan font-family="cmmi" font-size="12">&#x03C0; </tspan>that is greedy with respect to
the current Q-value function. An off-policy Monte Carlo method on the other
hand would follow an exploratory behaviour policy <tspan font-family="cmmi" font-size="12">b </tspan>whilst using policy iteration
to improve a target policy <tspan font-family="cmmi" font-size="12">&#x03C0;</tspan>, and estimating the distribution of <tspan font-family="cmmi" font-size="12">&#x03C0; </tspan>from <tspan font-family="cmmi" font-size="12">b </tspan>using a
technique called importance sampling^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">36</tspan></sup><sup><tspan font-family="cmmi" font-size="8">,</tspan></sup>^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">51</tspan></sup>.
<h4 class="subsectionHead"><span class="titlemark">1.5   </span> <a 
 id="x1-180001.5"></a>Temporal Difference Learning</h4>
<!--l. 410--><p class="noindent" >Temporal difference (TD) learning as described by Sutton^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">52</tspan></sup>, is learning to
predict, where learning is driven by difference in predictions over time. It is
a model-free RL approach, where the agent has no prior knowledge of
the environment^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">52</tspan></sup>. To use his weatherman example, if we are to predict
rain on Saturday, and we are given a 50% chance prediction on Monday,
and 75% on Tuesday, TD learning methods will increase predictions for
days similar to Monday. This is also done as soon as Tuesday is reached,
without waiting for Saturday to actually arrive, and is therefore a form of
bootstrapping^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">52</tspan></sup>. TD learning excells at a specific kind of specific kind of
prediction problem; a multi-step prediction problem. To clarify, a single-step
prediction problem is one where all information about the correctness of each
prediction is revealed at once, in contrast to multi-step where correctness
is not revealed until one or more steps after the prediction is made^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">52</tspan></sup>.
<br 
class="newline" /><br 
class="newline" />As mentioned in Section <a 
href="#x1-170001.4">1.4<!--tex4ht:ref: mcmethod --></a>, TD methods use bootstrapping to update their
estimates in real-time (online) during the course of an episode. Again
TD methods use a policy iteration protocol, but because they bootstrap,
whereas MC methods wait until the end of an episode to update their
estimates, TD methods can immediately update an estimate after a single
timestep. The simplest form of TD called <tspan font-family="cmmi" font-size="12">TD</tspan>(0) performs updates like so^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">52</tspan></sup>:
                                                                          

                                                                          
<hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
                                                                          

                                                                          
<!--l. 414--><p class="noindent" ><tspan font-family="cmmi" font-size="12">V </tspan>(<tspan font-family="cmmi" font-size="12">S</tspan><tspan font-family="cmmi" font-size="8">t</tspan>) = <tspan font-family="cmmi" font-size="12">V </tspan>(<tspan font-family="cmmi" font-size="12">S</tspan><tspan font-family="cmmi" font-size="8">t</tspan>) + <tspan font-family="cmmi" font-size="12">&#x03B1;</tspan>[<tspan font-family="cmmi" font-size="12">R</tspan><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">+1</tspan> + <tspan font-family="cmmi" font-size="12">&#x03B3;V </tspan>(<tspan font-family="cmmi" font-size="12">S</tspan><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">+1</tspan>) <tspan font-family="cmsy" font-size="10">- </tspan><tspan font-family="cmmi" font-size="12">V </tspan>(<tspan font-family="cmmi" font-size="12">S</tspan><tspan font-family="cmmi" font-size="8">t</tspan>)]
                                                                          

                                                                          
<!--l. 417--><p class="noindent" ></div><hr class="endfigure">
<!--l. 418--><p class="noindent" >Or in the case of the Q-value function: <hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
                                                                          

                                                                          
<!--l. 421--><p class="noindent" ><tspan font-family="cmmi" font-size="12">Q</tspan>(<tspan font-family="cmmi" font-size="12">S</tspan><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmmi" font-size="12">,A</tspan><tspan font-family="cmmi" font-size="8">t</tspan>) = <tspan font-family="cmmi" font-size="12">Q</tspan>(<tspan font-family="cmmi" font-size="12">S</tspan><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmmi" font-size="12">,A</tspan><tspan font-family="cmmi" font-size="8">t</tspan>) + <tspan font-family="cmmi" font-size="12">&#x03B1;</tspan>[<tspan font-family="cmmi" font-size="12">R</tspan><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">+1</tspan> + <tspan font-family="cmmi" font-size="12">&#x03B3;Q</tspan>(<tspan font-family="cmmi" font-size="12">S</tspan><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">+1</tspan><tspan font-family="cmmi" font-size="12">,A</tspan><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">+1</tspan>) <tspan font-family="cmsy" font-size="10">- </tspan><tspan font-family="cmmi" font-size="12">Q</tspan>(<tspan font-family="cmmi" font-size="12">S</tspan><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmmi" font-size="12">,A</tspan><tspan font-family="cmmi" font-size="8">t</tspan>)]
                                                                          

                                                                          
<!--l. 424--><p class="noindent" ></div><hr class="endfigure">
<!--l. 425--><p class="noindent" >Where <tspan font-family="cmmi" font-size="12">&#x03B1; </tspan>is our learning rate. The learning rate here is used similarly to the
learning rate of backpropagation (see Section <a 
href="#x1-140001.2.2">1.2.2<!--tex4ht:ref: backpropa --></a>), where we can tune this to
adjust our rate of change. The equation above is essentially saying &#8217;The new value
of state <tspan font-family="cmmi" font-size="12">S</tspan><sub><tspan font-family="cmmi" font-size="8">t</tspan></sub> is the old value plus the difference in value between the new state and
the old state, adjusted by a learning rate <tspan font-family="cmmi" font-size="12">&#x03B1;</tspan>&#8217;. To illustrate the difference
between this and Monte-Carlo methods, an equivalent update rule would be:
<hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
                                                                          

                                                                          
<!--l. 429--><p class="noindent" ><tspan font-family="cmmi" font-size="12">V </tspan>(<tspan font-family="cmmi" font-size="12">S</tspan><tspan font-family="cmmi" font-size="8">t</tspan>) = <tspan font-family="cmmi" font-size="12">V </tspan>(<tspan font-family="cmmi" font-size="12">S</tspan><tspan font-family="cmmi" font-size="8">t</tspan>) + <tspan font-family="cmmi" font-size="12">&#x03B1;</tspan>[<tspan font-family="cmmi" font-size="12">G</tspan><tspan font-family="cmmi" font-size="8">t</tspan> <tspan font-family="cmsy" font-size="10">- </tspan><tspan font-family="cmmi" font-size="12">V </tspan>(<tspan font-family="cmmi" font-size="12">S</tspan><tspan font-family="cmmi" font-size="8">t</tspan>)]
                                                                          

                                                                          
<!--l. 432--><p class="noindent" ></div><hr class="endfigure">
<!--l. 433--><p class="noindent" >Where <tspan font-family="cmmi" font-size="12">G</tspan><sub><tspan font-family="cmmi" font-size="8">t</tspan></sub> is our cumulative discounted reward after time <tspan font-family="cmmi" font-size="12">t</tspan>, which must be
calculated at the end of a full episode as opposed to <tspan font-family="cmmi" font-size="12">TD</tspan>(0)&#8217;s single timestep. As
a result, in TD updates, the real value of <tspan font-family="cmmi" font-size="12">V </tspan>(<tspan font-family="cmmi" font-size="12">S</tspan><sub><tspan font-family="cmmi" font-size="8">t</tspan></sub>) is unknown, as is the
case for MC methods, and so a sample return is used for an estimate,
however in addition to this it estimates <tspan font-family="cmmi" font-size="12">V </tspan>(<tspan font-family="cmmi" font-size="12">S</tspan><sub><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">+1</tspan></sub>) as well (in contast to
MC methods which would wait until the sample return <tspan font-family="cmmi" font-size="12">G</tspan><sub><tspan font-family="cmmi" font-size="8">t</tspan></sub> is known).
<br 
class="newline" /><br 
class="newline" />Intuitively, TD methods provide some obvious advantages over MC methods.
Firstly, in scenarios where episodes are long, it would be advantageous to begin
learning without having to wait for an episode to finish. As an extension of this,
learning problems that are continuous in nature and not episodic are in fact forced
to used TD methods; MC methods would not work at all in this scenario as the
true sampled return of an episode is never reached. Moreover, convergence of TD
methods with regards to general linear function approximation (meaning the
approximation of a general function by a linear function, for example
the weights and biases in the case of a NN) has been proven^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">53</tspan></sup><sup><tspan font-family="cmmi" font-size="8">,</tspan></sup>^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">54</tspan></sup>, so it
seems at first glance that TD methods are universally better. This leads
us neatly to a central problem of machine learning; the bias-variance
tradeoff.
<h5 class="subsubsectionHead"><span class="titlemark">1.5.1   </span> <a 
 id="x1-190001.5.1"></a>Bias-Variance Tradeoff</h5>
<!--l. 437--><p class="noindent" >The bias-variance tradeoff found in a 1992 paper by Geman et al.^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">55</tspan></sup>, is a conflict
between two source of error that cause issues with learning methods generalizing
beyond their training set. Although initially conceptualized for neural networks,
the same principle applies here in our comparison of MC and TD methods.
Loosely speaking, bias in this context is the difference between the current
estimate of <tspan font-family="cmmi" font-size="12">q</tspan>(<tspan font-family="cmmi" font-size="12">s,a</tspan>) (or <tspan font-family="cmmi" font-size="12">v</tspan>(<tspan font-family="cmmi" font-size="12">s</tspan>)) and the true value, whereas variance is the difference
in our predictions of <tspan font-family="cmmi" font-size="12">q</tspan>(<tspan font-family="cmmi" font-size="12">s,a</tspan>) (or <tspan font-family="cmmi" font-size="12">v</tspan>(<tspan font-family="cmmi" font-size="12">s</tspan>)) between data points. Bias is a case of
oversimplification and as such &#8217;underfitting&#8217;, whereas variance is a case of
&#8217;overfitting&#8217;. It is said that Monte-Carlo methods suffer from high variance, and
TD methods suffer from high bias. To see why, we must look at the updates each
method performs to the value functions. Recall the update rule for MC:
<hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
                                                                          

                                                                          
<!--l. 440--><p class="noindent" ><tspan font-family="cmmi" font-size="12">V </tspan>(<tspan font-family="cmmi" font-size="12">S</tspan><tspan font-family="cmmi" font-size="8">t</tspan>) = <tspan font-family="cmmi" font-size="12">V </tspan>(<tspan font-family="cmmi" font-size="12">S</tspan><tspan font-family="cmmi" font-size="8">t</tspan>) + <tspan font-family="cmmi" font-size="12">&#x03B1;</tspan>[<tspan font-family="cmmi" font-size="12">G</tspan><tspan font-family="cmmi" font-size="8">t</tspan> <tspan font-family="cmsy" font-size="10">- </tspan><tspan font-family="cmmi" font-size="12">V </tspan>(<tspan font-family="cmmi" font-size="12">S</tspan><tspan font-family="cmmi" font-size="8">t</tspan>)]
                                                                          

                                                                          
<!--l. 443--><p class="noindent" ></div><hr class="endfigure">
<!--l. 444--><p class="noindent" >Here the update is performed with <tspan font-family="cmmi" font-size="12">G</tspan><sub><tspan font-family="cmmi" font-size="8">t</tspan></sub>, which is the actual sampled return
for a sequence from time <tspan font-family="cmmi" font-size="12">t</tspan>. cumulative reward from time <tspan font-family="cmmi" font-size="12">t </tspan>is therefore:
<hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
                                                                          

                                                                          
<!--l. 447--><p class="noindent" > <tspan font-family="cmmi" font-size="8">i</tspan><tspan font-family="cmr" font-size="8">=</tspan><tspan font-family="cmmi" font-size="8">T</tspan>
 <tspan font-family="cmex" font-size="10">&#x2211;</tspan>
 <tspan font-family="cmmi" font-size="8">i</tspan><tspan font-family="cmr" font-size="8">=0</tspan> <tspan font-family="cmmi" font-size="12">&#x03B3;</tspan><tspan font-family="cmmi" font-size="8">i</tspan><tspan font-family="cmmi" font-size="12">R</tspan>
<tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">+</tspan><tspan font-family="cmmi" font-size="8">i</tspan><tspan font-family="cmr" font-size="8">+1</tspan>
                                                                          

                                                                          
<!--l. 450--><p class="noindent" ></div><hr class="endfigure">
<!--l. 451--><p class="noindent" >Where <tspan font-family="cmmi" font-size="12">T </tspan>is our final timestep. Recall our Q-value function: <hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
                                                                          

                                                                          
<!--l. 454--><p class="noindent" ><tspan font-family="cmmi" font-size="12">Q</tspan><tspan font-family="cmmi" font-size="8">&#x03C0;</tspan>(<tspan font-family="cmmi" font-size="12">s,a</tspan>) = <tspan font-family="cmmi" font-size="12">E</tspan><tspan font-family="cmmi" font-size="8">&#x03C0;</tspan>[<tspan font-family="cmmi" font-size="12">R</tspan>(<tspan font-family="cmmi" font-size="12">s,a</tspan>) + <tspan font-family="cmmi" font-size="12">&#x03B3;V </tspan>(<tspan font-family="cmmi" font-size="12">s</tspan><tspan font-family="cmsy" font-size="8">&#x2032;</tspan>)]
                                                                          

                                                                          
<!--l. 457--><p class="noindent" ></div><hr class="endfigure">
<!--l. 458--><p class="noindent" >Which can be rewritten in a non-recursive form as: <hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
                                                                          

                                                                          
<!--l. 461--><p class="noindent" ><tspan font-family="cmmi" font-size="12">Q</tspan><tspan font-family="cmmi" font-size="8">&#x03C0;</tspan>(<tspan font-family="cmmi" font-size="12">s,a</tspan>) = <tspan font-family="cmmi" font-size="12">E</tspan><tspan font-family="cmmi" font-size="8">&#x03C0;</tspan>[<tspan font-family="cmmi" font-size="8">i</tspan><tspan font-family="cmr" font-size="8">=</tspan><tspan font-family="cmmi" font-size="8">T</tspan><tspan font-family="cmex" font-size="10">&#x2211;</tspan>
 <tspan font-family="cmmi" font-size="8">i</tspan><tspan font-family="cmr" font-size="8">=0</tspan> <tspan font-family="cmmi" font-size="12">&#x03B3;</tspan><tspan font-family="cmmi" font-size="8">i</tspan><tspan font-family="cmmi" font-size="12">R</tspan>
<tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">+</tspan><tspan font-family="cmmi" font-size="8">i</tspan><tspan font-family="cmr" font-size="8">+1</tspan><tspan font-family="cmsy" font-size="10">|</tspan><tspan font-family="cmmi" font-size="12">S</tspan><tspan font-family="cmmi" font-size="8">t</tspan> = <tspan font-family="cmmi" font-size="12">s,A</tspan><tspan font-family="cmmi" font-size="8">t</tspan> = <tspan font-family="cmmi" font-size="12">a</tspan>]
                                                                          

                                                                          
<!--l. 464--><p class="noindent" ></div><hr class="endfigure">
<!--l. 465--><p class="noindent" >It is clear to see that MC updates will have no bias as they are the true sample
return. TD methods on the other hand bootstrap, and as such they will be
biased on whatever the initial value of <tspan font-family="cmmi" font-size="12">&#x03B3;Q</tspan>(<tspan font-family="cmmi" font-size="12">S</tspan><sub><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">+1</tspan></sub><tspan font-family="cmmi" font-size="12">,A</tspan><sub><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">+1</tspan></sub>) is set to. This of
course will converge with experience, as the true Q-value is approached,
however. Note that whilst convergence works for tabular methods, once
function approximators (read; neural networks) and off-policy learning
are introduced, instability and divergence can occur. Full details of the
<tspan font-family="cmti" font-size="12">deadly triad </tspan>can be found in Sutton and Barto^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">56</tspan></sup> and Hasselt et al.^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">57</tspan></sup>
<br 
class="newline" /><br 
class="newline" />Variance is harder to pinpoint, however the intuition is this; A TD update relies
on 3 random variables - The next reward, the next state and the next action. MC
methods on the other hand, rely on the sum of <tspan font-family="cmti" font-size="12">every </tspan>reward, action and state for
the entire trajectory of an episode. This introduces many more random variables
than the TD update, and as such, the variance is much higher. Whilst
bias-variance is always a tradeoff, there are methods which attempt to
compromise a middle ground, which leads us to <tspan font-family="cmmi" font-size="12">TD</tspan>(<tspan font-family="cmmi" font-size="12">&#x03BB;</tspan>).
<h5 class="subsubsectionHead"><span class="titlemark">1.5.2   </span> <a 
 id="x1-200001.5.2"></a>TD(<tspan font-family="cmmi" font-size="12">&#x03BB;</tspan>)</h5>
<!--l. 468--><p class="noindent" >Famously TD learning was used in a game-learning program called
TD-Gammon^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">58</tspan></sup>, which uses the variation TD(<tspan font-family="cmmi" font-size="12">&#x03BB;</tspan>). In contrast to games like chess
and checkers where brute-force deep searches for move discovery worked,
backgammon was one of the first games RL conquered where brute force methods
would not work. This is due to the high branching factor; each play in
backgammon involves dice rolls, with an average of 20 legal moves per
dice roll. This results in a branching factor of several hundred. Thus new
methods needed to be developed to solve this game, namely some form
of heuristic judgement. Previous approaches to this problem involved
designing a heuristic evaluation function based off of the knowledge of human
experts^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">59</tspan></sup>, however this has never been terribly successful due to the difficulty
of encapsulating the human reasoning process within a heuristic, likely
due to the unknowable factor of human intuition. TD-Gammon tries a
radically different approach at the time, which is learning through self-play^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">58</tspan></sup>.
<br 
class="newline" /><br 
class="newline" />To discuss <tspan font-family="cmmi" font-size="12">TD</tspan>(<tspan font-family="cmmi" font-size="12">&#x03BB;</tspan>) we must first introduce the concept of eligibility traces.
Eligibility traces are essentially a way to compromise <tspan font-family="cmmi" font-size="12">TD</tspan>(0) and Monte Carlo
methods, where instead of looking ahead one step (<tspan font-family="cmmi" font-size="12">TD</tspan>(0)) or waiting until the
end of an episode (Monte Carlo), we instead introduce a weight <tspan font-family="cmmi" font-size="12">&#x03BB; </tspan>for an <tspan font-family="cmti" font-size="12">n-step</tspan>
lookahead. <tspan font-family="cmti" font-size="12">n-step </tspan>lookahead can be thought of as an extension to <tspan font-family="cmmi" font-size="12">TD</tspan>(0).
Recall the update to the value of a state in <tspan font-family="cmmi" font-size="12">TD</tspan>(0) was performed as:
<hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
                                                                          

                                                                          
<!--l. 472--><p class="noindent" ><tspan font-family="cmmi" font-size="12">V </tspan>(<tspan font-family="cmmi" font-size="12">S</tspan><tspan font-family="cmmi" font-size="8">t</tspan>) = <tspan font-family="cmmi" font-size="12">V </tspan>(<tspan font-family="cmmi" font-size="12">S</tspan><tspan font-family="cmmi" font-size="8">t</tspan>) + <tspan font-family="cmmi" font-size="12">&#x03B1;</tspan>[<tspan font-family="cmmi" font-size="12">R</tspan><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">+1</tspan> + <tspan font-family="cmmi" font-size="12">&#x03B3;V </tspan>(<tspan font-family="cmmi" font-size="12">S</tspan><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">+1</tspan>) <tspan font-family="cmsy" font-size="10">- </tspan><tspan font-family="cmmi" font-size="12">V </tspan>(<tspan font-family="cmmi" font-size="12">S</tspan><tspan font-family="cmmi" font-size="8">t</tspan>)]
                                                                          

                                                                          
<!--l. 475--><p class="noindent" ></div><hr class="endfigure">
<!--l. 476--><p class="noindent" >And the update for Monte Carlo: <hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
                                                                          

                                                                          
<!--l. 479--><p class="noindent" ><tspan font-family="cmmi" font-size="12">V </tspan>(<tspan font-family="cmmi" font-size="12">S</tspan><tspan font-family="cmmi" font-size="8">t</tspan>) = <tspan font-family="cmmi" font-size="12">V </tspan>(<tspan font-family="cmmi" font-size="12">S</tspan><tspan font-family="cmmi" font-size="8">t</tspan>) + <tspan font-family="cmmi" font-size="12">&#x03B1;</tspan>[<tspan font-family="cmmi" font-size="12">G</tspan><tspan font-family="cmmi" font-size="8">t</tspan> <tspan font-family="cmsy" font-size="10">- </tspan><tspan font-family="cmmi" font-size="12">V </tspan>(<tspan font-family="cmmi" font-size="12">S</tspan><tspan font-family="cmmi" font-size="8">t</tspan>)]
                                                                          

                                                                          
<!--l. 482--><p class="noindent" ></div><hr class="endfigure">
<!--l. 483--><p class="noindent" >Where <tspan font-family="cmmi" font-size="12">G</tspan><sub><tspan font-family="cmmi" font-size="8">t</tspan></sub> as we know is the full expected return of an episode: <hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
                                                                          

                                                                          
<!--l. 486--><p class="noindent" ><tspan font-family="cmmi" font-size="12">G</tspan><tspan font-family="cmmi" font-size="8">t</tspan> = <tspan font-family="cmmi" font-size="8">i</tspan><tspan font-family="cmr" font-size="8">=</tspan><tspan font-family="cmmi" font-size="8">T</tspan>
 <tspan font-family="cmex" font-size="10">&#x2211;</tspan>
 <tspan font-family="cmmi" font-size="8">i</tspan><tspan font-family="cmr" font-size="8">=0</tspan> <tspan font-family="cmmi" font-size="12">&#x03B3;</tspan><tspan font-family="cmmi" font-size="8">i</tspan><tspan font-family="cmmi" font-size="12">R</tspan>
<tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">+</tspan><tspan font-family="cmmi" font-size="8">i</tspan><tspan font-family="cmr" font-size="8">+1</tspan> = <tspan font-family="cmmi" font-size="12">R</tspan><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">+1</tspan> + <tspan font-family="cmmi" font-size="12">&#x03B3;R</tspan><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">+2</tspan> + <tspan font-family="cmsy" font-size="10">&#x22C5;&#x22C5;&#x22C5;</tspan> + <tspan font-family="cmmi" font-size="12">&#x03B3;</tspan><tspan font-family="cmmi" font-size="8">T</tspan><tspan font-family="cmsy" font-size="8">-</tspan><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmsy" font-size="8">-</tspan><tspan font-family="cmr" font-size="8">1</tspan><tspan font-family="cmmi" font-size="12">R</tspan>
<tspan font-family="cmmi" font-size="8">T</tspan>
                                                                          

                                                                          
<!--l. 489--><p class="noindent" ></div><hr class="endfigure">
<!--l. 490--><p class="noindent" >The update <tspan font-family="cmmi" font-size="12">R</tspan><sub><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">+1</tspan></sub> + <tspan font-family="cmmi" font-size="12">&#x03B3;V </tspan>(<tspan font-family="cmmi" font-size="12">S</tspan><sub><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">+1</tspan></sub>) in <tspan font-family="cmmi" font-size="12">TD</tspan>(0) can be thought of as <tspan font-family="cmmi" font-size="12">G</tspan><sub><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">:</tspan><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">+1</tspan></sub>. Naturally,
then, the update for an <tspan font-family="cmti" font-size="12">n-step </tspan>return <tspan font-family="cmmi" font-size="12">G</tspan><sub><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">:</tspan><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">+</tspan><tspan font-family="cmmi" font-size="8">n</tspan></sub> would be: <hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
                                                                          

                                                                          
<!--l. 493--><p class="noindent" ><tspan font-family="cmmi" font-size="12">G</tspan><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">:</tspan><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">+</tspan><tspan font-family="cmmi" font-size="8">n</tspan> = <tspan font-family="cmmi" font-size="8">i</tspan><tspan font-family="cmr" font-size="8">=</tspan><tspan font-family="cmmi" font-size="8">n</tspan>
 <tspan font-family="cmex" font-size="10">&#x2211;</tspan>
 <tspan font-family="cmmi" font-size="8">i</tspan><tspan font-family="cmr" font-size="8">=0</tspan> <tspan font-family="cmmi" font-size="12">&#x03B3;</tspan><tspan font-family="cmmi" font-size="8">i</tspan><tspan font-family="cmmi" font-size="12">R</tspan>
<tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">+</tspan><tspan font-family="cmmi" font-size="8">i</tspan><tspan font-family="cmr" font-size="8">+1</tspan> = <tspan font-family="cmmi" font-size="12">R</tspan><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">+1</tspan> + <tspan font-family="cmmi" font-size="12">&#x03B3;R</tspan><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">+2</tspan> + <tspan font-family="cmsy" font-size="10">&#x22C5;&#x22C5;&#x22C5;</tspan> + <tspan font-family="cmmi" font-size="12">&#x03B3;</tspan><tspan font-family="cmmi" font-size="8">n</tspan><tspan font-family="cmsy" font-size="8">-</tspan><tspan font-family="cmr" font-size="8">1</tspan><tspan font-family="cmmi" font-size="12">R</tspan>
<tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">+</tspan><tspan font-family="cmmi" font-size="8">n</tspan> + <tspan font-family="cmmi" font-size="12">&#x03B3;</tspan><tspan font-family="cmmi" font-size="8">n</tspan><tspan font-family="cmmi" font-size="12">V </tspan>(<tspan font-family="cmmi" font-size="12">S</tspan>
<tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">+</tspan><tspan font-family="cmmi" font-size="8">n</tspan>)
                                                                          

                                                                          
<!--l. 496--><p class="noindent" ></div><hr class="endfigure">
<!--l. 497--><p class="noindent" >What <tspan font-family="cmmi" font-size="12">TD</tspan>(<tspan font-family="cmmi" font-size="12">&#x03BB;</tspan>) does^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">60</tspan></sup><sup><tspan font-family="cmmi" font-size="8">,</tspan></sup>^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">52</tspan></sup>, is take an average over all <tspan font-family="cmti" font-size="12">n-step </tspan>updates (that is, the
average of <tspan font-family="cmmi" font-size="12">G</tspan><sub><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">:</tspan><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">+1</tspan></sub><tspan font-family="cmmi" font-size="12">,G</tspan><sub><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">:</tspan><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">+2</tspan></sub><tspan font-family="cmmi" font-size="12">&#x2026;</tspan><tspan font-family="cmmi" font-size="12">G</tspan><sub><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">:</tspan><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">+</tspan><tspan font-family="cmmi" font-size="8">n</tspan></sub>), where each subsequent time step return is
weighted in a decaying fashion. The eligibility trace itself is a mechanism to
perform online learning via a short-term memory vector (short term in the sense
that it lasts less than the length of an episode, compared to the long-term weight
vector of the function approximator which accumulates over the lifetime of the
agent) that parallels the weight vector of some function approximator like a neural
network, with the primary advantage of (apart from online learning) reducing
the memory space of a learning function by storing a single trace vector
compared to having to store the entire history of vectors for an episode and
performing the update then. Since we want the weights to sum to one
to produce a weighted average, we must normalize the weights. If we
consider the weight <tspan font-family="cmmi" font-size="12">&#x03BB;</tspan><sup><tspan font-family="cmmi" font-size="8">n</tspan></sup> for the <tspan font-family="cmmi" font-size="12">nth </tspan>step return, then our weighted sum is :
<hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
                                                                          

                                                                          
<!--l. 500--><p class="noindent" > <tspan font-family="cmmi" font-size="8">n</tspan><tspan font-family="cmr" font-size="8">=</tspan><tspan font-family="cmsy" font-size="8">&#x221E;</tspan>
 <tspan font-family="cmex" font-size="10">&#x2211;</tspan>
 <tspan font-family="cmmi" font-size="8">n</tspan><tspan font-family="cmr" font-size="8">=1</tspan> <tspan font-family="cmmi" font-size="12">&#x03BB;</tspan><tspan font-family="cmmi" font-size="8">n</tspan><tspan font-family="cmsy" font-size="8">-</tspan><tspan font-family="cmr" font-size="8">1</tspan> = <tspan font-family="cmmi" font-size="8">n</tspan><tspan font-family="cmr" font-size="8">=</tspan><tspan font-family="cmsy" font-size="8">&#x221E;</tspan>
 <tspan font-family="cmex" font-size="10">&#x2211;</tspan>
 <tspan font-family="cmmi" font-size="8">n</tspan><tspan font-family="cmr" font-size="8">=0</tspan> <tspan font-family="cmmi" font-size="12">&#x03BB;</tspan><tspan font-family="cmmi" font-size="8">n</tspan> =    1 ___
1 <tspan font-family="cmsy" font-size="10">- </tspan><tspan font-family="cmmi" font-size="12">&#x03BB;</tspan>
                                                                          

                                                                          
<!--l. 503--><p class="noindent" ></div><hr class="endfigure">
<!--l. 504--><p class="noindent" >By the sum of an infinite geometric series. Therefore to normalize the sum to 1,
we multiply by (1 <tspan font-family="cmsy" font-size="10">- </tspan><tspan font-family="cmmi" font-size="12">&#x03BB;</tspan>). Thus the <tspan font-family="cmmi" font-size="12">TD</tspan>(<tspan font-family="cmmi" font-size="12">&#x03BB;</tspan>) update as presented by Sutton^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">60</tspan></sup> is
defined as: <hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
                                                                          

                                                                          
<!--l. 507--><p class="noindent" ><tspan font-family="cmmi" font-size="12">G</tspan><tspan font-family="cmmi" font-size="8">&#x03BB;</tspan>
<tspan font-family="cmmi" font-size="8">t</tspan>  = (1 <tspan font-family="cmsy" font-size="10">- </tspan><tspan font-family="cmmi" font-size="12">&#x03BB;</tspan>) <tspan font-family="cmmi" font-size="8">n</tspan><tspan font-family="cmr" font-size="8">=</tspan><tspan font-family="cmsy" font-size="8">&#x221E;</tspan>
 <tspan font-family="cmex" font-size="10">&#x2211;</tspan>
 <tspan font-family="cmmi" font-size="8">n</tspan><tspan font-family="cmr" font-size="8">=1</tspan> <tspan font-family="cmmi" font-size="12">&#x03BB;</tspan><tspan font-family="cmmi" font-size="8">n</tspan><tspan font-family="cmsy" font-size="8">-</tspan><tspan font-family="cmr" font-size="8">1</tspan><tspan font-family="cmmi" font-size="12">G</tspan>
<tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">:</tspan><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">+</tspan><tspan font-family="cmmi" font-size="8">n</tspan>
                                                                          

                                                                          
<!--l. 510--><p class="noindent" ></div><hr class="endfigure">
<!--l. 511--><p class="noindent" >Which reads; the return at time <tspan font-family="cmmi" font-size="12">t </tspan>with respect to <tspan font-family="cmmi" font-size="12">&#x03BB; </tspan>is the normalized sum of all
<tspan font-family="cmmi" font-size="12">n</tspan>-step returns after timestep <tspan font-family="cmmi" font-size="12">t</tspan>. All <tspan font-family="cmmi" font-size="12">n</tspan>-step returns exceeding some terminal state
are equivalent to the expected <tspan font-family="cmmi" font-size="12">G</tspan><sub><tspan font-family="cmmi" font-size="8">t</tspan></sub> return. To make clear that <tspan font-family="cmmi" font-size="12">TD</tspan>(<tspan font-family="cmmi" font-size="12">&#x03BB;</tspan>) is a
compromise between MC methods and <tspan font-family="cmmi" font-size="12">TD</tspan>(0) depending on <tspan font-family="cmmi" font-size="12">&#x03BB;</tspan>, we can separate
the sum out: <table 
class="gather"><tr><td class="gather1"><img 
src="litrev21x.png" alt="                     n=T-t-1                     n= &#x221E;
         &#x03BB;             &#x2211;      n-1                 &#x2211;     n-1
        Gt = (1 - &#x03BB;)        &#x03BB;    Gt:t+n + (1 - &#x03BB;)      &#x03BB;    Gt:t+n
                       n=1                       n=T-t
since returns after terminal T  are equivalent to Gt and  everything  in the

  second sum  is after time T (Gt:t+n for n = T -  t &#x2192; Gt:t+T -t = Gt:T :)
                      n=T&#x2211; -t-1                     n&#x2211;=&#x221E;
         G &#x03BB;t = (1 - &#x03BB;)        &#x03BB;n- 1Gt:t+n + (1 - &#x03BB;)      &#x03BB;n-1Gt
                        n=1                       n=T -t

                   sum of an infinite geometric series :
                            n=T&#x2211;-t-1
               G&#x03BB;t = (1 - &#x03BB; )        &#x03BB;n-1Gt:t+n + &#x03BB;T -t-1Gt
                              n=1
" ><a 
 id="x1-20004r1"></a><a 
 id="x1-20005r2"></a><a 
 id="x1-20006r3"></a></td><td class="equation-label"><br />(1)<br /><br />(2)<br /><br />(3)<br /></td></tr></table>Which makes clearer the effect of <tspan font-family="cmmi" font-size="12">&#x03BB;</tspan>. If <tspan font-family="cmmi" font-size="12">&#x03BB; </tspan>goes to 1, the first sum goes to
0, and the return is just <tspan font-family="cmmi" font-size="12">G</tspan><sub><tspan font-family="cmmi" font-size="8">t</tspan></sub>, making it a MC method. If <tspan font-family="cmmi" font-size="12">&#x03BB; </tspan>goes to 0, the return is
simply <tspan font-family="cmmi" font-size="12">G</tspan><sub><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">:</tspan><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">+1</tspan></sub> (accepting the convention that 0<sup><tspan font-family="cmr" font-size="8">0</tspan></sup> = 1 for <tspan font-family="cmmi" font-size="12">&#x03BB;</tspan><sup><tspan font-family="cmr" font-size="8">0</tspan></sup><tspan font-family="cmmi" font-size="12">G</tspan><sub>
<tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">:</tspan><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">+1</tspan></sub>), and this is just
<tspan font-family="cmmi" font-size="12">TD</tspan>(0). Thus adjusting the <tspan font-family="cmmi" font-size="12">&#x03BB; </tspan>value allows us to find some comprimise
between the two ends of the spectrum, depending on the problem we face.
<br 
class="newline" /><br 
class="newline" />Notice that this (along with all the other learning methods we have previously
looked at) is what&#8217;s known as a forward view, where we perform an update to
a state based on the values of the states that succeed it. Whilst this is
useful in theory, in practice it is hard for us to know the value of future
states (without performing some initial sampling such as in MC methods).
Arguably a more useful approach would be the backwards view, where an
agent looks backwards in time to discover what led him to this point.
<br 
class="newline" /><br 
class="newline" />Without going into detail, the eligiblity trace in essence keeps track of which
weights in the neural network have contributed towards a given state valuation,
and is used to update weights in subsequent iterations^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">60</tspan></sup>. It updates weights when
the value prediction changes, proportional to the amount they contributed to that
valuation; it checks the &#8217;eligibility&#8217; of each weight to be updated. This is a
backwards view of learning; updating weights in the function approximator with
regards to their past inputs to state (or action, although in the case of
basic TD methods such as these it is concerned with state) valuations.
<br 
class="newline" /><br 
class="newline" />In the case of TD-Gammon, the version of TD in play is known as semi-gradient
<tspan font-family="cmmi" font-size="12">TD</tspan>(<tspan font-family="cmmi" font-size="12">&#x03BB;</tspan>) (semi-gradient in the sense that we are not finding the true gradient of a
function with a function approximator, we are approximating it, with respect to
some weight vector). Updates are performed as^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">60</tspan></sup>: <hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
                                                                          

                                                                          
<!--l. 526--><p class="noindent" ><tspan font-family="cmmib" font-size="10">w</tspan><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">+1</tspan> = <tspan font-family="cmmib" font-size="10">w</tspan><tspan font-family="cmmi" font-size="8">t</tspan> + <tspan font-family="cmmi" font-size="12">&#x03B1;</tspan>[<tspan font-family="cmmi" font-size="12">R</tspan><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">+1</tspan> + <tspan font-family="cmmi" font-size="12">&#x03B3;</tspan>&#x02C6;<tspan font-family="cmmi" font-size="12">v</tspan> (<tspan font-family="cmmi" font-size="12">S</tspan><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">+1</tspan><tspan font-family="cmmi" font-size="12">,</tspan><tspan font-family="cmmib" font-size="10">w</tspan>) <tspan font-family="cmsy" font-size="10">-</tspan> &#x02C6; <tspan font-family="cmmi" font-size="12">v</tspan> (<tspan font-family="cmmi" font-size="12">S</tspan><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmmi" font-size="12">,</tspan><tspan font-family="cmmib" font-size="10">w</tspan>)](<tspan font-family="cmmi" font-size="12">&#x03B3;&#x03BB;</tspan><tspan font-family="cmmib" font-size="10">z</tspan><tspan font-family="cmmi" font-size="8">t</tspan> + &#x0394;&#x02C6;<tspan font-family="cmmi" font-size="12">v</tspan> (<tspan font-family="cmmi" font-size="12">S</tspan><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmmi" font-size="12">,</tspan><tspan font-family="cmmib" font-size="10">w</tspan>))
                                                                          

                                                                          
<!--l. 529--><p class="noindent" ></div><hr class="endfigure">
<!--l. 530--><p class="noindent" >Where <img 
src="litrev22x.png" alt="&#x02C6;v"  class="circ" > (<tspan font-family="cmmi" font-size="12">S</tspan><sub><tspan font-family="cmmi" font-size="8">t</tspan></sub><tspan font-family="cmmi" font-size="12">,</tspan><tspan font-family="cmmib" font-size="10">w</tspan>) is our approximation of the value function (i.e. the output of the
function approximator) with respect to some weight vector <tspan font-family="cmmib" font-size="10">w</tspan>, <tspan font-family="cmmi" font-size="12">&#x03B1; </tspan>is our learning
rate, <tspan font-family="cmmi" font-size="12">&#x03B3; </tspan>is the discount factor and <tspan font-family="cmmib" font-size="10">z</tspan><sub><tspan font-family="cmmi" font-size="8">t</tspan></sub> is our eligiblity trace vector. An equivalent
update function is given in the TD-Gammon paper^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">58</tspan></sup>. At each moment in time,
we are finding the error - a one step TD update (in the square brackets
above), and combining it with the eligiblity trace, to produce a new weight
vector which adjusts each weight within it according to how much each
of these weights contributed to the previous valuation. We can see the
function of the eligiblity trace as tracking which weights contributed in what
proportion to the valuation by examining the above update to the eligiblity
trace vector, seen in the round brackets; the eligiblity trace is adjusted
by the gradient of the valuation function with respect to each weight.
<br 
class="newline" /><br 
class="newline" />We can also see the impact of our decay parameter <tspan font-family="cmmi" font-size="12">&#x03BB;</tspan>. If <tspan font-family="cmmi" font-size="12">&#x03BB; </tspan>= 0, the eligiblity trace
in the function above at time <tspan font-family="cmmi" font-size="12">t </tspan>is equivalent to the value gradient at time <tspan font-family="cmmi" font-size="12">t</tspan>. This
is efficively <tspan font-family="cmmi" font-size="12">TD</tspan>(0), where only the previous state is updated by the error. If <tspan font-family="cmmi" font-size="12">&#x03BB; &#x003C; </tspan>1,
states in the past will be updated (in effect, by adjusting the weight vector
parameter according to the decayed impact of these valuations), but each more
distant state is updated less and less. Earlier states receive less credit for the
current error. If <tspan font-family="cmmi" font-size="12">&#x03BB; </tspan>= 1, then previous states only decay by <tspan font-family="cmmi" font-size="12">&#x03B3;</tspan>, which is in fact just
Monte Carlo behaviour. <br 
class="newline" /><br 
class="newline" />TD-Gammon performed very well for the time, outperforming or matching many
of the contemporary world champions^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">58</tspan></sup> (additionally outperforming Tesauro&#8217;s
previous supervised learning approach to backgammon - Neurogammon^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">61</tspan></sup>),
however it was noted that from a zero-knowledge starting point and a raw board
encoding, the optimally trained and sized network only learnt to play at a strong
intermediate level, and required hand-coding of more advanced rules to be able to
play at the highest level. Furthermore, as noted by Tesauro himself^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">58</tspan></sup>, games
more complex than backgammon such as Chess and Go, would require
more than just linear function approximation of raw board variables. He
suggests finding improved board representations, however as discussed in
the later sections, solutions like AlphaGo and MuZero purely used raw
board encoding as input^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">62</tspan></sup><sup><tspan font-family="cmmi" font-size="8">,</tspan></sup>^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">63</tspan></sup>, choosing to increase algorithmic complexity
instead.
                                                                          

                                                                          
<h4 class="subsectionHead"><span class="titlemark">1.6   </span> <a 
 id="x1-210001.6"></a>Q-Learning</h4>
<!--l. 535--><p class="noindent" >Q-Learning as defined by Watkins^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">47</tspan></sup>, is a form of off-policy, model-free RL for
Markovian domains as described in Section <a 
href="#x1-80001.1.1">1.1.1<!--tex4ht:ref: markov --></a>. It is also known as off-policy TD
control, and the update is performed as follows^<sup class="textsuperscript"><tspan font-family="cmbx" font-size="10">:</tspan></sup> <hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
                                                                          

                                                                          
<!--l. 538--><p class="noindent" ><tspan font-family="cmmi" font-size="12">Q</tspan>(<tspan font-family="cmmi" font-size="12">S</tspan><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmmi" font-size="12">,A</tspan><tspan font-family="cmmi" font-size="8">t</tspan>) = <tspan font-family="cmmi" font-size="12">Q</tspan>(<tspan font-family="cmmi" font-size="12">S</tspan><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmmi" font-size="12">,A</tspan><tspan font-family="cmmi" font-size="8">t</tspan>) + <tspan font-family="cmmi" font-size="12">&#x03B1;</tspan>[<tspan font-family="cmmi" font-size="12">R</tspan><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">+1</tspan> + <tspan font-family="cmmi" font-size="12">&#x03B3;</tspan> max 
<tspan font-family="cmmi" font-size="8">a</tspan>   <tspan font-family="cmmi" font-size="12">Q</tspan>(<tspan font-family="cmmi" font-size="12">S</tspan><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">+1</tspan><tspan font-family="cmmi" font-size="12">,a</tspan>) <tspan font-family="cmsy" font-size="10">- </tspan><tspan font-family="cmmi" font-size="12">Q</tspan>(<tspan font-family="cmmi" font-size="12">S</tspan><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmmi" font-size="12">,A</tspan><tspan font-family="cmmi" font-size="8">t</tspan>)]
                                                                          

                                                                          
<!--l. 541--><p class="noindent" ></div><hr class="endfigure">
<!--l. 542--><p class="noindent" >This is clearly a TD method as it bootstraps itself with the previous Q-value, and
updates policy with the return of the next state and the next greedy action
(max <sub><tspan font-family="cmmi" font-size="8">a</tspan></sub>), thus it is off-policy; the greedy action is chosen irrespective of the policy
followed. <br 
class="newline" /><br 
class="newline" />Q-learning has been shown to suffer from overestimation, or maximization bias^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">64</tspan></sup>,
which is a systematic overestimation of Q-values which generally only arises with
the use of function approximators. This is an example of the <tspan font-family="cmti" font-size="12">deadly triad </tspan>we
referred to earlier. Q-learning was shown to converge^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">47</tspan></sup>, however this only
occurs for the tabular method (i.e Q-values stored in lookup tables). When
Q-learning; an off-policy, bootstrapping method, is combined with function
approximation, instability occurs. Namely, through the mechanism of
function approximation, noise is introduced into our Q-values, and this noise
corrupts our estimates to the extent that they suffer from positive bias^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">64</tspan></sup><sup><tspan font-family="cmmi" font-size="8">,</tspan></sup>^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">46</tspan></sup>.
<br 
class="newline" /><br 
class="newline" />Amongst a variety of solutions such as Bias-corrected Q-learning^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">65</tspan></sup>, MaxMin
Learning^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">66</tspan></sup>, a prominant proposal is known as Double Q-learning. In
essence, the overestimation problem occurs because we are using a max
operator over a set of estimates of Q-values, where these estimates are
biased. As such, we pick new values based off of biased estimates and our
new value is in turn biased. To counter this, we can use two independent
Q estimators for each state-action pair, thus reducing the variance of
our estimates via cross-validation. We have two Q estimators <tspan font-family="cmmi" font-size="12">Q</tspan><sup><tspan font-family="cmr" font-size="8">1</tspan></sup> and <tspan font-family="cmmi" font-size="12">Q</tspan><sup><tspan font-family="cmr" font-size="8">2</tspan></sup>
(or <tspan font-family="cmmi" font-size="12">Q</tspan><sup><tspan font-family="cmmi" font-size="8">A</tspan></sup> and <tspan font-family="cmmi" font-size="12">Q</tspan><sup><tspan font-family="cmmi" font-size="8">B</tspan></sup> in some of the literature), and on each timestep of the
algorithm, we choose one at random to update. An update looks like^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">67</tspan></sup><sup><tspan font-family="cmmi" font-size="8">,</tspan></sup>^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">46</tspan></sup>:
<hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
                                                                          

                                                                          
<!--l. 547--><p class="noindent" ><tspan font-family="cmmi" font-size="12">Q</tspan><tspan font-family="cmr" font-size="8">1</tspan>(<tspan font-family="cmmi" font-size="12">S</tspan>
<tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmmi" font-size="12">,A</tspan><tspan font-family="cmmi" font-size="8">t</tspan>) = <tspan font-family="cmmi" font-size="12">Q</tspan><tspan font-family="cmr" font-size="8">1</tspan>(<tspan font-family="cmmi" font-size="12">S</tspan>
<tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmmi" font-size="12">,A</tspan><tspan font-family="cmmi" font-size="8">t</tspan>) + <tspan font-family="cmmi" font-size="12">&#x03B1;</tspan>[<tspan font-family="cmmi" font-size="12">R</tspan><tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">+1</tspan> + <tspan font-family="cmmi" font-size="12">&#x03B3;Q</tspan><tspan font-family="cmr" font-size="8">2</tspan>(<tspan font-family="cmmi" font-size="12">S</tspan>
<tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">+1</tspan><tspan font-family="cmmi" font-size="12">,</tspan> arg max 
<tspan font-family="cmmi" font-size="8">a</tspan>      <tspan font-family="cmmi" font-size="12">Q</tspan><tspan font-family="cmr" font-size="8">1</tspan>(<tspan font-family="cmmi" font-size="12">S</tspan>
<tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmr" font-size="8">+1</tspan><tspan font-family="cmmi" font-size="12">,a</tspan>)) <tspan font-family="cmsy" font-size="10">- </tspan><tspan font-family="cmmi" font-size="12">Q</tspan><tspan font-family="cmr" font-size="8">1</tspan>(<tspan font-family="cmmi" font-size="12">S</tspan>
<tspan font-family="cmmi" font-size="8">t</tspan><tspan font-family="cmmi" font-size="12">,A</tspan><tspan font-family="cmmi" font-size="8">t</tspan>)]
                                                                          

                                                                          
<!--l. 550--><p class="noindent" ></div><hr class="endfigure">
<!--l. 551--><p class="noindent" >And vice versa for <tspan font-family="cmmi" font-size="12">Q</tspan><sup><tspan font-family="cmr" font-size="8">2</tspan></sup>. This elmininates the overestimation bias found in regular
Q-learning^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">67</tspan></sup><sup><tspan font-family="cmmi" font-size="8">,</tspan></sup>^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">46</tspan></sup>, however introduces a new problem of underestimation; Double
Q-learning has been shown to have multiple suboptimal fixed points^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">68</tspan></sup>, which
raises the concern that it may get stuck in local regions and struggle to find the
optimal policy. <br 
class="newline" /><br 
class="newline" />Overestimation and underestimation are not always detrimental to learning, it is
highly dependent on the environment. Lan et al.^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">66</tspan></sup> show that for cases
where our environment has highly stochastic action spaces, this is where
overestimation is most likely to occur. And in situations where highly
stochastic regions correspond to high-value regions, then overestimation
encourages exploration in this region, and lead the agent to explore the
region further. Conversely, underestimation reduces this exploration. In
situations where highly stochastic regions have high value, overestimation
may lead to higher rewards, and in situations where they are low-value,
underestimation reduces wasted time and overexploration. Both can lead to
optimal policy in the correct environment, and fail to find it in a mismatched
environment.
<h4 class="subsectionHead"><span class="titlemark">1.7   </span> <a 
 id="x1-220001.7"></a>Key Algorithms</h4>
<!--l. 554--><p class="noindent" >Having covered the fundamentals of reinforcement learning, and the various
approaches to learning, this section now begins to outline more modern
algorithms, which progressively approach our solution needed for the game of Go
in the subsequent subsections.
<h5 class="subsubsectionHead"><span class="titlemark">1.7.1   </span> <a 
 id="x1-230001.7.1"></a>DQN</h5>
<!--l. 556--><p class="noindent" >DQNs or Deep Q-Networks as studied by Mnih et al. in 2015^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">49</tspan></sup><sup><tspan font-family="cmmi" font-size="8">,</tspan></sup>^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">50</tspan></sup>, are essentially
convolutional neural networks trained with a variant of Q-learning, whose input is
a set of raw pixels and whose output is a value function. In the paper (and in
general), the term &#8217;pixel&#8217; implies the physical individual images of an image,
where in the case of this paper the images are of Atari games the network is
training to play. However, a &#8217;pixel&#8217; in terms of an input to a convolutional
network are not limited to image pixels, as is the case in the Alpha family
of algorithms^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">63</tspan></sup><sup><tspan font-family="cmmi" font-size="8">,</tspan></sup>^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">69</tspan></sup><sup><tspan font-family="cmmi" font-size="8">,</tspan></sup>^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">62</tspan></sup>, where a pixel represents a board position of the
                                                                          

                                                                          
game.
<h5 class="subsubsectionHead"><span class="titlemark">1.7.2   </span> <a 
 id="x1-240001.7.2"></a>Convolutional Neural Networks (CNNs)</h5>
<!--l. 558--><p class="noindent" >As alluded to above, the general role of CNNs is to efficiently use regular
multilayer perceptrons (described in Section <a 
href="#x1-130001.2.1">1.2.1<!--tex4ht:ref: ffnets --></a>) on large inputs, generally 2D
shapes (as is the case for image-recognition which is the most common application
of CNNs, or the board state in Go), however can be extended to higher
dimensional spaces as shown by Choy et al.^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">70</tspan></sup> Note that the common approach for
higher dimensional images (such as 3D models) is to use a pooling layer, which is
discussed shortly. Consider the case for a 1920x1080 image. This would be an
input vector with over 2 million elements; a huge computational cost to a fully
connected multilayer network. As such, the distinct feature and use of a CNN is
the initial &#8217;convolutional&#8217; layers prepended to the network, which extract the
useful features from the large input space and &#8217;convolve&#8217; them. CNNs were
inspired by and can be likened to the function of neurons in the visual
cortex; neurons &#8217;convolve&#8217; and subsample the visual stream from the eye
and reduce to (in the mind of the observer) feature maps which enable
us to extract the most important information from what we perceive^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">71</tspan></sup>.
<br 
class="newline" /><br 
class="newline" />Each neuron in a layer has a <tspan font-family="cmti" font-size="12">receptive field </tspan>in the previous layer^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">10</tspan></sup>. This receptive
field is a restricted area of the previous layer. For example, in a 30x30
image, the first input layer may have neurons with a receptive field of
5x5. This enables us to preserve locality of feature maps in subsequent
layers. This locality is important in areas such as edge detection (where
locality is critical to the discovery of edges), but more importantly in the
domain of Go, allows us to take advantage of the essential locality of
Go^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">72</tspan></sup>; the &#8217;liberties&#8217; around points on the board must be considered in the
context of their adjacency for the analysis to have any meaning at all.
<br 
class="newline" /><br 
class="newline" />A convolutional layer consists of neurons that convolve their respective receptive
fields with a given kernel to produce a feature map. Consider a 27x27 input
image, and a single filter (that is, a single kernel used to detect a single type of
feature from the image - like edges; this would really be a waste of a CNN as you
generally want to detect more than one type of feature, but for simplicity&#8217;s sake)
                                                                          

                                                                          
of size 3x3. Then assuming no overlap, and a receptive field size of 3x3, then the
number of neurons in the convolutional layer would be 81 (<img 
src="litrev23x.png" alt="27*27-
3*3"  class="frac" align="middle">). These 81
neurons encode the feature map extracted from the image, and this group of
neurons is constrained to share the same set of synaptic weights^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">10</tspan></sup>, further
reducing the computational footprint when contrasted with a regular
multilayer perceptron. This weight-sharing structure happens to match the
structure of Go excellently^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">72</tspan></sup>, as the rules of the game are translationally
invariant; that is, regardless of the receptive field captured on the board,
the same rules apply. Compare this to Chess or Shogi, where rules are
positionally-dependent (e.g. pawns moving 2 squares on the second rank and 1
square thereafter) and assymmetric (e.g. castling), thus not taking advantages of
the properties of the CNN^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">72</tspan></sup>. Interesting to note as well is that locality is not
preserved in either of these games; pieces may make long-distance moves.
<br 
class="newline" /><br 
class="newline" />Following a convolutional layer you would typically have a pooling layer^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">10</tspan></sup>, which
subsample the feature maps produced from the previous layer and further reduce
the size via Max Pooling^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">73</tspan></sup> (retains the most prominent features of the feature
map) or Average Pooling (retains the average values of features of the feature
map). These are two of the most common pooling techniques; there are many
more, with different techniques best suited to different domains. With deep
networks, there may be multiple layers of convolution and pooling. Like the use of
deep networks to learn &#8217;higher-order&#8217; statistics mentioned in Section <a 
href="#x1-130001.2.1">1.2.1<!--tex4ht:ref: ffnets --></a>, Deep
CNNs have the ability to learn more general patterns in input. For example, the
first layer may learn edges, a second may learn eyes, the third may learn noses etc.
The final layer results in a &#8217;global&#8217; perspective, i.e. facial recognition. As each
convolutional layer typically has multiple filters, higher dimensional but smaller
resolution layers are produced (e.g. starting with a 27x27 image, with
3 filters, we have 3x(9x9) in the next layer; 81 <tspan font-family="cmsy" font-size="10">* </tspan>3 = 243 neurons). As
such, the output of the final pooling layer will have a small resolution
and a high dimensionality. This shape is then fed into a regular fully
connected multilayer network, and proceeds as described in Section <a 
href="#x1-130001.2.1">1.2.1<!--tex4ht:ref: ffnets --></a>.
<br 
class="newline" /><br 
class="newline" />There are many other subtleties of CNNs, however this covers the basic outline of
the process. Returning to DQNs, in the seminal study conducted by Mnih et
al.^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">50</tspan></sup><sup><tspan font-family="cmmi" font-size="8">,</tspan></sup>^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">49</tspan></sup> the learning task consisted of a set of Atari 2600 games. The CNN
architecture was as follows; an 84x84x4 input layer consisting of the raw pixel
                                                                          

                                                                          
input from the Atari 2600 emulator, the first convolutional layer convolves 16 8x8
filters with a ReLU activation function (a common function used in recent years
for deep learning), second convolutional layer consisting of 32 4x4 filters, followed
by 2 fully connected layers whose output is a probability output for each valid
action. This architecture lacks pooling layers, which is likely due to the small
input space. <br 
class="newline" /><br 
class="newline" />This learning architecture consists of off-policy, bootstrapping and function
approximation, and as a result suffers from the instabiity and divergence of the
<tspan font-family="cmti" font-size="12">deadly triad </tspan>mentioned earlier. To rectify this, experience replay was
employed^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">49</tspan></sup>. This is the variant of Q-learning previous mentioned; rather than
evaluating the Q-value function based on state-action pairs as is conventionally
done, a data set of previous experience of the agent in the environment is
stored. Q-value updates are then performed using a random uniformm
sample of this data set. Advantages of this approach include more efficient
use of previous experience, which can be a benefit in an environment
such as these Atari games where experience is costly (i.e. time cost due
to the games being configured for a human-level speed of input), and
primarily has been shown to outperform Q-learning in most cases in terms of
convergence time^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">74</tspan></sup>. Preliminary, non peer-reviewed (as of time of writing)
proof has been published by Szlak et al.^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">75</tspan></sup> to demonstrate theoretical
convergence of Q-learning with Experience Replay, with experimental evidence
supporting.
<h5 class="subsubsectionHead"><span class="titlemark">1.7.3   </span> <a 
 id="x1-250001.7.3"></a>DDQN</h5>
<!--l. 565--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">1.7.4   </span> <a 
 id="x1-260001.7.4"></a>AlphaZero</h5>
<!--l. 566--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">1.7.5   </span> <a 
 id="x1-270001.7.5"></a>MuZero</h5>
                                                                          

                                                                          
<!--l. 567--><p class="noindent" >
<h3 class="sectionHead"><span class="titlemark">2   </span> <a 
 id="x1-280002"></a>The Game</h3>
<!--l. 568--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">2.1   </span> <a 
 id="x1-290002.1"></a>Common Fate Graph (CFG)</h4>
<!--l. 569--><p class="noindent" >
<h3 class="sectionHead"><span class="titlemark">3   </span> <a 
 id="x1-300003"></a>The Analysis Engine</h3>
<!--l. 570--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">3.1   </span> <a 
 id="x1-310003.1"></a>Supervised Learning</h4>
<!--l. 571--><p class="noindent" >
<h3 class="sectionHead"><span class="titlemark">4   </span> <a 
 id="x1-320004"></a>The Website</h3>
<!--l. 572--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">4.0.1   </span> <a 
 id="x1-330004.0.1"></a>The Pole-Balancing Problem</h5>
<!--l. 573--><p class="noindent" >The pole-balancing problem as explored by Michie^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">76</tspan></sup> and Barto, Sutton
and Anderson^<sup class="textsuperscript"><tspan font-family="cmr" font-size="10">77</tspan></sup>, is a benchmark problem of control systems theory and
procedural learning. As the name suggests, the skill the agent must aquire
is to balance a pole. See below for an illustration of the environment:
<hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
<a 
 id="x1-33001r16"></a>
                                                                          

                                                                          
<!--l. 577--><p class="noindent" ><img 
src="1.png" alt="PIC"  
width="43" height="43" >
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;16: </span><span  
class="content">The Pole-Balancing Problem</span></div><!--tex4ht:label?: x1-33001r16 -->
                                                                          

                                                                          
<!--l. 580--><p class="noindent" ></div><hr class="endfigure">
<!--l. 581--><p class="noindent" >The cart is free to move along the track between the two endstops (shown with
arrows on Figure <a 
href="#x1-33001r16">16<!--tex4ht:ref: polebalance --></a>), and the pole is attached to the cart with a hinge, and is free
to move in the direction shown above the pole in Figure <a 
href="#x1-33001r16">16<!--tex4ht:ref: polebalance --></a>. The two
control actions an agent can take are to push the cart in either the left
or right direction. The MDP modelling the above problem would be as
follows:
      <ul class="itemize1">
      <li class="itemize"><tspan font-family="cmmi" font-size="12">S</tspan>
      </li>
      <li class="itemize"><tspan font-family="cmmi" font-size="12">A </tspan>is the set of actions LEFT,RIGHT indicating the direction to move
      the cart</li></ul>
<h3 class="sectionHead"><a 
 id="x1-340004.0.1"></a>References</h3>
<!--l. 587--><p class="noindent" >
      <dl class="thebibliography"><dt id="X0-Sutton2012-uz" class="thebibliography">
 [1]  </dt><dd 
id="bib-1" class="thebibliography">
      <!--l. 587--><p class="noindent" >Richard  S  Sutton,  ed.  <tspan font-family="cmti" font-size="12">Reinforcement  Learning</tspan>.  en.  The  Springer
      International Series in Engineering and Computer Science. New York,
      NY: Springer, Oct. 2012.
      </dd><dt id="X0-watkins1989learning" class="thebibliography">
 [2]  </dt><dd 
id="bib-2" class="thebibliography">
      <!--l. 587--><p class="noindent" >Christopher John Cornish Hellaby Watkins. &#8220;Learning from delayed
      rewards&#8221;. In: (1989).
      </dd><dt id="X0-bellman1957markovian" class="thebibliography">
 [3]  </dt><dd 
id="bib-3" class="thebibliography">
      <!--l. 587--><p class="noindent" >Richard  Bellman.  &#8220;A  Markovian  decision  process&#8221;.  In:  <tspan font-family="cmti" font-size="12">Journal  of</tspan>
      <tspan font-family="cmti" font-size="12">mathematics and mechanics </tspan>6.5 (1957), pp. 679&#8211;684.
      </dd><dt id="X0-brooks2011handbook" class="thebibliography">
 [4]  </dt><dd 
id="bib-4" class="thebibliography">
      <!--l. 587--><p class="noindent" >S.                Brooks               et               al.                <tspan font-family="cmti" font-size="12">Handbook</tspan>
      <tspan font-family="cmti" font-size="12">of Markov Chain Monte Carlo</tspan>. Chapman &amp; Hall/CRC Handbooks of
      Modern Statistical Methods. CRC Press, 2011. <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">i</span><span 
class="small-caps">s</span><span 
class="small-caps">b</span><span 
class="small-caps">n</span></tspan>: 9781420079425.
      <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">u</span><span 
class="small-caps">r</span><span 
class="small-caps">l</span></tspan>: <a 
href="https://books.google.co.uk/books?id=qfRsAIKZ4rIC" class="url" ><tspan font-family="cmtt" font-size="12">https://books.google.co.uk/books?id=qfRsAIKZ4rIC</tspan></a>.
                                                                          

                                                                          
      </dd><dt id="X0-1976Ttos" class="thebibliography">
 [5]  </dt><dd 
id="bib-5" class="thebibliography">
      <!--l. 587--><p class="noindent" >&#8220;The theory of stochastic processes: I. I. Gihman and A. V. Skorohod, I,
      Springer, 1974, 570 pp&#8221;. eng. In: <tspan font-family="cmti" font-size="12">Advances in Mathematics </tspan>19.3 (1976),
      pp. 419&#8211;419. <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">i</span><span 
class="small-caps">s</span><span 
class="small-caps">s</span><span 
class="small-caps">n</span></tspan>: 0001-8708.
      </dd><dt id="X0-thomas2015notation" class="thebibliography">
 [6]  </dt><dd 
id="bib-6" class="thebibliography">
      <!--l. 587--><p class="noindent" >Philip  S  Thomas  and  Billy  Okal.  &#8220;A  notation  for  Markov  decision
      processes&#8221;. In: <tspan font-family="cmti" font-size="12">arXiv preprint arXiv:1512.09075 </tspan>(2015).
      </dd><dt id="X0-haggstrom2002finite" class="thebibliography">
 [7]  </dt><dd 
id="bib-7" class="thebibliography">
      <!--l. 587--><p class="noindent" >Olle   Hggstrm   et   al.   <tspan font-family="cmti" font-size="12">Finite  Markov  chains  and  algorithmic</tspan>
      <tspan font-family="cmti" font-size="12">applications</tspan>. 52. Cambridge University Press, 2002.
      </dd><dt id="X0-mcculloch1943logical" class="thebibliography">
 [8]  </dt><dd 
id="bib-8" class="thebibliography">
      <!--l. 587--><p class="noindent" >Warren  S  McCulloch  and  Walter  Pitts.  &#8220;A  logical  calculus  of  the
      ideas immanent in nervous activity&#8221;. In: <tspan font-family="cmti" font-size="12">The bulletin of mathematical</tspan>
      <tspan font-family="cmti" font-size="12">biophysics </tspan>5.4 (1943), pp. 115&#8211;133.
      </dd><dt id="X0-werbos1974beyond" class="thebibliography">
 [9]  </dt><dd 
id="bib-9" class="thebibliography">
      <!--l. 587--><p class="noindent" >Paul  Werbos.  &#8220;Beyond  regression:&#8221;  new  tools  for  prediction  and
      analysis in the behavioral sciences&#8221;. In: <tspan font-family="cmti" font-size="12">Ph. D. dissertation, Harvard</tspan>
      <tspan font-family="cmti" font-size="12">University </tspan>(1974).
      </dd><dt id="X0-haykin2010neural" class="thebibliography">
[10]  </dt><dd 
id="bib-10" class="thebibliography">
      <!--l. 587--><p class="noindent" >S.              Haykin.              <tspan font-family="cmti" font-size="12">Neural            Networks            and</tspan>
      <tspan font-family="cmti" font-size="12">Learning Machines, 3/e</tspan>. PHI Learning, 2010. <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">i</span><span 
class="small-caps">s</span><span 
class="small-caps">b</span><span 
class="small-caps">n</span></tspan>: 9789332586253.
      <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">u</span><span 
class="small-caps">r</span><span 
class="small-caps">l</span></tspan>: <a 
href="https://books.google.se/books?id=ivK0DwAAQBAJ" class="url" ><tspan font-family="cmtt" font-size="12">https://books.google.se/books?id=ivK0DwAAQBAJ</tspan></a>.
      </dd><dt id="X0-dodge2003the" class="thebibliography">
[11]  </dt><dd 
id="bib-11" class="thebibliography">
      <!--l. 587--><p class="noindent" >Yadolah Dodge. <tspan font-family="cmti" font-size="12">The Oxford dictionary of statistical terms</tspan>. Oxford New
      York: Oxford University Press, 2003. <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">i</span><span 
class="small-caps">s</span><span 
class="small-caps">b</span><span 
class="small-caps">n</span></tspan>: 0-19-850994-4.
      </dd><dt id="X0-1995FNtA" class="thebibliography">
[12]  </dt><dd 
id="bib-12" class="thebibliography">
                                                                          

                                                                          
      <!--l. 587--><p class="noindent" ><tspan font-family="cmti" font-size="12">From  Natural  to  Artificial  Neural  Computation  :  International</tspan>
      <tspan font-family="cmti" font-size="12">Workshop on Artificial Neural Networks, Malaga-Torremolinos, Spain,</tspan>
      <tspan font-family="cmti" font-size="12">June 7 &#8211; 9, 1995 Proceedings</tspan>.  eng.  1st  ed.  1995.  Lecture  Notes  in
      Computer Science, 930. Berlin, Heidelberg: Springer Berlin Heidelberg
      : Imprint: Springer, 1995. <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">i</span><span 
class="small-caps">s</span><span 
class="small-caps">b</span><span 
class="small-caps">n</span></tspan>: 3-540-49288-7.
      </dd><dt id="X0-Keeler1712" class="thebibliography">
[13]  </dt><dd 
id="bib-13" class="thebibliography">
      <!--l. 587--><p class="noindent" >J  D  Keeler,  E  E  Pichler,  and  J  Ross.  &#8220;Noise  in  neural  networks:
      thresholds,  hysteresis,  and  neuromodulation  of  signal-to-noise&#8221;.  In:
      <tspan font-family="cmti" font-size="12">Proceedings  of  the  National  Academy  of  Sciences   </tspan>86.5  (1989),
      pp.  1712&#8211;1716.  <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">i</span><span 
class="small-caps">s</span><span 
class="small-caps">s</span><span 
class="small-caps">n</span></tspan>:  0027-8424.  <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">d</span><span 
class="small-caps">o</span><span 
class="small-caps">i</span></tspan>:  <a 
href="10.1073/pnas.86.5.1712" class="url" ><tspan font-family="cmtt" font-size="12">10.1073/pnas.86.5.1712</tspan></a>.
      eprint: <a 
href="https://www.pnas.org/content/86/5/1712.full.pdf" class="url" ><tspan font-family="cmtt" font-size="12">https://www.pnas.org/content/86/5/1712.full.pdf</tspan></a>. <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">u</span><span 
class="small-caps">r</span><span 
class="small-caps">l</span></tspan>:
      <a 
href="https://www.pnas.org/content/86/5/1712" class="url" ><tspan font-family="cmtt" font-size="12">https://www.pnas.org/content/86/5/1712</tspan></a>.
      </dd><dt id="X0-rubin1915synsoplevede" class="thebibliography">
[14]  </dt><dd 
id="bib-14" class="thebibliography">
      <!--l. 587--><p class="noindent" >E.   Rubin.   <tspan font-family="cmti" font-size="12">Synsoplevede  Figurer</tspan>.   v.   1.   Gyldendal,   1915.   <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">u</span><span 
class="small-caps">r</span><span 
class="small-caps">l</span></tspan>:
      <a 
href="https://books.google.co.uk/books?id=1vGmnQAACAAJ" class="url" ><tspan font-family="cmtt" font-size="12">https://books.google.co.uk/books?id=1vGmnQAACAAJ</tspan></a>.
      </dd><dt id="X0-rosenblatt1958perceptron" class="thebibliography">
[15]  </dt><dd 
id="bib-15" class="thebibliography">
      <!--l. 587--><p class="noindent" >Frank   Rosenblatt.   &#8220;The   perceptron:   a   probabilistic   model   for
      information storage and organization in the brain.&#8221; In: <tspan font-family="cmti" font-size="12">Psychological</tspan>
      <tspan font-family="cmti" font-size="12">review </tspan>65.6 (1958), p. 386.
      </dd><dt id="X0-8093960" class="thebibliography">
[16]  </dt><dd 
id="bib-16" class="thebibliography">
      <!--l. 587--><p class="noindent" >Marvin Minsky, Seymour A. Papert, and Leon Bottou. &#8220;Introduction&#8221;.
      In: <tspan font-family="cmti" font-size="12">Perceptrons: An Introduction to Computational Geometry</tspan>. 2017,
      pp. 1&#8211;20.
      </dd><dt id="X0-8308186" class="thebibliography">
[17]  </dt><dd 
id="bib-17" class="thebibliography">
      <!--l. 587--><p class="noindent" >Saad        Albawi,        Tareq        Abed        Mohammed,        and
      Saad Al-Zawi. &#8220;Understanding of a convolutional neural network&#8221;. In:
      <tspan font-family="cmti" font-size="12">2017 International Conference on Engineering and Technology (ICET)</tspan>.
      2017, pp. 1&#8211;6. <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">d</span><span 
class="small-caps">o</span><span 
class="small-caps">i</span></tspan>: <a 
href="10.1109/ICEngTechnol.2017.8308186" class="url" ><tspan font-family="cmtt" font-size="12">10.1109/ICEngTechnol.2017.8308186</tspan></a>.
      </dd><dt id="X0-ChurchlandPatriciaS2016CO" class="thebibliography">
[18]  </dt><dd 
id="bib-18" class="thebibliography">
                                                                          

                                                                          
      <!--l. 587--><p class="noindent" >Patricia  S  Churchland  and  Terrence  J  Sejnowski.  &#8220;Computational
      Overview&#8221;. eng. In: <tspan font-family="cmti" font-size="12">The Computational Brain</tspan>. The MIT Press, 2016.
      <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">i</span><span 
class="small-caps">s</span><span 
class="small-caps">b</span><span 
class="small-caps">n</span></tspan>: 0262533391.
      </dd><dt id="X0-Q+A_SAS" class="thebibliography">
[19]  </dt><dd 
id="bib-19" class="thebibliography">
      <!--l. 587--><p class="noindent" >Warren Sarle. <tspan font-family="cmti" font-size="12">comp.ai.neural-nets FAQ</tspan>. Tech. rep. SAS Institute Inc.,
      Cary, NC, USA, 2002.
      </dd><dt id="X0-cybenko1989approximations" class="thebibliography">
[20]  </dt><dd 
id="bib-20" class="thebibliography">
      <!--l. 587--><p class="noindent" >George Cybenko. &#8220;Approximations by superpositions of a sigmoidal
      function&#8221;. In: <tspan font-family="cmti" font-size="12">Mathematics of Control, Signals and Systems </tspan>2 (1989),
      pp. 183&#8211;192.
      </dd><dt id="X0-hornik1991approximation" class="thebibliography">
[21]  </dt><dd 
id="bib-21" class="thebibliography">
      <!--l. 587--><p class="noindent" >Kurt Hornik. &#8220;Approximation capabilities of multilayer feedforward
      networks&#8221;. In: <tspan font-family="cmti" font-size="12">Neural networks </tspan>4.2 (1991), pp. 251&#8211;257.
      </dd><dt id="X0-248452" class="thebibliography">
[22]  </dt><dd 
id="bib-22" class="thebibliography">
      <!--l. 587--><p class="noindent" >R. Reed. &#8220;Pruning algorithms-a survey&#8221;. In: <tspan font-family="cmti" font-size="12">IEEE Transactions on</tspan>
      <tspan font-family="cmti" font-size="12">Neural Networks </tspan>4.5 (1993), pp. 740&#8211;747. <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">d</span><span 
class="small-caps">o</span><span 
class="small-caps">i</span></tspan>: <a 
href="10.1109/72.248452" class="url" ><tspan font-family="cmtt" font-size="12">10.1109/72.248452</tspan></a>.
      </dd><dt id="X0-simpson1991artificial" class="thebibliography">
[23]  </dt><dd 
id="bib-23" class="thebibliography">
      <!--l. 587--><p class="noindent" >Patrick K Simpson. <tspan font-family="cmti" font-size="12">Artificial neural systems: foundations, paradigms,</tspan>
      <tspan font-family="cmti" font-size="12">applications, and implementations</tspan>. McGraw-Hill, Inc., 1991.
      </dd><dt id="X0-118638" class="thebibliography">
[24]  </dt><dd 
id="bib-24" class="thebibliography">
      <!--l. 587--><p class="noindent" >Hecht-Nielsen.  &#8220;Theory  of  the  backpropagation  neural  network&#8221;.
      In:  <tspan font-family="cmti" font-size="12">International 1989 Joint Conference on Neural Networks</tspan>.  1989,
      593&#8211;605 vol.1. <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">d</span><span 
class="small-caps">o</span><span 
class="small-caps">i</span></tspan>: <a 
href="10.1109/IJCNN.1989.118638" class="url" ><tspan font-family="cmtt" font-size="12">10.1109/IJCNN.1989.118638</tspan></a>.
      </dd><dt id="X0-zurada1992introduction" class="thebibliography">
[25]  </dt><dd 
id="bib-25" class="thebibliography">
      <!--l. 587--><p class="noindent" >Jacek   Zurada.   <tspan font-family="cmti" font-size="12">Introduction   to   artificial   neural   systems</tspan>.   West
      Publishing Co., 1992.
      </dd><dt id="X0-rumelhart1986learning" class="thebibliography">
[26]  </dt><dd 
id="bib-26" class="thebibliography">
                                                                          

                                                                          
      <!--l. 587--><p class="noindent" >David  E  Rumelhart,  Geoffrey  E  Hinton,  and  Ronald  J  Williams.
      &#8220;Learning  representations  by  back-propagating  errors&#8221;.  In:  <tspan font-family="cmti" font-size="12">nature</tspan>
      323.6088 (1986), pp. 533&#8211;536.
      </dd><dt id="X0-bryson2018applied" class="thebibliography">
[27]  </dt><dd 
id="bib-27" class="thebibliography">
      <!--l. 587--><p class="noindent" >Arthur E Bryson and Yu-Chi Ho. <tspan font-family="cmti" font-size="12">Applied optimal control: optimization,</tspan>
      <tspan font-family="cmti" font-size="12">estimation, and control</tspan>. Routledge, 2018.
      </dd><dt id="X0-parker1987optimal" class="thebibliography">
[28]  </dt><dd 
id="bib-28" class="thebibliography">
      <!--l. 587--><p class="noindent" >David B Parker. &#8220;Optimal algorithms for adaptive networks: Second
      order  backpropagation,  second  order  direct  backpropagation,  and
      second   order   hebbing   learning&#8221;.   In:   <tspan font-family="cmti" font-size="12">IEEE  International  Joint</tspan>
      <tspan font-family="cmti" font-size="12">Conference on Neural Networks, 1987</tspan>. 1987.
      </dd><dt id="X0-ref1" class="thebibliography">
[29]  </dt><dd 
id="bib-29" class="thebibliography">
      <!--l. 587--><p class="noindent" >&#8220;Mean Squared Error&#8221;. In: <tspan font-family="cmti" font-size="12">Encyclopedia of Machine Learning</tspan>. Ed. by
      Claude                                                                        Sammut
      and Geoffrey I. Webb. Boston, MA: Springer US, 2010, pp. 653&#8211;653.
      <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">i</span><span 
class="small-caps">s</span><span 
class="small-caps">b</span><span 
class="small-caps">n</span></tspan>:   978-0-387-30164-8.   <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">d</span><span 
class="small-caps">o</span><span 
class="small-caps">i</span></tspan>:   <a 
href="10.1007/978-0-387-30164-8_528" class="url" ><tspan font-family="cmtt" font-size="12">10.1007/978-0-387-30164-8_528</tspan></a>.
      <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">u</span><span 
class="small-caps">r</span><span 
class="small-caps">l</span></tspan>: <a 
href="https://doi.org/10.1007/978-0-387-30164-8_528" class="url" ><tspan font-family="cmtt" font-size="12">https://doi.org/10.1007/978-0-387-30164-8_528</tspan></a>.
      </dd><dt id="X0-10.1007/978-3-642-70911-1_15" class="thebibliography">
[30]  </dt><dd 
id="bib-30" class="thebibliography">
      <!--l. 587--><p class="noindent" >G.  L.  Shaw.  &#8220;Donald  Hebb:  The  Organization  of  Behavior&#8221;.  In:
      <tspan font-family="cmti" font-size="12">Brain  Theory</tspan>.  Ed.  by  Gnther  Palm  and  Ad  Aertsen.  Berlin,
      Heidelberg:  Springer  Berlin  Heidelberg,  1986,  pp.  231&#8211;233.  <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">i</span><span 
class="small-caps">s</span><span 
class="small-caps">b</span><span 
class="small-caps">n</span></tspan>:
      978-3-642-70911-1.
      </dd><dt id="X0-doi:10.1126/science.1372754" class="thebibliography">
[31]  </dt><dd 
id="bib-31" class="thebibliography">
      <!--l. 587--><p class="noindent" >Siegrid
      Lwel and Wolf Singer. &#8220;Selection of Intrinsic Horizontal Connections
      in the Visual Cortex by Correlated Neuronal Activity&#8221;. In: <tspan font-family="cmti" font-size="12">Science</tspan>
      255.5041 (1992), pp. 209&#8211;212. <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">d</span><span 
class="small-caps">o</span><span 
class="small-caps">i</span></tspan>: <a 
href="10.1126/science.1372754" class="url" ><tspan font-family="cmtt" font-size="12">10.1126/science.1372754</tspan></a>. eprint:
      <a 
href="https://www.science.org/doi/pdf/10.1126/science.1372754" class="url" ><tspan font-family="cmtt" font-size="12">https://www.science.org/doi/pdf/10.1126/science.1372754</tspan></a>.
      <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">u</span><span 
class="small-caps">r</span><span 
class="small-caps">l</span></tspan>:
      <a 
href="https://www.science.org/doi/abs/10.1126/science.1372754" class="url" ><tspan font-family="cmtt" font-size="12">https://www.science.org/doi/abs/10.1126/science.1372754</tspan></a>.
                                                                          

                                                                          
      </dd><dt id="X0-rumelhart1995backpropagation" class="thebibliography">
[32]  </dt><dd 
id="bib-32" class="thebibliography">
      <!--l. 587--><p class="noindent" >David  E  Rumelhart  et  al.  &#8220;Backpropagation:  The  basic  theory&#8221;.
      In:  <tspan font-family="cmti" font-size="12">Backpropagation:  Theory,  architectures  and  applications  </tspan>(1995),
      pp. 1&#8211;34.
      </dd><dt id="X0-279210" class="thebibliography">
[33]  </dt><dd 
id="bib-33" class="thebibliography">
      <!--l. 587--><p class="noindent" >G. Kechriotis and E.S. Manolakos.  &#8220;Training fully recurrent neural
      networks with complex weights&#8221;. In: <tspan font-family="cmti" font-size="12">IEEE Transactions on Circuits</tspan>
      <tspan font-family="cmti" font-size="12">and  Systems  II:  Analog  and  Digital  Signal  Processing  </tspan>41.3  (1994),
      pp. 235&#8211;238. <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">d</span><span 
class="small-caps">o</span><span 
class="small-caps">i</span></tspan>: <a 
href="10.1109/82.279210" class="url" ><tspan font-family="cmtt" font-size="12">10.1109/82.279210</tspan></a>.
      </dd><dt id="X0-bullinaria2013recurrent" class="thebibliography">
[34]  </dt><dd 
id="bib-34" class="thebibliography">
      <!--l. 587--><p class="noindent" >John   A   Bullinaria.   &#8220;Recurrent   neural   networks&#8221;.   In:   <tspan font-family="cmti" font-size="12">Neural</tspan>
      <tspan font-family="cmti" font-size="12">Computation: Lecture </tspan>12 (2013).
      </dd><dt id="X0-Bellman:DynamicProgramming" class="thebibliography">
[35]  </dt><dd 
id="bib-35" class="thebibliography">
      <!--l. 587--><p class="noindent" >Richard Bellman. <tspan font-family="cmti" font-size="12">Dynamic Programming</tspan>. Dover Publications, 1957.
      <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">i</span><span 
class="small-caps">s</span><span 
class="small-caps">b</span><span 
class="small-caps">n</span></tspan>: 9780486428093.
      </dd><dt id="X0-rlintrochap5" class="thebibliography">
[36]  </dt><dd 
id="bib-36" class="thebibliography">
      <!--l. 587--><p class="noindent" >Richard S. Sutton and Andrew G. Barto. <tspan font-family="cmti" font-size="12">Reinforcement Learning: An</tspan>
      <tspan font-family="cmti" font-size="12">Introduction</tspan>. Cambridge, MA, USA: A Bradford Book, 2018. Chap.&#x00A0;5.
      <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">i</span><span 
class="small-caps">s</span><span 
class="small-caps">b</span><span 
class="small-caps">n</span></tspan>: 0262039249.
      </dd><dt id="X0-book:91856322" class="thebibliography">
[37]  </dt><dd 
id="bib-37" class="thebibliography">
      <!--l. 587--><p class="noindent" >Martin  L.  Puterman.  <tspan font-family="cmti" font-size="12">Markov decision processes: discrete stochastic</tspan>
      <tspan font-family="cmti" font-size="12">dynamic  programming</tspan>.   1st&#x00A0;ed.   Wiley   series   in   probability   and
      mathematical statistics. John Wiley &amp; Sons, 1994. <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">i</span><span 
class="small-caps">s</span><span 
class="small-caps">b</span><span 
class="small-caps">n</span></tspan>: 0471727822;
      9780471727828.
      </dd><dt id="X0-rlintrochap4" class="thebibliography">
[38]  </dt><dd 
id="bib-38" class="thebibliography">
      <!--l. 587--><p class="noindent" >Richard S. Sutton and Andrew G. Barto. <tspan font-family="cmti" font-size="12">Reinforcement Learning: An</tspan>
      <tspan font-family="cmti" font-size="12">Introduction</tspan>. Cambridge, MA, USA: A Bradford Book, 2018. Chap.&#x00A0;4.
      <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">i</span><span 
class="small-caps">s</span><span 
class="small-caps">b</span><span 
class="small-caps">n</span></tspan>: 0262039249.
                                                                          

                                                                          
      </dd><dt id="X0-10.1007/BFb0026709" class="thebibliography">
[39]  </dt><dd 
id="bib-39" class="thebibliography">
      <!--l. 587--><p class="noindent" >Doina Precup, Richard S. Sutton, and Satinder Singh. &#8220;Theoretical
      results on reinforcement learning with temporally abstract options&#8221;.
      In: <tspan font-family="cmti" font-size="12">Machine Learning: ECML-98</tspan>. Ed. by Claire Ndellec and Cline
      Rouveirol.  Berlin,  Heidelberg:  Springer  Berlin  Heidelberg,  1998,
      pp. 382&#8211;393. <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">i</span><span 
class="small-caps">s</span><span 
class="small-caps">b</span><span 
class="small-caps">n</span></tspan>: 978-3-540-69781-7.
      </dd><dt id="X0-barto1995learning" class="thebibliography">
[40]  </dt><dd 
id="bib-40" class="thebibliography">
      <!--l. 587--><p class="noindent" >Andrew G Barto, Steven J Bradtke, and Satinder P Singh. &#8220;Learning
      to act using real-time dynamic programming&#8221;. In: <tspan font-family="cmti" font-size="12">Artificial intelligence</tspan>
      72.1-2 (1995), pp. 81&#8211;138.
      </dd><dt id="X0-SobolI.M.IliaMeerovich1994Apft" class="thebibliography">
[41]  </dt><dd 
id="bib-41" class="thebibliography">
      <!--l. 587--><p class="noindent" >I. M. (Il&#8217;ia Meerovich) Sobol&#8217;. <tspan font-family="cmti" font-size="12">A primer for the Monte Carlo method</tspan>.
      eng. Boca Raton ; London: CRC, 1994. <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">i</span><span 
class="small-caps">s</span><span 
class="small-caps">b</span><span 
class="small-caps">n</span></tspan>: 9780849386732.
      </dd><dt id="X0-10.2307/2280232" class="thebibliography">
[42]  </dt><dd 
id="bib-42" class="thebibliography">
      <!--l. 587--><p class="noindent" >Nicholas
      Metropolis and S. Ulam. &#8220;The Monte Carlo Method&#8221;. In: <tspan font-family="cmti" font-size="12">Journal of</tspan>
      <tspan font-family="cmti" font-size="12">the American Statistical Association </tspan>44.247 (1949), pp. 335&#8211;341. <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">i</span><span 
class="small-caps">s</span><span 
class="small-caps">s</span><span 
class="small-caps">n</span></tspan>:
      01621459. <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">u</span><span 
class="small-caps">r</span><span 
class="small-caps">l</span></tspan>: <a 
href="http://www.jstor.org/stable/2280232" class="url" ><tspan font-family="cmtt" font-size="12">http://www.jstor.org/stable/2280232</tspan></a>.
      </dd><dt id="X0-Robert2004" class="thebibliography">
[43]  </dt><dd 
id="bib-43" class="thebibliography">
      <!--l. 587--><p class="noindent" >Christian P. Robert and George Casella. &#8220;Monte Carlo Integration&#8221;. In:
      <tspan font-family="cmti" font-size="12">Monte Carlo Statistical Methods</tspan>. New York, NY: Springer New York,
      2004,                                    pp.                                    79&#8211;122.
      <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">i</span><span 
class="small-caps">s</span><span 
class="small-caps">b</span><span 
class="small-caps">n</span></tspan>: 978-1-4757-4145-2. <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">d</span><span 
class="small-caps">o</span><span 
class="small-caps">i</span></tspan>: <a 
href="10.1007/978-1-4757-4145-2_3" class="url" ><tspan font-family="cmtt" font-size="12">10.1007/978-1-4757-4145-2_3</tspan></a>. <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">u</span><span 
class="small-caps">r</span><span 
class="small-caps">l</span></tspan>:
      <a 
href="https://doi.org/10.1007/978-1-4757-4145-2_3" class="url" ><tspan font-family="cmtt" font-size="12">https://doi.org/10.1007/978-1-4757-4145-2_3</tspan></a>.
      </dd><dt id="X0-sutton1984temporal" class="thebibliography">
[44]  </dt><dd 
id="bib-44" class="thebibliography">
      <!--l. 587--><p class="noindent" >Richard Stuart Sutton. &#8220;Temporal credit assignment in reinforcement
      learning&#8221;. PhD thesis. University of Massachusetts Amherst, 1984.
      </dd><dt id="X0-rlintrochap1" class="thebibliography">
[45]  </dt><dd 
id="bib-45" class="thebibliography">
                                                                          

                                                                          
      <!--l. 587--><p class="noindent" >Richard S. Sutton and Andrew G. Barto. <tspan font-family="cmti" font-size="12">Reinforcement Learning: An</tspan>
      <tspan font-family="cmti" font-size="12">Introduction</tspan>. Cambridge, MA, USA: A Bradford Book, 2018. Chap.&#x00A0;1.
      <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">i</span><span 
class="small-caps">s</span><span 
class="small-caps">b</span><span 
class="small-caps">n</span></tspan>: 0262039249.
      </dd><dt id="X0-rlintrochap6" class="thebibliography">
[46]  </dt><dd 
id="bib-46" class="thebibliography">
      <!--l. 587--><p class="noindent" >Richard S. Sutton and Andrew G. Barto. <tspan font-family="cmti" font-size="12">Reinforcement Learning: An</tspan>
      <tspan font-family="cmti" font-size="12">Introduction</tspan>. Cambridge, MA, USA: A Bradford Book, 2018. Chap.&#x00A0;6.
      <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">i</span><span 
class="small-caps">s</span><span 
class="small-caps">b</span><span 
class="small-caps">n</span></tspan>: 0262039249.
      </dd><dt id="X0-watkins1992q" class="thebibliography">
[47]  </dt><dd 
id="bib-47" class="thebibliography">
      <!--l. 587--><p class="noindent" >Christopher JCH Watkins and Peter Dayan. &#8220;Q-learning&#8221;. In: <tspan font-family="cmti" font-size="12">Machine</tspan>
      <tspan font-family="cmti" font-size="12">learning </tspan>8.3-4 (1992), pp. 279&#8211;292.
      </dd><dt id="X0-pmlr-v29-Wirth13" class="thebibliography">
[48]  </dt><dd 
id="bib-48" class="thebibliography">
      <!--l. 587--><p class="noindent" >Christian  Wirth  and  Johannes  Frnkranz.  &#8220;EPMC:  Every  Visit
      Preference Monte Carlo for Reinforcement Learning&#8221;. In: <tspan font-family="cmti" font-size="12">Proceedings</tspan>
      <tspan font-family="cmti" font-size="12">of  the  5th  Asian  Conference  on  Machine  Learning</tspan>.  Ed.  by  Cheng
      Soon Ong and Tu Bao Ho. Vol.&#x00A0;29. Proceedings of Machine Learning
      Research.                           Australian                           National
      University, Canberra, Australia: PMLR, Nov. 2013, pp. 483&#8211;497. <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">u</span><span 
class="small-caps">r</span><span 
class="small-caps">l</span></tspan>:
      <a 
href="https://proceedings.mlr.press/v29/Wirth13.html" class="url" ><tspan font-family="cmtt" font-size="12">https://proceedings.mlr.press/v29/Wirth13.html</tspan></a>.
      </dd><dt id="X0-Mnih2015" class="thebibliography">
[49]  </dt><dd 
id="bib-49" class="thebibliography">
      <!--l. 587--><p class="noindent" >Volodymyr            Mnih            et            al.            &#8220;Human-level
      control  through  deep  reinforcement  learning&#8221;.  In:  <tspan font-family="cmti" font-size="12">Nature  </tspan>518.7540
      (Feb. 2015), pp. 529&#8211;533. <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">i</span><span 
class="small-caps">s</span><span 
class="small-caps">s</span><span 
class="small-caps">n</span></tspan>: 1476-4687. <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">d</span><span 
class="small-caps">o</span><span 
class="small-caps">i</span></tspan>: <a 
href="10.1038/nature14236" class="url" ><tspan font-family="cmtt" font-size="12">10.1038/nature14236</tspan></a>.
      <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">u</span><span 
class="small-caps">r</span><span 
class="small-caps">l</span></tspan>: <a 
href="https://doi.org/10.1038/nature14236" class="url" ><tspan font-family="cmtt" font-size="12">https://doi.org/10.1038/nature14236</tspan></a>.
      </dd><dt id="X0-DBLP:journals/corr/MnihKSGAWR13" class="thebibliography">
[50]  </dt><dd 
id="bib-50" class="thebibliography">
      <!--l. 587--><p class="noindent" >Volodymyr  Mnih  et  al.  &#8220;Playing  Atari  with  Deep  Reinforcement
      Learning&#8221;. In: <tspan font-family="cmti" font-size="12">CoRR </tspan>abs/1312.5602 (2013). arXiv: <a 
href="1312.5602" class="url" ><tspan font-family="cmtt" font-size="12">1312.5602</tspan></a>. <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">u</span><span 
class="small-caps">r</span><span 
class="small-caps">l</span></tspan>:
      <a 
href="http://arxiv.org/abs/1312.5602" class="url" ><tspan font-family="cmtt" font-size="12">http://arxiv.org/abs/1312.5602</tspan></a>.
      </dd><dt id="X0-rubinstein2011simulation" class="thebibliography">
[51]  </dt><dd 
id="bib-51" class="thebibliography">
                                                                          

                                                                          
      <!--l. 587--><p class="noindent" >R.Y.                                 Rubinstein                                 and
      D.P. Kroese. <tspan font-family="cmti" font-size="12">Simulation and the Monte Carlo Method</tspan>. Wiley Series in
      Probability and Statistics. Wiley, 2011. Chap.&#x00A0;5. <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">i</span><span 
class="small-caps">s</span><span 
class="small-caps">b</span><span 
class="small-caps">n</span></tspan>: 9781118210529.
      <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">u</span><span 
class="small-caps">r</span><span 
class="small-caps">l</span></tspan>: <a 
href="https://books.google.co.uk/books?id=yWcvT80gQK4C" class="url" ><tspan font-family="cmtt" font-size="12">https://books.google.co.uk/books?id=yWcvT80gQK4C</tspan></a>.
      </dd><dt id="X0-sutton1988learning" class="thebibliography">
[52]  </dt><dd 
id="bib-52" class="thebibliography">
      <!--l. 587--><p class="noindent" >Richard S Sutton. &#8220;Learning to predict by the methods of temporal
      differences&#8221;. In: <tspan font-family="cmti" font-size="12">Machine learning </tspan>3.1 (1988), pp. 9&#8211;44.
      </dd><dt id="X0-tadic2001convergence" class="thebibliography">
[53]  </dt><dd 
id="bib-53" class="thebibliography">
      <!--l. 587--><p class="noindent" >Vladislav Tadi&#263;. &#8220;On the convergence of temporal-difference learning
      with linear function approximation&#8221;. In: <tspan font-family="cmti" font-size="12">Machine learning </tspan>42.3 (2001),
      pp. 241&#8211;267.
      </dd><dt id="X0-sutton2008convergent" class="thebibliography">
[54]  </dt><dd 
id="bib-54" class="thebibliography">
      <!--l. 587--><p class="noindent" >Richard   S   Sutton,   Csaba   Szepesvri,   and   Hamid   Reza   Maei.
      &#8220;A  convergent  O  (n)  algorithm  for  off-policy  temporal-difference
      learning with linear function approximation&#8221;. In: <tspan font-family="cmti" font-size="12">Advances in neural</tspan>
      <tspan font-family="cmti" font-size="12">information processing systems </tspan>21.21 (2008), pp. 1609&#8211;1616.
      </dd><dt id="X0-GemanStuart1992NNat" class="thebibliography">
[55]  </dt><dd 
id="bib-55" class="thebibliography">
      <!--l. 587--><p class="noindent" >Stuart Geman, Elie Bienenstock, and Ren Doursat. &#8220;Neural Networks
      and  the  Bias/Variance  Dilemma&#8221;.  eng.  In:  <tspan font-family="cmti" font-size="12">Neural computation  </tspan>4.1
      (1992), pp. 1&#8211;58. <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">i</span><span 
class="small-caps">s</span><span 
class="small-caps">s</span><span 
class="small-caps">n</span></tspan>: 1530-888X.
      </dd><dt id="X0-Sutton1998" class="thebibliography">
[56]  </dt><dd 
id="bib-56" class="thebibliography">
      <!--l. 587--><p class="noindent" >Richard S. Sutton and Andrew G. Barto. <tspan font-family="cmti" font-size="12">Reinforcement Learning: An</tspan>
      <tspan font-family="cmti" font-size="12">Introduction</tspan>.      Second.      The      MIT      Press,      2018.      <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">u</span><span 
class="small-caps">r</span><span 
class="small-caps">l</span></tspan>:
      <a 
href="http://incompleteideas.net/book/the-book-2nd.html" class="url" ><tspan font-family="cmtt" font-size="12">http://incompleteideas.net/book/the-book-2nd.html</tspan></a>.
      </dd><dt id="X0-vanhasselt2018deep" class="thebibliography">
[57]  </dt><dd 
id="bib-57" class="thebibliography">
      <!--l. 587--><p class="noindent" >Hado van Hasselt et al. <tspan font-family="cmti" font-size="12">Deep Reinforcement Learning and the Deadly</tspan>
      <tspan font-family="cmti" font-size="12">Triad</tspan>. 2018. arXiv: <a 
href="1812.02648" class="url" ><tspan font-family="cmtt" font-size="12">1812.02648</tspan></a> <tspan font-family="cmtt" font-size="12">[cs.AI]</tspan>.
      </dd><dt id="X0-tesauro1995temporal" class="thebibliography">
[58]  </dt><dd 
id="bib-58" class="thebibliography">
                                                                          

                                                                          
      <!--l. 587--><p class="noindent" >Gerald Tesauro et al. &#8220;Temporal difference learning and TD-Gammon&#8221;.
      In: <tspan font-family="cmti" font-size="12">Communications of the ACM </tspan>38.3 (1995), pp. 58&#8211;68.
      </dd><dt id="X0-berliner1980computer" class="thebibliography">
[59]  </dt><dd 
id="bib-59" class="thebibliography">
      <!--l. 587--><p class="noindent" >Hans  Berliner.  &#8220;Computer  backgammon&#8221;.  In:  <tspan font-family="cmti" font-size="12">Scientific  American</tspan>
      242.6 (1980), pp. 64&#8211;73.
      </dd><dt id="X0-rlintrochap12" class="thebibliography">
[60]  </dt><dd 
id="bib-60" class="thebibliography">
      <!--l. 587--><p class="noindent" >Richard S. Sutton and Andrew G. Barto. <tspan font-family="cmti" font-size="12">Reinforcement Learning: An</tspan>
      <tspan font-family="cmti" font-size="12">Introduction</tspan>. Cambridge, MA, USA: A Bradford Book, 2018. Chap.&#x00A0;12.
      <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">i</span><span 
class="small-caps">s</span><span 
class="small-caps">b</span><span 
class="small-caps">n</span></tspan>: 0262039249.
      </dd><dt id="X0-DBLP:journals/corr/abs-1812-02648" class="thebibliography">
[61]  </dt><dd 
id="bib-61" class="thebibliography">
      <!--l. 587--><p class="noindent" >Hado van Hasselt et al. &#8220;Deep Reinforcement Learning and the Deadly
      Triad&#8221;. In: <tspan font-family="cmti" font-size="12">CoRR  </tspan>abs/1812.02648 (2018). arXiv: <a 
href="1812.02648" class="url" ><tspan font-family="cmtt" font-size="12">1812.02648</tspan></a>. <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">u</span><span 
class="small-caps">r</span><span 
class="small-caps">l</span></tspan>:
      <a 
href="http://arxiv.org/abs/1812.02648" class="url" ><tspan font-family="cmtt" font-size="12">http://arxiv.org/abs/1812.02648</tspan></a>.
      </dd><dt id="X0-MuZero" class="thebibliography">
[62]  </dt><dd 
id="bib-62" class="thebibliography">
      <!--l. 587--><p class="noindent" >Julian Schrittwieser et al. &#8220;Mastering Atari, Go, Chess and Shogi by
      Planning with a Learned Model&#8221;. In: <tspan font-family="cmti" font-size="12">CoRR </tspan>abs/1911.08265 (2019).
      arXiv: <a 
href="1911.08265" class="url" ><tspan font-family="cmtt" font-size="12">1911.08265</tspan></a>. <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">u</span><span 
class="small-caps">r</span><span 
class="small-caps">l</span></tspan>: <a 
href="http://arxiv.org/abs/1911.08265" class="url" ><tspan font-family="cmtt" font-size="12">http://arxiv.org/abs/1911.08265</tspan></a>.
      </dd><dt id="X0-Silver2017" class="thebibliography">
[63]  </dt><dd 
id="bib-63" class="thebibliography">
      <!--l. 587--><p class="noindent" >David              Silver              et              al.              &#8220;Mastering
      the game of Go without human knowledge&#8221;. In: <tspan font-family="cmti" font-size="12">Nature </tspan>550.7676 (Oct.
      2017), pp. 354&#8211;359. <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">i</span><span 
class="small-caps">s</span><span 
class="small-caps">s</span><span 
class="small-caps">n</span></tspan>: 1476-4687. <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">d</span><span 
class="small-caps">o</span><span 
class="small-caps">i</span></tspan>: <a 
href="10.1038/nature24270" class="url" ><tspan font-family="cmtt" font-size="12">10.1038/nature24270</tspan></a>. <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">u</span><span 
class="small-caps">r</span><span 
class="small-caps">l</span></tspan>:
      <a 
href="https://doi.org/10.1038/nature24270" class="url" ><tspan font-family="cmtt" font-size="12">https://doi.org/10.1038/nature24270</tspan></a>.
      </dd><dt id="X0-thrun1993issues" class="thebibliography">
[64]  </dt><dd 
id="bib-64" class="thebibliography">
      <!--l. 587--><p class="noindent" >Sebastian  Thrun  and  Anton  Schwartz.  &#8220;Issues  in  using  function
      approximation  for  reinforcement  learning&#8221;.  In:  <tspan font-family="cmti" font-size="12">Proceedings  of  the</tspan>
      <tspan font-family="cmti" font-size="12">1993 Connectionist Models Summer School Hillsdale, NJ. Lawrence</tspan>
      <tspan font-family="cmti" font-size="12">Erlbaum</tspan>. Vol.&#x00A0;6. 1993.
      </dd><dt id="X0-lee2013bias" class="thebibliography">
[65]  </dt><dd 
id="bib-65" class="thebibliography">
                                                                          

                                                                          
      <!--l. 587--><p class="noindent" >Donghun Lee, Boris Defourny, and Warren B Powell. &#8220;Bias-corrected
      q-learning to control max-operator bias in q-learning&#8221;. In: <tspan font-family="cmti" font-size="12">2013 IEEE</tspan>
      <tspan font-family="cmti" font-size="12">Symposium  on  Adaptive  Dynamic  Programming  and  Reinforcement</tspan>
      <tspan font-family="cmti" font-size="12">Learning (ADPRL)</tspan>. IEEE. 2013, pp. 93&#8211;99.
      </dd><dt id="X0-DBLP:journals/corr/abs-2002-06487" class="thebibliography">
[66]  </dt><dd 
id="bib-66" class="thebibliography">
      <!--l. 587--><p class="noindent" >Qingfeng Lan et al. &#8220;Maxmin Q-learning: Controlling the Estimation
      Bias   of   Q-learning&#8221;.   In:   <tspan font-family="cmti" font-size="12">CoRR   </tspan>abs/2002.06487   (2020).   arXiv:
      <a 
href="2002.06487" class="url" ><tspan font-family="cmtt" font-size="12">2002.06487</tspan></a>. <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">u</span><span 
class="small-caps">r</span><span 
class="small-caps">l</span></tspan>: <a 
href="https://arxiv.org/abs/2002.06487" class="url" ><tspan font-family="cmtt" font-size="12">https://arxiv.org/abs/2002.06487</tspan></a>.
      </dd><dt id="X0-NIPS2010_091d584f" class="thebibliography">
[67]  </dt><dd 
id="bib-67" class="thebibliography">
      <!--l. 587--><p class="noindent" >Hado Hasselt. &#8220;Double Q-learning&#8221;. In: <tspan font-family="cmti" font-size="12">Advances in Neural Information</tspan>
      <tspan font-family="cmti" font-size="12">Processing Systems</tspan>. Ed. by J. Lafferty et al. Vol.&#x00A0;23. Curran Associates,
      Inc., 2010. <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">u</span><span 
class="small-caps">r</span><span 
class="small-caps">l</span></tspan>: <a 
href="https://proceedings.neurips.cc/paper/2010/file/091d584fced301b442654dd8c23b3fc9-Paper.pdf" class="url" ><tspan font-family="cmtt" font-size="12">https://proceedings.neurips.cc/paper/2010/file/091d584fced301b442654dd8c23b3fc9-Paper.pdf</tspan></a>.
      </dd><dt id="X0-DBLP:journals/corr/abs-2109-14419" class="thebibliography">
[68]  </dt><dd 
id="bib-68" class="thebibliography">
      <!--l. 587--><p class="noindent" >Zhizhou        Ren        et        al.        &#8220;On        the        Estimation
      Bias in Double Q-Learning&#8221;. In: <tspan font-family="cmti" font-size="12">CoRR </tspan>abs/2109.14419 (2021). arXiv:
      <a 
href="2109.14419" class="url" ><tspan font-family="cmtt" font-size="12">2109.14419</tspan></a>. <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">u</span><span 
class="small-caps">r</span><span 
class="small-caps">l</span></tspan>: <a 
href="https://arxiv.org/abs/2109.14419" class="url" ><tspan font-family="cmtt" font-size="12">https://arxiv.org/abs/2109.14419</tspan></a>.
      </dd><dt id="X0-Silver2016" class="thebibliography">
[69]  </dt><dd 
id="bib-69" class="thebibliography">
      <!--l. 587--><p class="noindent" >David  Silver  et  al.  &#8220;Mastering  the  game  of  Go  with  deep  neural
      networks   and   tree   search&#8221;.   In:   <tspan font-family="cmti" font-size="12">Nature   </tspan>529.7587   (Jan.   2016),
      pp.  484&#8211;489.  <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">i</span><span 
class="small-caps">s</span><span 
class="small-caps">s</span><span 
class="small-caps">n</span></tspan>:  1476-4687.  <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">d</span><span 
class="small-caps">o</span><span 
class="small-caps">i</span></tspan>:  <a 
href="10.1038/nature16961" class="url" ><tspan font-family="cmtt" font-size="12">10.1038/nature16961</tspan></a>.  <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">u</span><span 
class="small-caps">r</span><span 
class="small-caps">l</span></tspan>:
      <a 
href="https://doi.org/10.1038/nature16961" class="url" ><tspan font-family="cmtt" font-size="12">https://doi.org/10.1038/nature16961</tspan></a>.
      </dd><dt id="X0-Choy_2020_CVPR" class="thebibliography">
[70]  </dt><dd 
id="bib-70" class="thebibliography">
      <!--l. 587--><p class="noindent" >Christopher Choy et al. &#8220;High-Dimensional Convolutional Networks
      for Geometric Pattern Recognition&#8221;. In: <tspan font-family="cmti" font-size="12">Proceedings of the IEEE/CVF</tspan>
      <tspan font-family="cmti" font-size="12">Conference  on  Computer  Vision  and  Pattern  Recognition  (CVPR)</tspan>.
      June 2020.
      </dd><dt id="X0-lindsay2021convolutional" class="thebibliography">
[71]  </dt><dd 
id="bib-71" class="thebibliography">
      <!--l. 587--><p class="noindent" >Grace W Lindsay. &#8220;Convolutional neural networks as a model of the
      visual  system:  Past,  present,  and  future&#8221;.  In:  <tspan font-family="cmti" font-size="12">Journal  of  cognitive</tspan>
      <tspan font-family="cmti" font-size="12">neuroscience </tspan>33.10 (2021), pp. 2017&#8211;2031.
                                                                          

                                                                          
      </dd><dt id="X0-AlphaZero" class="thebibliography">
[72]  </dt><dd 
id="bib-72" class="thebibliography">
      <!--l. 587--><p class="noindent" >David Silver et al. &#8220;A general reinforcement learning algorithm that
      masters chess, shogi, and Go through self-play&#8221;. In: <tspan font-family="cmti" font-size="12">Science </tspan>362.6419
      (2018),   pp.   1140&#8211;1144.   <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">d</span><span 
class="small-caps">o</span><span 
class="small-caps">i</span></tspan>:   <a 
href="10.1126/science.aar6404" class="url" ><tspan font-family="cmtt" font-size="12">10.1126/science.aar6404</tspan></a>.   eprint:
      <a 
href="https://www.science.org/doi/pdf/10.1126/science.aar6404" class="url" ><tspan font-family="cmtt" font-size="12">https://www.science.org/doi/pdf/10.1126/science.aar6404</tspan></a>.
      <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">u</span><span 
class="small-caps">r</span><span 
class="small-caps">l</span></tspan>:
      <a 
href="https://www.science.org/doi/abs/10.1126/science.aar6404" class="url" ><tspan font-family="cmtt" font-size="12">https://www.science.org/doi/abs/10.1126/science.aar6404</tspan></a>.
      </dd><dt id="X0-riesenhuber1999hierarchical" class="thebibliography">
[73]  </dt><dd 
id="bib-73" class="thebibliography">
      <!--l. 587--><p class="noindent" >Maximilian Riesenhuber and Tomaso Poggio. &#8220;Hierarchical models of
      object  recognition  in  cortex&#8221;.  In:  <tspan font-family="cmti" font-size="12">Nature  neuroscience  </tspan>2.11  (1999),
      pp. 1019&#8211;1025.
      </dd><dt id="X0-pieters2016q" class="thebibliography">
[74]  </dt><dd 
id="bib-74" class="thebibliography">
      <!--l. 587--><p class="noindent" >Mathijs Pieters and Marco A Wiering. &#8220;Q-learning with experience
      replay in a dynamic environment&#8221;. In: <tspan font-family="cmti" font-size="12">2016 IEEE Symposium Series</tspan>
      <tspan font-family="cmti" font-size="12">on Computational Intelligence (SSCI)</tspan>. IEEE. 2016, pp. 1&#8211;8.
      </dd><dt id="X0-DBLP:journals/corr/abs-2112-04213" class="thebibliography">
[75]  </dt><dd 
id="bib-75" class="thebibliography">
      <!--l. 587--><p class="noindent" >Liran Szlak and Ohad Shamir. &#8220;Convergence Results For Q-Learning
      With Experience Replay&#8221;. In: <tspan font-family="cmti" font-size="12">CoRR </tspan>abs/2112.04213 (2021). arXiv:
      <a 
href="2112.04213" class="url" ><tspan font-family="cmtt" font-size="12">2112.04213</tspan></a>. <tspan font-family="cmcsc" font-size="10"><span 
class="small-caps">u</span><span 
class="small-caps">r</span><span 
class="small-caps">l</span></tspan>: <a 
href="https://arxiv.org/abs/2112.04213" class="url" ><tspan font-family="cmtt" font-size="12">https://arxiv.org/abs/2112.04213</tspan></a>.
      </dd><dt id="X0-michie1968boxes" class="thebibliography">
[76]  </dt><dd 
id="bib-76" class="thebibliography">
      <!--l. 587--><p class="noindent" >Donald Michie and Roger A Chambers. &#8220;BOXES: An experiment in
      adaptive control&#8221;. In: <tspan font-family="cmti" font-size="12">Machine intelligence </tspan>2.2 (1968), pp. 137&#8211;152.
      </dd><dt id="X0-barto1983neuronlike" class="thebibliography">
[77]  </dt><dd 
id="bib-77" class="thebibliography">
      <!--l. 587--><p class="noindent" >Andrew  G  Barto,  Richard  S  Sutton,  and  Charles  W  Anderson.
      &#8220;Neuronlike adaptive elements that can solve difficult learning control
      problems&#8221;. In: <tspan font-family="cmti" font-size="12">IEEE transactions on systems, man, and cybernetics </tspan>5
      (1983), pp. 834&#8211;846.</dd></dl>
 
</body></html> 

                                                                          

                                                                          
                                                                          


