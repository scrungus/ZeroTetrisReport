\BOOKMARK [1][-]{section.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.2}{Literature Survey and Review}{}% 2
\BOOKMARK [2][-]{subsection.2.1}{Markov Decision Processes \(MDPs\)}{section.2}% 3
\BOOKMARK [2][-]{subsection.2.2}{The Recycling Robot}{section.2}% 4
\BOOKMARK [2][-]{subsection.2.3}{Neural Networks}{section.2}% 5
\BOOKMARK [3][-]{subsubsection.2.3.1}{Feedforward Networks}{subsection.2.3}% 6
\BOOKMARK [3][-]{subsubsection.2.3.2}{Backpropagation}{subsection.2.3}% 7
\BOOKMARK [3][-]{subsubsection.2.3.3}{Convolutional Neural Networks \(CNNs\)}{subsection.2.3}% 8
\BOOKMARK [3][-]{subsubsection.2.3.4}{Recurrent Networks}{subsection.2.3}% 9
\BOOKMARK [3][-]{subsubsection.2.3.5}{Long Short-Term Memory \(LSTM\)}{subsection.2.3}% 10
\BOOKMARK [2][-]{subsection.2.4}{Policy Iteration}{section.2}% 11
\BOOKMARK [2][-]{subsection.2.5}{Monte Carlo Method}{section.2}% 12
\BOOKMARK [2][-]{subsection.2.6}{Temporal Difference Learning}{section.2}% 13
\BOOKMARK [3][-]{subsubsection.2.6.1}{Bias-Variance Tradeoff}{subsection.2.6}% 14
\BOOKMARK [3][-]{subsubsection.2.6.2}{Lg}{subsection.2.6}% 15
\BOOKMARK [2][-]{subsection.2.7}{Q-Learning}{section.2}% 16
\BOOKMARK [3][-]{subsubsection.2.7.1}{Double Q-Learning}{subsection.2.7}% 17
\BOOKMARK [3][-]{subsubsection.2.7.2}{Trust Region Policy Optimization \(TRPO\)}{subsection.2.7}% 18
\BOOKMARK [2][-]{subsection.2.8}{Key Algorithms}{section.2}% 19
\BOOKMARK [3][-]{subsubsection.2.8.1}{Deep Q-Network \(DQN\)}{subsection.2.8}% 20
\BOOKMARK [3][-]{subsubsection.2.8.2}{Proximal Policy Optimization \(PPO\)}{subsection.2.8}% 21
\BOOKMARK [3][-]{subsubsection.2.8.3}{Generalized Advantage Estimation \(GAE\)}{subsection.2.8}% 22
\BOOKMARK [1][-]{section.3}{Machine Learning in Tetris}{}% 23
\BOOKMARK [1][-]{section.4}{Experiment Design}{}% 24
\BOOKMARK [2][-]{subsection.4.1}{Bayesian Optimization}{section.4}% 25
\BOOKMARK [1][-]{section.5}{Algorithm Implementation}{}% 26
\BOOKMARK [2][-]{subsection.5.1}{Model}{section.5}% 27
\BOOKMARK [2][-]{subsection.5.2}{DataLoader}{section.5}% 28
\BOOKMARK [2][-]{subsection.5.3}{Optimizer}{section.5}% 29
\BOOKMARK [2][-]{subsection.5.4}{Training Step}{section.5}% 30
\BOOKMARK [1][-]{section.6}{Results and Analysis}{}% 31
\BOOKMARK [2][-]{subsection.6.1}{Optimization Results}{section.6}% 32
\BOOKMARK [3][-]{subsubsection.6.1.1}{DQN Optimization Results}{subsection.6.1}% 33
\BOOKMARK [3][-]{subsubsection.6.1.2}{PPO Optimization Results}{subsection.6.1}% 34
\BOOKMARK [2][-]{subsection.6.2}{Experiment 1 : State Representation}{section.6}% 35
\BOOKMARK [3][-]{subsubsection.6.2.1}{Discussion - PPO}{subsection.6.2}% 36
\BOOKMARK [3][-]{subsubsection.6.2.2}{Follow-Up Experiment : State History with Larger Neural Net}{subsection.6.2}% 37
\BOOKMARK [3][-]{subsubsection.6.2.3}{Discussion - DQN}{subsection.6.2}% 38
\BOOKMARK [3][-]{subsubsection.6.2.4}{Follow-up Experiment: DQN with Larger Neural Net}{subsection.6.2}% 39
\BOOKMARK [2][-]{subsection.6.3}{Experiment 2 : Reward Shaping}{section.6}% 40
\BOOKMARK [3][-]{subsubsection.6.3.1}{Reward Function Design : Time-Based Negative Reward}{subsection.6.3}% 41
\BOOKMARK [3][-]{subsubsection.6.3.2}{Reward Function Design : Score-Based Reward}{subsection.6.3}% 42
\BOOKMARK [3][-]{subsubsection.6.3.3}{Reward Function Design : Potential-Based Reward for Holes}{subsection.6.3}% 43
\BOOKMARK [3][-]{subsubsection.6.3.4}{Discussion - PPO}{subsection.6.3}% 44
\BOOKMARK [3][-]{subsubsection.6.3.5}{Discussion - DQN}{subsection.6.3}% 45
\BOOKMARK [2][-]{subsection.6.4}{Experiment 3: Exploration Strategy}{section.6}% 46
\BOOKMARK [3][-]{subsubsection.6.4.1}{Discussion - PPO}{subsection.6.4}% 47
\BOOKMARK [3][-]{subsubsection.6.4.2}{Discussion - DQN}{subsection.6.4}% 48
\BOOKMARK [2][-]{subsection.6.5}{Experiment Attempt : Network Architecture}{section.6}% 49
\BOOKMARK [1][-]{section.7}{Limitations and Further Work}{}% 50
\BOOKMARK [1][-]{section.8}{Conclusion}{}% 51
\BOOKMARK [1][-]{appendix.A}{Optimization - Navigating the Loss Landscape}{}% 52
\BOOKMARK [1][-]{appendix.B}{PPO - All Optimization Runs}{}% 53
\BOOKMARK [1][-]{appendix.C}{Behaviour of Potential-Based Rewards Function}{}% 54
\BOOKMARK [1][-]{appendix.D}{Proposed Architecture Experiment}{}% 55
\BOOKMARK [1][-]{appendix.E}{CNN Architectures}{}% 56
\BOOKMARK [1][-]{appendix.F}{Code}{}% 57
\BOOKMARK [1][-]{appendix.G}{Permissions}{}% 58
