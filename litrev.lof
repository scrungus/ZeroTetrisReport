\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax 
\contentsline {figure}{\numberline {1}{\ignorespaces an MDP of the recycling robot problem}}{4}{figure.1}%
\contentsline {figure}{\numberline {2}{\ignorespaces Transition Probabilities and Rewards}}{5}{figure.2}%
\contentsline {figure}{\numberline {3}{\ignorespaces A McCulloch-Pitts Neuron }}{6}{figure.3}%
\contentsline {figure}{\numberline {4}{\ignorespaces Single-Layered Feedforward Network}}{8}{figure.4}%
\contentsline {figure}{\numberline {5}{\ignorespaces Multi-Layered Feedforward Neural Network}}{9}{figure.5}%
\contentsline {figure}{\numberline {6}{\ignorespaces A $k+2$ layered perceptron}}{12}{figure.6}%
\contentsline {figure}{\numberline {7}{\ignorespaces A simplified view of a RNN}}{16}{figure.7}%
\contentsline {figure}{\numberline {8}{\ignorespaces Recurrence on scalar in a network with no hidden units}}{17}{figure.8}%
\contentsline {figure}{\numberline {9}{\ignorespaces Basic Recurrent Neural Network Architecture}}{18}{figure.9}%
\contentsline {figure}{\numberline {10}{\ignorespaces LSTM Network Architecture}}{18}{figure.10}%
\contentsline {figure}{\numberline {11}{\ignorespaces Hit-or-Miss example - Monte-Carlo Integration}}{20}{figure.11}%
\contentsline {figure}{\numberline {12}{\ignorespaces Plots showing a single timestep of $L^{CLIP}$ (Source: \textcite {schulman2017proximal})}}{31}{figure.12}%
\contentsline {figure}{\numberline {13}{\ignorespaces The seven possible tetrimino shapes, along with the standard naming conventions for each (Source: \textcite {lewis2015generalisation})}}{32}{figure.13}%
\contentsline {figure}{\numberline {14}{\ignorespaces Rewards per episode for 500 epochs of optimization, moving average with 50 Epoch window}}{39}{figure.14}%
\contentsline {figure}{\numberline {15}{\ignorespaces Rewards per episode for 500 epochs of optimization, moving average with 50 Epoch window - top 10 runs}}{40}{figure.15}%
\contentsline {figure}{\numberline {16}{\ignorespaces Run 2}}{40}{figure.16}%
\contentsline {figure}{\numberline {17}{\ignorespaces Parameters for Run 2}}{41}{figure.17}%
\contentsline {figure}{\numberline {18}{\ignorespaces Rewards per episode for 500 epochs of optimization, moving average with 100 Epoch window. Top 10 runs - PPO normalized, DQN unnormalized.}}{42}{figure.18}%
\contentsline {figure}{\numberline {19}{\ignorespaces Average reward over 25,000 epochs for agents with 0, 1 and 3 step histories}}{45}{figure.19}%
\contentsline {figure}{\numberline {20}{\ignorespaces Average reward over 25,000 epochs for agents with 0, 1 and 3 step histories, neural network scaled.}}{46}{figure.20}%
\contentsline {figure}{\numberline {21}{\ignorespaces Average returns per epoch for different history lengths, DQN.}}{47}{figure.21}%
\contentsline {figure}{\numberline {22}{\ignorespaces Example of a run from DQN optimisation: cumulative reward per epoch in blue, average reward per episode in red.}}{49}{figure.22}%
\contentsline {figure}{\numberline {23}{\ignorespaces Agent behaviour in Dominos environment under the first revision of negative reward signal}}{51}{figure.23}%
\contentsline {figure}{\numberline {24}{\ignorespaces Average lines cleared per episode over 25000 epochs; reward-shaped agents versus standard reward agent.}}{53}{figure.24}%
\contentsline {figure}{\numberline {25}{\ignorespaces An example of a perfect two-row clear using Score-based reward. We see no holes in the structure and correct placement of the L-shape tetrimino so as not to break our clear.}}{54}{figure.25}%
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Frame before clear}}}{54}{subfigure.25.1}%
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Frame after clear}}}{54}{subfigure.25.2}%
\contentsline {figure}{\numberline {26}{\ignorespaces Average lines cleared per episode over 25000 epochs; reward-shaped agents versus standard reward agent.}}{56}{figure.26}%
\contentsline {figure}{\numberline {27}{\ignorespaces PPO-VIME compared to standard PPO}}{60}{figure.27}%
\contentsline {figure}{\numberline {28}{\ignorespaces Bootstrapped DQN in training and evaluation modes, compared to standard DQN}}{62}{figure.28}%
\contentsline {figure}{\numberline {29}{\ignorespaces An example of a loss landscape (Source: \textcite {umd})}}{79}{figure.29}%
\contentsline {figure}{\numberline {30}{\ignorespaces The loss surfaces of ResNet-56 with/without skip connections (Source: \textcite {umd})}}{80}{figure.30}%
\contentsline {figure}{\numberline {31}{\ignorespaces Rewards per episode for 500 epochs of optimization, moving average with 50 Epoch window}}{80}{figure.31}%
